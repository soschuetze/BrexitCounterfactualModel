{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "672d56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import datetime as datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as sm\n",
    "import dgl.function as fn\n",
    "from tqdm import tqdm\n",
    "\n",
    "#imports for graph creation\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#imports for graph learning\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.data as data\n",
    "import torch_geometric.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b829ad2f",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454ec20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeNetwork:\n",
    "    \"\"\"\n",
    "    We define a class which computes the MST trade network for a given year \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"data\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_features(self, filter_gdp = True):\n",
    "        \n",
    "        ###IMPORT GDP###\n",
    "        #prepare GDP as a set of features \n",
    "        self.gdp = pd.read_csv(self.data_dir + \"/gdp.csv\")\n",
    "        scaler = StandardScaler()\n",
    "        self.gdp[[\"prev_gdp\"]] = scaler.fit_transform(np.log(self.gdp[[str(self.year-1)]]))\n",
    "        self.gdp[[\"current_gdp\"]] = scaler.fit_transform(np.log(self.gdp[[str(self.year)]]))\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp[\"country_code\"] = self.gdp[\"Country Code\"]\n",
    "        self.gdp = self.gdp[[\"country_code\", \"prev_gdp\", \"current_gdp\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP GROWTH###\n",
    "        #prepare GDP growth\n",
    "        self.gdp_growth = pd.read_csv(self.data_dir + \"/gdp_growth.csv\")\n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth[str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth[str(self.year)] \n",
    "        self.gdp_growth[\"future_gdp_growth\"] = self.gdp_growth[str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_growth[\"country_code\"] = self.gdp_growth[\"Country Code\"]\n",
    "        self.gdp_growth = self.gdp_growth[[\"country_code\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\", \"future_gdp_growth\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA###\n",
    "        self.gdp_per_capita = pd.read_csv(self.data_dir + \"/GDP_per_capita.csv\", skiprows = 4)\n",
    "        self.gdp_per_capita[\"prev_gdp_per_cap\"] = self.gdp_per_capita[str(self.year-1)]\n",
    "        self.gdp_per_capita[\"current_gdp_per_cap\"] = self.gdp_per_capita[str(self.year)]\n",
    "        self.gdp_per_capita[\"future_gdp_per_cap\"] = self.gdp_per_capita[str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita[\"country_code\"] = self.gdp_per_capita[\"Country Code\"]\n",
    "        self.gdp_per_capita = self.gdp_per_capita[[\"country_code\", \"prev_gdp_per_cap\",\n",
    "                                \"current_gdp_per_cap\", \"future_gdp_per_cap\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA GROWTH###\n",
    "        self.gdp_per_capita_growth = pd.read_csv(self.data_dir + \"/GDP_per_capita_growth.csv\", skiprows = 4)\n",
    "        self.gdp_per_capita_growth[\"prev_gdp_per_cap_growth\"] = self.gdp_per_capita_growth[str(self.year-1)]\n",
    "        self.gdp_per_capita_growth[\"current_gdp_per_cap_growth\"] = self.gdp_per_capita_growth[str(self.year)]\n",
    "        self.gdp_per_capita_growth[\"future_gdp_per_cap_growth\"] = self.gdp_per_capita_growth[str(self.year+1)]\n",
    "        \n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita_growth[\"country_code\"] = self.gdp_per_capita_growth[\"Country Code\"]\n",
    "        self.gdp_per_capita_growth = self.gdp_per_capita_growth[[\"country_code\", \"prev_gdp_per_cap_growth\",\n",
    "                                \"current_gdp_per_cap_growth\", \"future_gdp_per_cap_growth\"]].dropna()\n",
    "        \n",
    "        ###MERGE ALL DATA FEATURES###\n",
    "        self.features = pd.merge(self.gdp_growth, self.gdp, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita_growth, on = \"country_code\").dropna()\n",
    "\n",
    "    def prepare_network(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = pd.read_json('https://comtrade.un.org/data/cache/classificationS2.json')\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"results\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def[\"text\"].split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(self.data_dir + \"/ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"/country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = imports1.append(imports2).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(self.export_types, exporter_total, on = \"exporter_code\")\n",
    "        #multiply by 100 to allow weights to scale better in GNN\n",
    "        self.export_types[\"category_fraction\"] = self.export_types.import_value/self.export_types.export_total*10\n",
    "        ss = StandardScaler()\n",
    "        columns = list(set(self.export_types[\"product_category\"]))\n",
    "        self.export_types = self.export_types[[\"exporter_code\", \"product_category\", \"category_fraction\"]]\\\n",
    "        .pivot(index = [\"exporter_code\"], columns = [\"product_category\"], values = \"category_fraction\")\\\n",
    "        .reset_index().fillna(0)\n",
    "        #rename columns\n",
    "        rename_columns = []\n",
    "        for col in self.export_types.columns:\n",
    "            if(col == \"exporter_code\"):\n",
    "                rename_columns.append(col)\n",
    "            else:\n",
    "                rename_columns.append(\"resource_\" + col)\n",
    "        self.export_types.columns = rename_columns\n",
    "        self.export_types = self.export_types.rename(columns = {\"exporter_code\": \"country_code\"})\n",
    "        self.features = pd.merge(self.features, self.export_types, \n",
    "                                on = \"country_code\", how = \"left\")\n",
    "        \n",
    "        #look at fraction of goods traded with each counterparty\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, exporter_total, how = \"left\")\n",
    "        imports_table_grouped[\"export_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"export_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"export_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"export_percent\"]]))\n",
    "        imports_table_grouped[\"export_percent_feature\"] = imports_table_grouped[\"export_percent_feature\"] + abs(min(imports_table_grouped[\"export_percent_feature\"]))\n",
    "        \n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, how = \"left\")\n",
    "        imports_table_grouped[\"import_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"import_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"import_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"import_percent\"]]))\n",
    "        imports_table_grouped[\"import_percent_feature\"] = imports_table_grouped[\"import_percent_feature\"] + abs(min(imports_table_grouped[\"import_percent_feature\"]))\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "\n",
    "    def graph_create(self, exporter = True,\n",
    "            node_features = ['prev_gdp_growth', 'current_gdp_growth','prev_gdp','current_gdp'],\n",
    "            node_labels = 'future_gdp_growth'):\n",
    "        \n",
    "        if(exporter):\n",
    "            center_node = \"exporter_code\"\n",
    "            neighbors = \"importer_code\"\n",
    "            edge_features = 'export_percent'\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features.fillna(0, inplace = True)\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        #create lookup dictionary making node number / node features combatible with ordering of nodes\n",
    "        #in our edge table\n",
    "        self.node_lookup1 = self.features.set_index('node_numbers').to_dict()['country_code']\n",
    "        self.node_lookup2 = self.features.set_index('country_code').to_dict()['node_numbers']\n",
    "        \n",
    "        #get individual country's features\n",
    "        self.regression_table = pd.merge(self.features, self.trade_data,\n",
    "                        left_on = \"country_code\",\n",
    "                        right_on = center_node, how = 'right')\n",
    "        #get features for trade partners\n",
    "        self.regression_table = pd.merge(self.features, self.regression_table,\n",
    "                                        left_on = \"country_code\",\n",
    "                                        right_on = neighbors, how = \"right\",\n",
    "                                        suffixes = (\"_neighbors\", \"\"))\n",
    "        \n",
    "        self.regression_table[\"source\"] = self.trade_data[neighbors].apply(lambda x: self.node_lookup2[x])\n",
    "        self.regression_table[\"target\"] = self.trade_data[center_node].apply(lambda x: self.node_lookup2[x])    \n",
    "        self.regression_table.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "        #filter only to relevant columns\n",
    "        self.relevant_columns = [\"source\", \"target\"]\n",
    "        self.relevant_columns.extend(node_features)\n",
    "        self.relevant_columns.append(node_labels)\n",
    "        self.graph_table = self.regression_table[self.relevant_columns]\n",
    "        \n",
    "        if(self.graph_table.isnull().values.any()): print(\"edges contain null / inf values\")\n",
    "\n",
    "        self.node_attributes = torch.tensor(np.array(self.features[node_features]))\\\n",
    "        .to(torch.float)\n",
    "        self.source_nodes = list(self.graph_table[\"source\"])\n",
    "        self.target_nodes = list(self.graph_table[\"target\"])\n",
    "        self.edge_attributes = list(self.trade_data[edge_features])\n",
    "    \n",
    "        self.pyg_graph = data.Data(x = self.node_attributes,\n",
    "                                   edge_index = torch.tensor([self.source_nodes, self.target_nodes]),\n",
    "                                   edge_attr = torch.tensor(self.edge_attributes).to(torch.float),\n",
    "                                   y = torch.tensor(list(self.features[node_labels])).to(torch.float))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25a50a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 1.98 s, total: 16.9 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trade_1985 = TradeNetwork(year = 1970)\n",
    "trade_1985.prepare_features()\n",
    "trade_1985.prepare_network()\n",
    "trade_1985.graph_create( node_features = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth',\n",
    "    'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "       'resource_8', 'resource_9'],\n",
    "        node_labels = 'future_gdp_per_cap_growth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aac08b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([99, 12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_1985.node_attributes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01e14a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[99, 12], edge_index=[2, 3643], edge_attr=[3643], y=[99])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_1985.pyg_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea44a95",
   "metadata": {},
   "source": [
    "# Regression - Playground for Testing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c20acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [1965, 1966, 1976, 1968, 1969, 1970]:\n",
    "    trade = TradeNetwork(year = year)\n",
    "    trade.prepare_features()\n",
    "    trade.prepare_network()\n",
    "    trade.graph_create()\n",
    "    \n",
    "    trade.regression_table[\"exporter_code\"] = trade.regression_table[\"exporter_code\"] + str(year)\n",
    "    \n",
    "    if(year == 1965):\n",
    "        reg_table = trade.regression_table\n",
    "    else:\n",
    "        reg_table = reg_table.append(trade.regression_table)\n",
    "    if(year == 1970):\n",
    "        val_table = trade.regression_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d8724d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1975\n",
    "trade = TradeNetwork(year = year)\n",
    "trade.prepare_features()\n",
    "trade.prepare_network()\n",
    "trade.graph_create()\n",
    "\n",
    "val_table = trade.regression_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec7dd291",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table_2_code = reg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1203ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table_1_code = reg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc535c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_regression(regression_table, agg_col = \"export_percent\",\n",
    "                       pred = \"future_gdp_growth\",\n",
    "                       neighbour = [\"current_gdp_per_cap_growth\", \"prev_gdp_growth\"],\n",
    "                       AR = True, return_reg_table_pre_agg = False, return_reg_table_post_agg = False,\n",
    "                       return_predictions = True, val_table = None):\n",
    "    if(agg_col == \"export_percent\"):\n",
    "        aggregate_to = \"exporter_code\"\n",
    "    \n",
    "    cols_for_neighbors = []\n",
    "    for col in neighbour: \n",
    "        regression_table[col + \"_weighted_neighbors\"] = regression_table[col + \"_neighbors\"] * regression_table[agg_col]\n",
    "        cols_for_neighbors.append(col + \"_weighted_neighbors\")\n",
    "    \n",
    "    \n",
    "    group_cols1 = [aggregate_to]\n",
    "    group_cols1.extend(neighbour)\n",
    "    group_cols1.append(pred)\n",
    "    \n",
    "    #return regression_table\n",
    "    if(return_reg_table_pre_agg):\n",
    "        return regression_table\n",
    "    \n",
    "    regression_table = regression_table.groupby(group_cols1)[cols_for_neighbors].agg([\"sum\"]).reset_index()\n",
    "    \n",
    "    group_cols2 = group_cols1.copy()\n",
    "    group_cols2.extend(cols_for_neighbors)\n",
    "    regression_table.columns = group_cols2\n",
    "    \n",
    "    if(return_reg_table_post_agg):\n",
    "        return regression_table\n",
    "    \n",
    "    formula = pred + \" ~ \"\n",
    "\n",
    "    cols_for_prediction = []\n",
    "    \n",
    "    if(AR):\n",
    "        for col in neighbour:\n",
    "            cols_for_prediction.append(col)\n",
    "            formula = formula + col + \"+\"\n",
    "\n",
    "    for col in cols_for_neighbors:\n",
    "        cols_for_prediction.append(col)\n",
    "        formula = formula + col + \"+\"\n",
    "    #formula  = formula [:-1]\n",
    "    formula = formula + \"0\"\n",
    "    \n",
    "    print(formula)\n",
    "    \n",
    "    result = sm.ols(formula=formula, \n",
    "             data=regression_table).fit()\n",
    "\n",
    "    print(result.summary())\n",
    "    \n",
    "    if(return_predictions):\n",
    "        pred = result.predict(regression_table[cols_for_prediction])\n",
    "        return (pred, regression_table)\n",
    "    \n",
    "    if(val_table is not None ):\n",
    "        print(\"validating\")\n",
    "        for col in neighbour: \n",
    "            val_table[col + \"_weighted_neighbors\"] = val_table[col + \"_neighbors\"] * val_table[agg_col]\n",
    "\n",
    "        val_table = val_table.groupby(group_cols1)[cols_for_neighbors].agg([\"sum\"]).reset_index()\n",
    "        val_table.columns = group_cols2\n",
    "        \n",
    "        pred = result.predict(val_table[cols_for_prediction])\n",
    "        return (pred, val_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34f87807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       BEL\n",
       "1       DEU\n",
       "2       DNK\n",
       "3       ESP\n",
       "4       FRA\n",
       "       ... \n",
       "5012    GBR\n",
       "5013    JPN\n",
       "5014    MWI\n",
       "5015    USA\n",
       "5016    ZMB\n",
       "Name: exporter_code, Length: 5017, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_table[\"exporter_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23ce809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future_gdp_per_cap_growth ~ prev_gdp_per_cap_growth+current_gdp_per_cap_growth+prev_gdp_per_cap_growth_weighted_neighbors+current_gdp_per_cap_growth_weighted_neighbors+0\n",
      "                                    OLS Regression Results                                    \n",
      "==============================================================================================\n",
      "Dep. Variable:     future_gdp_per_cap_growth   R-squared (uncentered):                   0.309\n",
      "Model:                                   OLS   Adj. R-squared (uncentered):              0.304\n",
      "Method:                        Least Squares   F-statistic:                              62.48\n",
      "Date:                       Fri, 03 Feb 2023   Prob (F-statistic):                    1.18e-43\n",
      "Time:                               11:02:05   Log-Likelihood:                         -1685.4\n",
      "No. Observations:                        564   AIC:                                      3379.\n",
      "Df Residuals:                            560   BIC:                                      3396.\n",
      "Df Model:                                  4                                                  \n",
      "Covariance Type:                   nonrobust                                                  \n",
      "=================================================================================================================\n",
      "                                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "prev_gdp_per_cap_growth                           0.0445      0.034      1.291      0.197      -0.023       0.112\n",
      "current_gdp_per_cap_growth                        0.1755      0.035      5.069      0.000       0.107       0.243\n",
      "prev_gdp_per_cap_growth_weighted_neighbors        0.1426      0.076      1.885      0.060      -0.006       0.291\n",
      "current_gdp_per_cap_growth_weighted_neighbors     0.3721      0.071      5.216      0.000       0.232       0.512\n",
      "==============================================================================\n",
      "Omnibus:                       42.324   Durbin-Watson:                   1.966\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              184.639\n",
      "Skew:                          -0.057   Prob(JB):                     8.06e-41\n",
      "Kurtosis:                       5.801   Cond. No.                         4.54\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "validating\n"
     ]
    }
   ],
   "source": [
    "pred, aggregated_reg_table =  neighbor_regression(reg_table.fillna(0),\n",
    "                                neighbour = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth'],\n",
    "#'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "#       'resource_8', 'resource_9'],\n",
    "                                 pred = \"future_gdp_per_cap_growth\", val_table = val_table.fillna(0),\n",
    "                                                 return_predictions = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b404621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exporter_code</th>\n",
       "      <th>prev_gdp_per_cap_growth</th>\n",
       "      <th>current_gdp_per_cap_growth</th>\n",
       "      <th>resource_0</th>\n",
       "      <th>resource_1</th>\n",
       "      <th>resource_2</th>\n",
       "      <th>resource_3</th>\n",
       "      <th>resource_4</th>\n",
       "      <th>resource_5</th>\n",
       "      <th>resource_6</th>\n",
       "      <th>...</th>\n",
       "      <th>resource_0_weighted_neighbors</th>\n",
       "      <th>resource_1_weighted_neighbors</th>\n",
       "      <th>resource_2_weighted_neighbors</th>\n",
       "      <th>resource_3_weighted_neighbors</th>\n",
       "      <th>resource_4_weighted_neighbors</th>\n",
       "      <th>resource_5_weighted_neighbors</th>\n",
       "      <th>resource_6_weighted_neighbors</th>\n",
       "      <th>resource_7_weighted_neighbors</th>\n",
       "      <th>resource_8_weighted_neighbors</th>\n",
       "      <th>resource_9_weighted_neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AND</td>\n",
       "      <td>1.027602</td>\n",
       "      <td>-3.354398</td>\n",
       "      <td>0.957342</td>\n",
       "      <td>0.625844</td>\n",
       "      <td>0.445320</td>\n",
       "      <td>0.622001</td>\n",
       "      <td>0.007012</td>\n",
       "      <td>0.027435</td>\n",
       "      <td>1.374730</td>\n",
       "      <td>...</td>\n",
       "      <td>1.422760</td>\n",
       "      <td>0.254662</td>\n",
       "      <td>0.393571</td>\n",
       "      <td>0.071137</td>\n",
       "      <td>0.083411</td>\n",
       "      <td>0.815986</td>\n",
       "      <td>2.375428</td>\n",
       "      <td>3.366039</td>\n",
       "      <td>1.130081</td>\n",
       "      <td>0.086924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARG</td>\n",
       "      <td>3.851758</td>\n",
       "      <td>-1.587829</td>\n",
       "      <td>6.286805</td>\n",
       "      <td>0.107449</td>\n",
       "      <td>0.557943</td>\n",
       "      <td>0.053863</td>\n",
       "      <td>0.275576</td>\n",
       "      <td>0.454025</td>\n",
       "      <td>0.554104</td>\n",
       "      <td>...</td>\n",
       "      <td>2.084845</td>\n",
       "      <td>0.158952</td>\n",
       "      <td>1.173070</td>\n",
       "      <td>0.612699</td>\n",
       "      <td>0.097153</td>\n",
       "      <td>0.606588</td>\n",
       "      <td>1.898759</td>\n",
       "      <td>2.395063</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.136070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUS</td>\n",
       "      <td>1.503392</td>\n",
       "      <td>0.095665</td>\n",
       "      <td>3.340058</td>\n",
       "      <td>0.019892</td>\n",
       "      <td>3.081748</td>\n",
       "      <td>1.283181</td>\n",
       "      <td>0.089824</td>\n",
       "      <td>0.311045</td>\n",
       "      <td>1.141278</td>\n",
       "      <td>...</td>\n",
       "      <td>1.077622</td>\n",
       "      <td>0.082747</td>\n",
       "      <td>0.946227</td>\n",
       "      <td>0.652324</td>\n",
       "      <td>0.128572</td>\n",
       "      <td>0.600789</td>\n",
       "      <td>2.309727</td>\n",
       "      <td>3.243780</td>\n",
       "      <td>0.822282</td>\n",
       "      <td>0.135930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUT</td>\n",
       "      <td>3.765373</td>\n",
       "      <td>-0.099030</td>\n",
       "      <td>0.376489</td>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.864920</td>\n",
       "      <td>0.210284</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.437064</td>\n",
       "      <td>3.773407</td>\n",
       "      <td>...</td>\n",
       "      <td>1.017742</td>\n",
       "      <td>0.136670</td>\n",
       "      <td>0.665819</td>\n",
       "      <td>0.779030</td>\n",
       "      <td>0.070671</td>\n",
       "      <td>0.760651</td>\n",
       "      <td>2.092277</td>\n",
       "      <td>3.352353</td>\n",
       "      <td>0.974534</td>\n",
       "      <td>0.150254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BDI</td>\n",
       "      <td>-1.851440</td>\n",
       "      <td>-0.783647</td>\n",
       "      <td>8.231727</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>1.365744</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.012425</td>\n",
       "      <td>0.317947</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248517</td>\n",
       "      <td>0.140640</td>\n",
       "      <td>0.755797</td>\n",
       "      <td>0.305850</td>\n",
       "      <td>0.087140</td>\n",
       "      <td>0.892711</td>\n",
       "      <td>1.782667</td>\n",
       "      <td>3.724558</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.163972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>VCT</td>\n",
       "      <td>-9.841106</td>\n",
       "      <td>-8.619881</td>\n",
       "      <td>8.207326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877022</td>\n",
       "      <td>0.184614</td>\n",
       "      <td>0.059695</td>\n",
       "      <td>...</td>\n",
       "      <td>2.660015</td>\n",
       "      <td>0.127796</td>\n",
       "      <td>0.627389</td>\n",
       "      <td>5.071664</td>\n",
       "      <td>0.002579</td>\n",
       "      <td>0.691548</td>\n",
       "      <td>0.222427</td>\n",
       "      <td>0.210955</td>\n",
       "      <td>0.290549</td>\n",
       "      <td>0.095078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>VEN</td>\n",
       "      <td>-0.823411</td>\n",
       "      <td>-0.010197</td>\n",
       "      <td>0.323616</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>1.452826</td>\n",
       "      <td>7.624834</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.129574</td>\n",
       "      <td>0.208183</td>\n",
       "      <td>...</td>\n",
       "      <td>2.090453</td>\n",
       "      <td>0.164551</td>\n",
       "      <td>1.152187</td>\n",
       "      <td>0.347308</td>\n",
       "      <td>0.100886</td>\n",
       "      <td>0.870909</td>\n",
       "      <td>1.296044</td>\n",
       "      <td>3.124029</td>\n",
       "      <td>0.686648</td>\n",
       "      <td>0.166985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ZAF</td>\n",
       "      <td>3.347548</td>\n",
       "      <td>-0.898473</td>\n",
       "      <td>3.028184</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>2.901512</td>\n",
       "      <td>0.226838</td>\n",
       "      <td>0.070318</td>\n",
       "      <td>0.439105</td>\n",
       "      <td>2.582186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919210</td>\n",
       "      <td>0.185844</td>\n",
       "      <td>0.630177</td>\n",
       "      <td>0.369621</td>\n",
       "      <td>0.063708</td>\n",
       "      <td>0.808727</td>\n",
       "      <td>2.230780</td>\n",
       "      <td>3.703016</td>\n",
       "      <td>0.910613</td>\n",
       "      <td>0.178304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>ZMB</td>\n",
       "      <td>2.883305</td>\n",
       "      <td>-5.522808</td>\n",
       "      <td>0.089742</td>\n",
       "      <td>0.401505</td>\n",
       "      <td>4.426580</td>\n",
       "      <td>0.032178</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.133474</td>\n",
       "      <td>4.780873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852845</td>\n",
       "      <td>0.190191</td>\n",
       "      <td>0.607113</td>\n",
       "      <td>0.267574</td>\n",
       "      <td>0.076116</td>\n",
       "      <td>0.765742</td>\n",
       "      <td>2.397338</td>\n",
       "      <td>3.688692</td>\n",
       "      <td>0.998753</td>\n",
       "      <td>0.155636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ZWE</td>\n",
       "      <td>2.991832</td>\n",
       "      <td>-5.184619</td>\n",
       "      <td>1.964892</td>\n",
       "      <td>0.116369</td>\n",
       "      <td>0.843940</td>\n",
       "      <td>0.384348</td>\n",
       "      <td>0.033206</td>\n",
       "      <td>0.232284</td>\n",
       "      <td>5.608059</td>\n",
       "      <td>...</td>\n",
       "      <td>1.998178</td>\n",
       "      <td>1.406598</td>\n",
       "      <td>1.571935</td>\n",
       "      <td>0.253065</td>\n",
       "      <td>0.188975</td>\n",
       "      <td>0.540357</td>\n",
       "      <td>1.296537</td>\n",
       "      <td>1.988754</td>\n",
       "      <td>0.390595</td>\n",
       "      <td>0.365007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    exporter_code  prev_gdp_per_cap_growth  current_gdp_per_cap_growth  \\\n",
       "0             AND                 1.027602                   -3.354398   \n",
       "1             ARG                 3.851758                   -1.587829   \n",
       "2             AUS                 1.503392                    0.095665   \n",
       "3             AUT                 3.765373                   -0.099030   \n",
       "4             BDI                -1.851440                   -0.783647   \n",
       "..            ...                      ...                         ...   \n",
       "103           VCT                -9.841106                   -8.619881   \n",
       "104           VEN                -0.823411                   -0.010197   \n",
       "105           ZAF                 3.347548                   -0.898473   \n",
       "106           ZMB                 2.883305                   -5.522808   \n",
       "107           ZWE                 2.991832                   -5.184619   \n",
       "\n",
       "     resource_0  resource_1  resource_2  resource_3  resource_4  resource_5  \\\n",
       "0      0.957342    0.625844    0.445320    0.622001    0.007012    0.027435   \n",
       "1      6.286805    0.107449    0.557943    0.053863    0.275576    0.454025   \n",
       "2      3.340058    0.019892    3.081748    1.283181    0.089824    0.311045   \n",
       "3      0.376489    0.041008    0.864920    0.210284    0.008297    0.437064   \n",
       "4      8.231727    0.001893    1.365744    0.001488    0.005549    0.012425   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "103    8.207326    0.000000    0.457558    0.000000    0.877022    0.184614   \n",
       "104    0.323616    0.011878    1.452826    7.624834    0.000123    0.129574   \n",
       "105    3.028184    0.062012    2.901512    0.226838    0.070318    0.439105   \n",
       "106    0.089742    0.401505    4.426580    0.032178    0.000492    0.133474   \n",
       "107    1.964892    0.116369    0.843940    0.384348    0.033206    0.232284   \n",
       "\n",
       "     resource_6  ...  resource_0_weighted_neighbors  \\\n",
       "0      1.374730  ...                       1.422760   \n",
       "1      0.554104  ...                       2.084845   \n",
       "2      1.141278  ...                       1.077622   \n",
       "3      3.773407  ...                       1.017742   \n",
       "4      0.317947  ...                       1.248517   \n",
       "..          ...  ...                            ...   \n",
       "103    0.059695  ...                       2.660015   \n",
       "104    0.208183  ...                       2.090453   \n",
       "105    2.582186  ...                       0.919210   \n",
       "106    4.780873  ...                       0.852845   \n",
       "107    5.608059  ...                       1.998178   \n",
       "\n",
       "     resource_1_weighted_neighbors  resource_2_weighted_neighbors  \\\n",
       "0                         0.254662                       0.393571   \n",
       "1                         0.158952                       1.173070   \n",
       "2                         0.082747                       0.946227   \n",
       "3                         0.136670                       0.665819   \n",
       "4                         0.140640                       0.755797   \n",
       "..                             ...                            ...   \n",
       "103                       0.127796                       0.627389   \n",
       "104                       0.164551                       1.152187   \n",
       "105                       0.185844                       0.630177   \n",
       "106                       0.190191                       0.607113   \n",
       "107                       1.406598                       1.571935   \n",
       "\n",
       "     resource_3_weighted_neighbors  resource_4_weighted_neighbors  \\\n",
       "0                         0.071137                       0.083411   \n",
       "1                         0.612699                       0.097153   \n",
       "2                         0.652324                       0.128572   \n",
       "3                         0.779030                       0.070671   \n",
       "4                         0.305850                       0.087140   \n",
       "..                             ...                            ...   \n",
       "103                       5.071664                       0.002579   \n",
       "104                       0.347308                       0.100886   \n",
       "105                       0.369621                       0.063708   \n",
       "106                       0.267574                       0.076116   \n",
       "107                       0.253065                       0.188975   \n",
       "\n",
       "     resource_5_weighted_neighbors  resource_6_weighted_neighbors  \\\n",
       "0                         0.815986                       2.375428   \n",
       "1                         0.606588                       1.898759   \n",
       "2                         0.600789                       2.309727   \n",
       "3                         0.760651                       2.092277   \n",
       "4                         0.892711                       1.782667   \n",
       "..                             ...                            ...   \n",
       "103                       0.691548                       0.222427   \n",
       "104                       0.870909                       1.296044   \n",
       "105                       0.808727                       2.230780   \n",
       "106                       0.765742                       2.397338   \n",
       "107                       0.540357                       1.296537   \n",
       "\n",
       "     resource_7_weighted_neighbors  resource_8_weighted_neighbors  \\\n",
       "0                         3.366039                       1.130081   \n",
       "1                         2.395063                       0.836800   \n",
       "2                         3.243780                       0.822282   \n",
       "3                         3.352353                       0.974534   \n",
       "4                         3.724558                       0.898148   \n",
       "..                             ...                            ...   \n",
       "103                       0.210955                       0.290549   \n",
       "104                       3.124029                       0.686648   \n",
       "105                       3.703016                       0.910613   \n",
       "106                       3.688692                       0.998753   \n",
       "107                       1.988754                       0.390595   \n",
       "\n",
       "     resource_9_weighted_neighbors  \n",
       "0                         0.086924  \n",
       "1                         0.136070  \n",
       "2                         0.135930  \n",
       "3                         0.150254  \n",
       "4                         0.163972  \n",
       "..                             ...  \n",
       "103                       0.095078  \n",
       "104                       0.166985  \n",
       "105                       0.178304  \n",
       "106                       0.155636  \n",
       "107                       0.365007  \n",
       "\n",
       "[108 rows x 26 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_reg_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "4a5b4518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7fb2a33a60>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaxklEQVR4nO3df4xd5X3n8ffX41sykG0GxCS1xxBTrdc0hIRpRlYqS2340ZpV0nqWbBKqtsvuIlmVSJukkbd2U5WmG4QlK21X2x9aK2SXqrRgFWLcJO2EAFHbqCQZx6aOgWlQKOAxLdMmsy14gPHw7R9zrnvnzjnn3nPPOffc89zPS0Keub/Oc6/x5zz3+/w45u6IiEiYNlTdABERKY9CXkQkYAp5EZGAKeRFRAKmkBcRCdjGqhvQ6tJLL/WtW7dW3QwRkVo5duzYP7r7eNx9AxXyW7duZXZ2tupmiIjUipk9m3SfyjUiIgFTyIuIBEwhLyISMIW8iEjAFPIiIgEbqNk1Uh9Hjs9zcGaOM4tLbB4bZe+u7UxPTlTdLBFpo5CXzI4cn2f/AydZWl4BYH5xif0PnARQ0IsMGJVrJLODM3PnA75paXmFgzNzFbVIRJIo5CWzM4tLmW4Xkeoo5CWzzWOjmW4Xkeoo5CWzvbu2M9oYWXPbaGOEvbu2V9QiEUmigVfJrDm4qtk1IoNPIS89mZ6cUKiL1IDKNSIiAcsd8mb2BjP7upk9bmanzOyT0e2XmNlDZvbt6M+L8zdXRESyKKIn/ypwnbu/E7gGuNHM3g3sAx52923Aw9HvIiLSR7lD3le9FP3aiP5zYDdwd3T73cB03mOJiEg2hdTkzWzEzE4ALwIPufvXgLe4+wsA0Z9vTnjuHjObNbPZhYWFIpojIiKRQkLe3Vfc/RpgC7DDzN6e4bmH3H3K3afGx2MvUSgiIj0qdHaNuy8CXwFuBP7BzDYBRH++WOSxRESksyJm14yb2Vj08yhwA/AUcBS4JXrYLcCDeY8lIiLZFLEYahNwt5mNsHrSOOzunzezvwYOm9mtwHPABwo4loiIZJA75N39b4DJmNv/Cbg+7+uLiEjvtOJVRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJWO6QN7PLzOxRM3vSzE6Z2Uei2y8xs4fM7NvRnxfnb66IiGRRRE/+HPBxd/8h4N3AbWb2NmAf8LC7bwMejn4XEZE+yh3y7v6Cu38z+vlfgCeBCWA3cHf0sLuB6bzHEhGRbAqtyZvZVmAS+BrwFnd/AVZPBMCbE56zx8xmzWx2YWGhyOaIiAy9wkLezN4I3A981N3/udvnufshd59y96nx8fGimiMiIhQU8mbWYDXg73H3B6Kb/8HMNkX3bwJeLOJYIiLSvSJm1xhwF/Cku/9my11HgVuin28BHsx7LBERyWZjAa+xE/g54KSZnYhu+xXgAHDYzG4FngM+UMCxREQkg9wh7+5/BVjC3dfnfX0REemdVryKiARMIS8iEjCFvIhIwIoYeJUhdOT4PAdn5jizuMTmsVH27trO9ORE1c0SkTYKecnsyPF59j9wkqXlFQDmF5fY/8BJAAW9yIBRuUYyOXJ8no8ffvx8wDctLa9wcGauolaJSBKFvHSt2YNfcY+9/8ziUp9bJCKdKOSlawdn5tb14FttHhvtY2tEpBuqyQekdTD0TaMNzGDx7HJhA6NpPfXRxgh7d23P9fppNNAr0huFfCDaB0MXl5bP31fUwOjmsVHmY4J+xIw7b7q6tNDVQK9I71SuCUSnUkoRA6N7d21ntDGy5rbRxgif/uA7Sw3buPemgV6R7qgnH4huBj3jeuFZNIO832WTpPemgV6RzhTygUgqpbQyVksfeUJ5enKi0FDvptae9N400CvSmco1gYgrpbRzGKgSR7PWPr+4hPNvtfYjx+fXPC6pTFTmQK9IKNSTD0R7KSV+Jnt8iaOqmStptfbW41dVJhIJgUI+IK2llJ0HHumqxFHlzJUstfaiy0Qiw0LlmkB1W+KocuZKUk1dtXaR4qgnH5D2ssv73zXBo08tpJY4qpy5snfX9jXfIkC1dpGiKeRzGKRVmHFll/uPzXdcpFTlzBXV2kXKp5Dv0aCtwux2ELNdmb3pbk6CqrWLlEs1+R4N2irMXssu05MT3HnT1UyMjWLAxNhoIVsUdDs9UkTKpZ58jwZtFWaesksZvelO3ywGqdQlEjL15Hs0aDNDBm3BUNpJUL18kf4pJOTN7LNm9qKZfavltkvM7CEz+3b058VFHGtQDFqollV26VXaSXDQSl0iISuqJ///gBvbbtsHPOzu24CHo9+DMWih2mzTV/ddxzMH3stX911XaVvSToKDVuoSCVkhNXl3/wsz29p2827gPdHPdwNfAX65iOMNCs0MSZY2PfLgzJw2HBPpkzIHXt/i7i8AuPsLZvbmEo81NKoesMxy/KSToBZBifRP5bNrzGwPsAfg8ssvr7g1g63quflFHT/LIqiqT2oidWfuSfsVZnyh1XLN59397dHvc8B7ol78JuAr7p7aVZuamvLZ2dlC2hOipE3HJsZG+eq+61IDsYiw7HT8orWfVGC1x1/12IfIoDGzY+4+FXdfmVMojwK3RD/fAjxY4rGGQq/TEuPu++h9J5j8jS9lmrbY7wFTzcIRya+Qco2Z/TGrg6yXmtlp4HbgAHDYzG4FngM+UMSxhlnagqdOgRh3/dfvnV3OVG4pap+bbr9VaBaOSH6F9OTd/afdfZO7N9x9i7vf5e7/5O7Xu/u26M/vFnGsYdbrtMS0UMzSM772ynGs7basA6Zx3yo+dt8JfvXIyXWPHbQFZyJ1pBWvNZI2Nz8tEDuFYjc94yPH57n/2PyaK04Z8P53ZZtGGveNw4F7HntOl/0TKUHls2skm16nJbbf18pZHVRNG4xNCudHn1rI1P6kE0rz+rO67J9IsRTygegmEH/96CkWl5Zjn99pOmSW+nhazT2prp/0WlpwJpKPQr6Pyp7znRaIzfuabYgL2rT95zsNura+rsH5sk77yWPvru187L4TsRcaz1Jr1/x5ke6oJt8ng7LzYnN/m/YB1KakHntafbz1vQHrArx1cHd6coKfeffluQZwswzeigy7IHrydejV9XrlprKk9cxbe+UjZqy4M5FyzdidBx5JrPc3tR7rU9NXM/XWS3r+O0sbvJ166yUDu82DSBVqH/JVL/Xv1iDN+T5yfJ6XXz237vbRxgjXXjm+5vNciVZEp10zttv3MPkbX+L2n7zqfOmo9XWOHJ9n54FHugrLLIO3ZarL/3sy3GpfrqnLqsii5nw3w/CKfV9g54FHMpd7msHUPgC7wTjfU0/qlSd9rmMXNro6dnPxVXubs5ay0j6zfp406/L/ngy32od8tz3kvOGY195d22lsWFuJbmyw3AuJstb144IJ4HWH+4/NJ858aYr7XF96Zf23giRxIZg1LPfu2p44pvCm0Ubf/p4H6duZSJLah3w3PeRBGfRcl0xJSZWgiJ5jp9WvnZrU/nkfnJlj+fVsm9y1tyFrWCYN3jY2GC+/dq5vf89akSt1UPuQ72ZV5CB8rT44M8fyytowXF7xQgI6S8+xUwB1iuuzr53jV4+cPN9b7tTz76YNvYTlp6av5rc+dM2a1b9vfMPGdZ9x+99zkd/otCJX6qD2A6/dLAIahK/VRQV03g3C4lbGZvG9s8v84WPP9fTcppdfPceR4/Pn/456vYhI++DtFfu+EPu45mecNFA6++x3Y2cNdaIVuVIHtQ956LwqsqjdE/MoK6Cz9hynJyeYffa73PO15yjoUgKZLS6t3f2yqLDs9BknfaO757HnEhdvdaIVuTLoggj5TgbhcnNFBTRkD8PWudxjFzZ46ZVzlQV8U/sagTxhmbTaFtZ+xmlTL9PaJlJnQxHyg/C1uqg2ZA3D9hLF987G712TpD00i5QUulkWGLW/P29p80SGfXO6bZtI3QxFyMPwfq1OmjKZpjUkr71ynPuPzfdcw08zdmFj3QIoINMCo6TVr82APzgzx8fuO8HmhPeSdBLTDBkJxdCEfNWqWh2ZtUdqwM+8+3I+NX31+dum3noJn/zTU5m/BTSNNjYAtiZcGyPGS6+cO/+azc/jgo0bMm3/kPT+mq/X+nnff2x+3dYMccGvGTISEoV8i6z7kGR5fKdpnGWVkrKUKCB+j/jpyQk++aenejp+Y4Nx503vANa+x5dfPbdu1e3S8kriN4akME97f3Gf96NPLay76HiefXREBp1CPpK1p93p8e0ngKQgiutxFtnDT5oymVZrj1vV2ksvvr0m3s10xyRJ5ZOsU0K1Z70Mm9ovhipK1gVTaY+PW2GbtJJ0xCzTcbNqXjLw4rb9ZZoDlHHiVrVmdfGFDb6677rE8EwK7Q3Guu0f0son7ZdEHLH0NbuqtcuwUchHsi5WSrs9aTAwbg/1lYS5jEXO7pienIidMpnUpvZA7aUtL71yLnU1adxqUVjdQweDsdHGuuvYJmnukf/MgffyesrcUNXaZRipXBPJulgp7fFp87Enovs3t8z+KHuh1pHj84mX/YtrU3ugZq3rAyy/7rGDpe1z9l9ZXllXNlpecS66YCMnbv+JTMdMa+uI2ZqThfaBl2Ghnnwk6z4kaY9PCuiJsdHzPc5mKaMf+5+klVtGbHUnzNY2tUvqdXcSV9tvLWN97+xy1+MC3Ur6PD/9wXeuCfiB2LBOpA8U8pH22m6nMkHa47MEd9bj9iItMFfcOwZcs41J9e6k2+Nq+90OkHb7TaZ9wzGg4+c5CBvWifRL6eUaM7sR+F/ACPAZdz9Q9jF7lXWWRdLjs65uLXt2R6dySzfL+Jv3xW3N8P53TXQ117zb3nm332SSZjjdedPV66ZJdtMOrXKVEJUa8mY2Avwu8OPAaeAbZnbU3Z8o87iDoJftB8qqEXczzbCbgIs7eV175fj5q0k1rwd78YUN3OFj953g4Mzc+ffSTW3fWL1CVTfvvdfr5g7ChnUi/VJ2T34H8LS7fwfAzO4FdgPBh3wWZa+GbQ3npJDtNuBaT17t7V5xP7+StXkhkdb30s3JJm4xVpwjx5OvYtXphDUIG9aJ9EvZNfkJ4PmW309Ht51nZnvMbNbMZhcWOv/jDlE/asTNaYa//aFrMg30pl1kI67dyyu+7kpRrb3r1np5kk4h3Ty5JOl0wurHOIjIoCi7Jx/3b3lNArj7IeAQwNTUVMUb4FajHzXi1nLQm0YbvKGxgcWzy6mloU7fMLK0r/nY1m8COw880lPZJG0At9seuVa5yrAoO+RPA5e1/L4FOFPyMWun7Bpxe1gvLi0z2hjhtz50TWrQJX3D+Pjhx1PbHad5ge3WMYdeyyZpJ5f2Hrnmw8uwK7tc8w1gm5ldYWbfB9wMHC35mLVT9lz5XstBSWHanHZ57ZXj69rdGLF12xIkXWAbOk93jJO2DqE94DUfXoZdqT15dz9nZh8GZlidQvlZd+9tO8OAFXHFpzeNNjAjtgTTazmo0w6Pjz61wJ03Xb2u3e3v5exr59ZtcNY8yaTtb5Ok228Avc6+EQlJ6fPk3f2LwBfLPk7d5b3iU+u2Be21817LQZ1mw5xZXEptdzPoi17V2u1JUfPhRbR3TW11Wj3a2mPttfbdDM2PH348diO1pJNE+wkoSZ4xh25OipoPL3VQ9riRtjWoqW56o60zWnqdMjg9OcGnP/jOTGMG3Wxf0I956UWMdaRNIRXJqx/jRurJ11Q3M1tae6x5pgxmHTNIOwFZ1K5+zHLJe/H0qi7ZKMOjH+NGCvma6lQvT+qx9vrVMMtJIukE1NyFs5/ynNw0cCtl68e4kco1NdVeghkbbXDxhekX2ujXlMJ+bJ/cDxq4lbKlXa+iKOrJ90FZAytZe6n96pnmLZMMCg3cStn6sY+SQr5kg1TX7WfPNIRtA7SRmZStHx0ihXzJBqmuq55pNqF8I5HBVnaHSCFfskGq66pnml0I30hkuCnkSzZIvecsPVNt7CUSBoV8yQat99zeM20u9mnfe2ZQxhHy0IlKRCFfukGu6yYNCl+wccPAjCP0apAGvEWqpJDvg0Gt6yYNCqdtSFYXgzTgLVIlLYYaYllDu06zcAZpwFukSgr5IZYU2hdf2Kj9itV+rCQUqQOF/BBL2n7g9p+8qvYXug5lawWRvFSTH2KdBoXrFOrtBnnAW6SfzGMuBlGVqakpn52drboZPdF0vXz0+Yn0zsyOuftU3H3qyRdA0/U6SwtxfX4i5VFNvgBp0/Wk8xbH+vxEyqOQL4Cm66XrFOL6/ETKo5AvgKbrpesU4vr8RMqjkC+Apuul6xTi+vxEyqOQL0D7pfjqOK+8TJ1CXJ+fSHlyTaE0sw8Avw78ELDD3Wdb7tsP3AqsAL/o7jOdXq/OUyglnaZIipSnzCmU3wJuAv5P2wHfBtwMXAVsBr5sZv/B3eN3vpLgtS9Oag66KuhFypUr5N39SQAza79rN3Cvu78KPGNmTwM7gL/OczypL82FF6lGWTX5CeD5lt9PR7etY2Z7zGzWzGYXFhZKao5UTXPhRarRsSdvZl8GfiDmrk+4+4NJT4u5Lbb47+6HgEOwWpPv1B6pJ82FF6lGx5B39xt6eN3TwGUtv28BzvTwOhKIQbrWrcgwKatccxS42cwuMLMrgG3A10s6ltSA5sKLVCPXwKuZ/SfgfwPjwBfM7IS773L3U2Z2GHgCOAfcppk1w01b/4pUQ1sNi4jUXNo8ea14FREJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYAp5EVEAqaQFxEJmEJeRCRgCnkRkYDlCnkzO2hmT5nZ35jZ58xsrOW+/Wb2tJnNmdmu3C0VEZHM8vbkHwLe7u7vAP4W2A9gZm8DbgauAm4Efs/MRnIeS0REMsoV8u7+JXc/F/36GLAl+nk3cK+7v+ruzwBPAzvyHEtERLIrsib/34E/i36eAJ5vue90dNs6ZrbHzGbNbHZhYaHA5oiIyMZODzCzLwM/EHPXJ9z9wegxnwDOAfc0nxbzeI97fXc/BBwCmJqain2MiIj0pmPIu/sNafeb2S3A+4Dr3b0Z0qeBy1oetgU402sjRUSkN3ln19wI/DLwU+5+tuWuo8DNZnaBmV0BbAO+nudYIiKSXceefAe/A1wAPGRmAI+5+8+7+ykzOww8wWoZ5zZ3X8l5LBERyShXyLv7v0+57w7gjjyvLyIi+WjFq4hIwBTyIiIBU8iLiARMIS8iEjCFvIhIwBTyIiIBU8iLiARMIS8iEjCFvIhIwBTyIiIBU8iLiARMIS8iEjCFvIhIwBTyIiIBU8iLiARMIS8iEjCFvIhIwPJe/k+kto4cn+fgzBxnFpfYPDbK3l3bmZ6cqLpZIoVSyMtQOnJ8nv0PnGRpefXSw/OLS+x/4CSAgl6ConKNDKWDM3PnA75paXmFgzNzFbVIpBwKeRlKZxaXMt0uUlcKeRlKm8dGM90uUlcKeRlKe3dtZ7Qxsua20cYIe3dtr6hFIuXQwKsMpebgqmbXSOhyhbyZ/U9gN/A68CLwX939THTffuBWYAX4RXefydlWkUJNT04o1CV4ecs1B939He5+DfB54NcAzOxtwM3AVcCNwO+Z2Ujiq4iISClyhby7/3PLrxcBHv28G7jX3V9192eAp4EdeY4lIiLZ5a7Jm9kdwH8B/j9wbXTzBPBYy8NOR7fFPX8PsAfg8ssvz9scERFp0bEnb2ZfNrNvxfy3G8DdP+HulwH3AB9uPi3mpTzmNtz9kLtPufvU+Ph4r+9DRERidOzJu/sNXb7WHwFfAG5nted+Wct9W4AzmVsnIiK5mHtsB7u7J5ttc/dvRz//AvBj7v6fzewqVkN/B7AZeBjY5u4rya8GZrYAPNtzg9a6FPjHgl6rKnV/D3VvP9T/PdS9/aD30I23untsKSRvTf6AmW1ndQrls8DPA7j7KTM7DDwBnANu6xTw0fMKq9eY2ay7TxX1elWo+3uoe/uh/u+h7u0HvYe8coW8u78/5b47gDvyvL6IiOSjbQ1ERAIWcsgfqroBBaj7e6h7+6H+76Hu7Qe9h1xyDbyKiMhgC7knLyIy9BTyIiIBCzLkzexGM5szs6fNbF/V7cnKzD5rZi+a2beqbksvzOwyM3vUzJ40s1Nm9pGq25SFmb3BzL5uZo9H7f9k1W3qlZmNmNlxM/t81W3phZn9nZmdNLMTZjZbdXuyMrMxM/sTM3sq+vfwI31vQ2g1+Wi3y78FfpzVlbffAH7a3Z+otGEZmNmPAi8Bf+Dub6+6PVmZ2SZgk7t/08z+HXAMmK7L34GZGXCRu79kZg3gr4CPuPtjHZ46cMzsl4Ap4Pvd/X1VtycrM/s7YMrda7kYyszuBv7S3T9jZt8HXOjui/1sQ4g9+R3A0+7+HXd/DbiX1V0xa8Pd/wL4btXt6JW7v+Du34x+/hfgSRI2qBtEvuql6NdG9F/tekNmtgV4L/CZqtsyjMzs+4EfBe4CcPfX+h3wEGbITwDPt/yeuAOmlM/MtgKTwNcqbkomUZnjBKsXw3nI3WvV/shvA/+D1RXpdeXAl8zsWLRjbZ38ILAA/N+oZPYZM7uo340IMeS73gFTymVmbwTuBz7adu2BgefuK9HFcLYAO8ysVmUzM3sf8KK7H6u6LTntdPcfBv4jcFtUyqyLjcAPA7/v7pPAy0DfxwhDDHntgDkAolr2/cA97v5A1e3pVfT1+iusXuGsTnYCPxXVtO8FrjOzP6y2Sdk1Lyfq7i8Cn6NeFx86DZxu+Rb4J6yGfl+FGPLfALaZ2RXRQMfNwNGK2zRUooHLu4An3f03q25PVmY2bmZj0c+jwA3AU5U2KiN33+/uW9x9K6v/Bh5x95+tuFmZmNlF0cA9UZnjJ4DazDhz978Hno82cQS4ntVNG/sq95WhBo27nzOzDwMzwAjwWXc/VXGzMjGzPwbeA1xqZqeB2939rmpblclO4OeAk1FdG+BX3P2L1TUpk03A3dFMrQ3AYXev5RTEmnsL8LnVPgMbgT9y9z+vtkmZ/QJwT9Th/A7w3/rdgOCmUIqIyL8JsVwjIiIRhbyISMAU8iIiAVPIi4gETCEvIhIwhbyISMAU8iIiAftXqagfUQ/nPlsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(pred, aggregated_reg_table[\"future_gdp_per_cap_growth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "028abcdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-30.856814197901453, 35.81370408617056)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "65f27c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023212796599265423"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(pred, aggregated_reg_table[\"future_gdp_per_cap_growth\"])[0,1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d473a3",
   "metadata": {},
   "source": [
    "# Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "c60748d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 12])\n",
      "torch.Size([99, 12])\n",
      "torch.Size([108, 12])\n",
      "torch.Size([108, 12])\n",
      "torch.Size([108, 12])\n",
      "torch.Size([108, 12])\n",
      "torch.Size([110, 12])\n",
      "torch.Size([112, 12])\n",
      "torch.Size([114, 12])\n",
      "torch.Size([119, 12])\n",
      "torch.Size([119, 12])\n",
      "torch.Size([121, 12])\n",
      "torch.Size([130, 12])\n",
      "torch.Size([133, 12])\n",
      "torch.Size([136, 12])\n",
      "CPU times: user 4min 4s, sys: 22 s, total: 4min 26s\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "years = range(1970,1985)\n",
    "\n",
    "val_years = [1983]\n",
    "test_years = [1984]\n",
    "\n",
    "train_graphs = []\n",
    "val_graphs = []\n",
    "test_graphs = []\n",
    "i = 0\n",
    "\n",
    "for year in tqdm(years):\n",
    "    print(str(year), end='\\r')\n",
    "    \n",
    "    trade = TradeNetwork(year = year)\n",
    "    trade.prepare_features()\n",
    "    trade.prepare_network()\n",
    "    trade.graph_create(node_features = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth',\n",
    "    'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "       'resource_8', 'resource_9'],\n",
    "        node_labels = 'future_gdp_per_cap_growth')\n",
    "    \n",
    "    if(year in val_years):\n",
    "        val_graphs.append(trade.pyg_graph)\n",
    "    elif(year in test_years):\n",
    "        test_graphs.append(trade.pyg_graph)\n",
    "    else: \n",
    "        train_graphs.append(trade.pyg_graph)\n",
    "        \n",
    "    trade.features[\"year\"] = year\n",
    "    \n",
    "    if(i == 0):\n",
    "        trade_df = trade.features\n",
    "    else: \n",
    "        trade_df = trade_df.append(trade.features)\n",
    "        \n",
    "    i = i+1\n",
    "    print(trade.node_attributes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "7fc56ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = data.Batch().from_data_list(test_graphs)\n",
    "val_batch = data.Batch().from_data_list(val_graphs)\n",
    "train_batch = data.Batch().from_data_list(train_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "b980d51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'country_code', 'prev_gdp_growth', 'current_gdp_growth',\n",
       "       'future_gdp_growth', 'prev_gdp', 'current_gdp', 'prev_gdp_per_cap',\n",
       "       'current_gdp_per_cap', 'future_gdp_per_cap', 'prev_gdp_per_cap_growth',\n",
       "       'current_gdp_per_cap_growth', 'future_gdp_per_cap_growth', 'resource_0',\n",
       "       'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5',\n",
       "       'resource_6', 'resource_7', 'resource_8', 'resource_9', 'node_numbers',\n",
       "       'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "5ff03b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                OLS Regression Results                               \n",
      "=====================================================================================\n",
      "Dep. Variable:     future_gdp_per_cap_growth   R-squared:                       0.081\n",
      "Model:                                   OLS   Adj. R-squared:                  0.080\n",
      "Method:                        Least Squares   F-statistic:                     75.94\n",
      "Date:                       Wed, 14 Dec 2022   Prob (F-statistic):           2.48e-32\n",
      "Time:                               17:05:12   Log-Likelihood:                -5484.0\n",
      "No. Observations:                       1724   AIC:                         1.097e+04\n",
      "Df Residuals:                           1721   BIC:                         1.099e+04\n",
      "Df Model:                                  2                                         \n",
      "Covariance Type:                   nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      1.0653      0.151      7.053      0.000       0.769       1.362\n",
      "current_gdp_per_cap_growth     0.2545      0.023     10.928      0.000       0.209       0.300\n",
      "prev_gdp_per_cap_growth        0.0562      0.023      2.434      0.015       0.011       0.102\n",
      "==============================================================================\n",
      "Omnibus:                      191.170   Durbin-Watson:                   1.891\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1595.220\n",
      "Skew:                          -0.111   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.707   Cond. No.                         8.26\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "result = sm.ols(formula='future_gdp_per_cap_growth~ current_gdp_per_cap_growth+prev_gdp_per_cap_growth', \n",
    "             data=trade_df).fit()\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c98d9de",
   "metadata": {},
   "source": [
    "# PyG Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "4d71ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv, GATConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "6c331ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_batch.transform = T.NormalizeFeatures()\n",
    "#val_batch.transform = T.NormalizeFeatures()\n",
    "#train_batch.transform = T.NormalizeFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "89819f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_edge_features,\n",
    "                 hidden_dim = 10, num_heads = 40, out_feats = 10,drop = 0.2):\n",
    "        super(GNN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_edge_features = num_edge_features\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.out_feats = out_feats\n",
    "        self.drop = drop\n",
    "        \n",
    "        \n",
    "        self.conv1 = GCNConv(self.num_features, self.out_feats, add_self_loops = False, bias = False)\n",
    "        \n",
    "        #self.conv1 = GATConv(self.num_features, self.out_feats, heads=self.num_heads, \n",
    "        #                       dropout=self.drop, edge_dim = self.num_edge_features, \n",
    "        #                     bias = False, add_self_loops = True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.num_features, 1)\n",
    "        \n",
    "        #self.linear2= nn.Linear(self.out_feats*self.num_heads + 1,1, bias = True)\n",
    "        self.linear2= nn.Linear(self.out_feats,1, bias = True)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        #x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        #x = F.dropout(x, p=self.drop, training=self.training)\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        #x = F.elu(x)\n",
    "        #x = F.dropout(x, p=self.drop, training=self.training)\n",
    "        #x = self.conv2(x, edge_index)\n",
    "        #x2 = self.linear1(x2)\n",
    "        #self.linear2(torch.cat((x2,x),1))\n",
    "        return self.linear2(x)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.linear2.reset_parameters()\n",
    "        self.conv1.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "733a040e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4000/4000 [02:31<00:00, 26.43it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction = \"mean\")\n",
    "model = GNN(num_features = train_batch.num_features, \n",
    "            num_edge_features = train_batch.num_edge_features).to(device)\n",
    "#model = GNN(num_features = train_batch.num_features, \n",
    "#            num_edge_features = train_batch.num_edge_features).to(device)\n",
    "model.reset()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "train_mse = []\n",
    "valid_mse = []\n",
    "best_val_mse = np.inf \n",
    "best_model = None\n",
    "i = 0 \n",
    "model.train()\n",
    "\n",
    "for epoch in trange(4000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model.forward(train_batch.x, train_batch.edge_index, train_batch.edge_attr)\n",
    "    #print(out)\n",
    "    loss = loss_fn(out.to(torch.float).flatten(), train_batch.y)\n",
    "    train_mse.append(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #validation\n",
    "    model.eval()\n",
    "    out_val = model(val_batch.x, val_batch.edge_index, val_batch.edge_attr)\n",
    "    current_val_mse = loss_fn(out_val.to(torch.float).flatten(), val_batch.y).item()\n",
    "    valid_mse.append(current_val_mse)\n",
    "    \n",
    "    #print(model.linear1.weight)\n",
    "    \n",
    "   # # early stopping \n",
    "    if current_val_mse < best_val_mse:\n",
    "        best_val_mse = current_val_mse\n",
    "        best_weights = model.state_dict()\n",
    "        best_iteration = i\n",
    "    \n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd21be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "2a63f5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "0782af8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7fb0555df0>]"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAenklEQVR4nO3deXRc5Znn8e9TixZrsWRLMt53wASwwY6BsBtCjEkgZ9JZyNLMhMSTBLqBydJkmZ5OnzM9SbozgXSSCVsISUhoCEmHEBIg7JuNF2yzGS/yJgSWvMi2FmupeuaPurIluSRLtqQq3fp9zqlT9966b92nrsu/evXeW7fM3RERkXCLZLoAEREZegp7EZEcoLAXEckBCnsRkRygsBcRyQGxTBeQTkVFhU+bNi3TZYiIjBirVq3a5e6VvT2elWE/bdo0Vq5cmekyRERGDDPb1tfj/R7GMbOomb1iZg8H82PM7HEz2xjcl/fSbrGZvWVmm8zs5oGVLyIig2EgY/Y3AG92mb8ZeMLdZwNPBPPdmFkU+DFwOXAKcLWZnXLs5YqIyLHoV9ib2STgCuDOLouvAu4Jpu8BPpym6UJgk7tXu3sbcF/QTkREhlF/e/a3AF8Dkl2WjXP3dwCC+6o07SYCO7rM1wTLjmBmS81spZmtrK+v72dZIiLSH0cNezP7IFDn7quO4fktzbK0F+Nx99vdfYG7L6is7PWAsoiIHIP+nI1zLnClmS0BCoBSM/sVsNPMxrv7O2Y2HqhL07YGmNxlfhJQe7xFi4jIwBy1Z+/uX3f3Se4+DfgE8KS7fxp4CLgmWO0a4A9pmq8AZpvZdDPLC9o/NCiVi4hIvx3PN2i/A7zfzDYC7w/mMbMJZvYIgLt3ANcDj5I6k+d+d3/9+Eru3Q+f2MgzGzTeLyLS04C+VOXuTwNPB9O7gUvSrFMLLOky/wjwyPEU2V8/fWYzn1w4hQtP1Ji/iEhXobo2Tl4sQlsiefQVRURyTLjCPhqhrUNhLyLSU6jCPq6wFxFJK1Rhnx+L0KphHBGRI4Qq7PNiEdrVsxcROULowl4HaEVEjhSusNeYvYhIWuEK+5jCXkQknfCFvYZxRESOEK6w1zCOiEha4Qp7DeOIiKQVrrCPRmhV2IuIHCFcYR+L0K4xexGRI4Qu7HWAVkTkSOEKex2gFRFJK1xhrwO0IiJphS7sO5JOMpn2N81FRHJW6MIe0Li9iEgP4Qr7aOrl6PRLEZHuwhX2nT17hb2ISDfhCvuohnFERNIJV9gHPXv9gImISHehDHv17EVEugtX2Ec1Zi8ikk64wj6ms3FERNIJZdirZy8i0l2owj5fY/YiImmFKuzjGrMXEUkrVGGvYRwRkfTCFfZBz14/YCIi0l24wl49exGRtEIZ9q3q2YuIdBOqsM+PRgH17EVEegpV2GsYR0QkPYW9iEgOCFXYRyNGNGK0JRKZLkVEJKuEKuwhdfqlevYiIt3FjraCmRUAzwL5wfq/dff/ZWZzgZ8CxcBW4FPuvj9N+63AASABdLj7gkGrPo38eISD7Qp7EZGu+tOzbwUWuftcYB6w2MzOBu4Ebnb304DfA1/t4zkudvd5Qx30AAWxKAfbNYwjItLVUcPeUxqD2Xhwc+AkUj1+gMeBjwxJhQNUEI9wUMM4IiLd9GvM3syiZrYGqAMed/flwGvAlcEqHwUm99LcgcfMbJWZLe1jG0vNbKWZrayvr+/3C+ipIK6evYhIT/0Ke3dPuPs8YBKw0MxOBT4LXGdmq4ASoK2X5ue6+5nA5cH6F/SyjdvdfYG7L6isrBzo6zgkX2EvInKEAZ2N4+4NwNPAYndf7+6Xuft84DfA5l7a1Ab3daTG9hceT8FHUxCL0KoDtCIi3Rw17M2s0szKgulC4FJgvZlVBcsiwLdInZnTs22RmZV0TgOXkRr+GTIF8SgHO9SzFxHpqj89+/HAU2a2DlhBasz+YeBqM9sArAdqgbsBzGyCmT0StB0HPG9ma4GXgT+5+18G+0V0VRCPaBhHRKSHo55n7+7rgDPSLL8VuDXN8lpgSTBdDcw9/jL7L3WAVsM4IiJdhe4btDrPXkTkSOELew3jiIgcIYRhH9WXqkREeghd2OfHo7R1JEkmPdOliIhkjdCFfUE8+GlC9e5FRA4JX9jHUj9NqHF7EZHDwhf28SDs9cUqEZFDQhj2qZekc+1FRA4LYdhrGEdEpKcQhn1nz15hLyLSKXxhf+gArYZxREQ6hS7s83WAVkTkCKEL+0Pn2WsYR0TkkBCGvYZxRER6CnHYq2cvItIpfGEf09k4IiI9hS/sDx2g1TCOiEin0IV9YRD2zW3q2YuIdApd2Ecixqi8KC1tHZkuRUQka4Qu7AFG5cVoUs9eROSQUIZ9UX6U5lb17EVEOoUy7NWzFxHpLqRhH6VZY/YiIoeENuybWtWzFxHpFMqwL8qLqWcvItJFKMN+VL569iIiXYUy7NWzFxHpLpRhPyo/qm/Qioh0EcqwL8qL0dqRpCOh6+OIiEBIw35UXnB9HF35UkQECGnYF+XHAGjWQVoRESCkYd/Zs2/SQVoRESCkYV+Up569iEhXoQz7Ufnq2YuIdBXOsO/s2SvsRUSAkIZ9SUEq7A8cVNiLiEBIw760IA7AfoW9iAjQj7A3swIze9nM1prZ62b27WD5XDN7ycxeNbM/mllpL+0Xm9lbZrbJzG4e7BeQTmfPfn9L+3BsTkQk6/WnZ98KLHL3ucA8YLGZnQ3cCdzs7qcBvwe+2rOhmUWBHwOXA6cAV5vZKYNUe68K4lHyYxGFvYhI4Khh7ymNwWw8uDlwEvBssPxx4CNpmi8ENrl7tbu3AfcBVx131f1QWhhn/0GFvYgI9HPM3syiZrYGqAMed/flwGvAlcEqHwUmp2k6EdjRZb4mWJZuG0vNbKWZrayvr+9n+b0rLYixv0Vj9iIi0M+wd/eEu88DJgELzexU4LPAdWa2CigB2tI0tXRP18s2bnf3Be6+oLKysl/F90U9exGRwwZ0No67NwBPA4vdfb27X+bu84HfAJvTNKmhe49/ElB7bKUOzOjCuMbsRUQC/Tkbp9LMyoLpQuBSYL2ZVQXLIsC3gJ+mab4CmG1m080sD/gE8NAg1d6n0oK4Tr0UEQn0p2c/HnjKzNaRCu/H3f1hUmfWbADWk+qt3w1gZhPM7BEAd+8ArgceBd4E7nf31wf/ZRyptDCmnr2ISCB2tBXcfR1wRprltwK3plleCyzpMv8I8MjxlTlwpQVx9rW04+6YpTt0ICKSO0L5DVpIHaDtSDot+gETEZEQh33nJRN0+qWISHjDfnRhKuwbWtKdESoikltCG/Zji/MA2NOosBcRCW3YVwRhv6tJYS8iEtqwH1uUD8DuxtYMVyIiknmhDfvRhXGiEWO3hnFERMIb9pGIMaYoj91N6tmLiIQ27AHGFuWxSz17EZFwh31Fcb7G7EVECHnYjy3OY7fOxhERCXfYVxTnU7e/Ffe0l9AXEckZoQ77CWWFtLQnaGjW1S9FJLeFOuwnlhUA8HZDS4YrERHJrJCH/ShAYS8iEuqwnxD07GsV9iKS40Id9mOK8siPRRT2IpLzQh32ZsbEskJq9irsRSS3hTrsAaZVFLFlV1OmyxARyajQh/3sqmKq65voSCQzXYqISMaEPuxnVRXTlkiyQ0M5IpLDciLsATbuPJDhSkREMidnwv6tdxX2IpK7Qh/2JQVxZlYWsWZHQ6ZLERHJmNCHPcCZU8p5ZUeDLogmIjkrN8J+ajl7mtrYtrs506WIiGREToT9/KnlACyr3p3hSkREMiMnwn52VTETRhfw5Pq6TJciIpIRORH2ZsaiOVU8v2kXrR2JTJcjIjLsciLsAS45eRzNbQme27Ar06WIiAy7nAn782ZXUFGcx29X1WS6FBGRYZczYR+PRvjwvIk8sX4ne/Qj5CKSY3Im7AE+9t7JtCec37y8PdOliIgMq5wK+xPHlXDhiZXc/cIWDrbrQK2I5I6cCnuAL1w4k12NbTywckemSxERGTY5F/ZnzxjD/Knl/PDJTTS1dmS6HBGRYZFzYW9mfGPJHOoPtHLbs9WZLkdEZFgcNezNrMDMXjaztWb2upl9O1g+z8yWmdkaM1tpZgt7ab/VzF7tXG+wX8CxmD+1nCtOH8/tz25mu66XIyI5oD89+1ZgkbvPBeYBi83sbOB7wLfdfR7wj8F8by5293nuvuA46x0031wyh1gkwtceXEsyqathiki4HTXsPaUxmI0HNw9upcHy0UDtkFQ4RCaUFfKtK+awrHoPv1y2LdPliIgMqX6N2ZtZ1MzWAHXA4+6+HLgR+Fcz2wH8G/D1Xpo78JiZrTKzpX1sY2kwHLSyvr5+IK/hmH38vZO56KRK/vef3tSPm4hIqPUr7N09EQzXTAIWmtmpwBeBm9x9MnATcFcvzc919zOBy4HrzOyCXrZxu7svcPcFlZWVA30dx8TM+MHH5lFZks8Xf7WK+gOtw7JdEZHhNqCzcdy9AXgaWAxcA/wueOgBIO0BWnevDe7rgN/3tl6mlBflcdtn5rO3uY3/evfL7D/YnumSREQGXX/Oxqk0s7JguhC4FFhPaoz+wmC1RcDGNG2LzKykcxq4DHhtUCofRKdOHM1PPz2fDTsPcO3PV+j8exEJnf707McDT5nZOmAFqTH7h4HPA983s7XAvwBLAcxsgpk9ErQdBzwfrPMy8Cd3/8tgv4jBcNFJVdzy8TNYvb2Bq+9Yxq5GDemISHhYNv4I94IFC3zlysyckv/Emzu57terGVdawO2fWcBJJ5RkpA4RkYEws1V9nd6ec9+gPZpL5ozj3s+dTVNrgqt+/DwP6vr3IhICCvs05k8t55G/P4+5k8r48gNrue7e1TpTR0RGNIV9L6pKC7j3c2fxlctO5PE3dvL+HzzD/St26Nu2IjIiKez7EItGuH7RbB654TxmVRbztQfX8aEfPc+Lm/U7tiIysijs+2FWVQkPfOEcfnj1GTQ0t/PJO5bzmbuWs2rbnkyXJiLSLzobZ4AOtif4xUtbue2ZanY3tXHurLH8/aLZnDVjbKZLE5EcdrSzcRT2x6i5rYNfL9/OT5+pZldjK2dOKePz58/gsvecQDRimS5PRHKMwn6IHWxP8B8rdnDX81vYvqeZKWNGce150/nogkmMyotlujwRyREK+2GSSDqPvf4udzxXzertDYwujPPps6dwzTnTqCotyHR5IhJyCvsMWLVtD3c8u4VH33iXWMS4cu5EPnf+dOaMLz16YxGRY3C0sNc4wxCYP3UM8z8zhq27mrj7hS3cv7KGB1fXcP7sCj53/gwumF2Bmcb1RWT4qGc/DBqa27h3+XbueXErdQdaOWlcCdeeP52r5k0gPxbNdHkiEgIaxskibR1J/ri2ljueq2b9uweoKM7nmnOm8umzp1JelJfp8kRkBFPYZyF354VNu7njuWqe2VBPQTzC38yfxLXnzWB6RVGmyxOREUhj9lnIzDhvdgXnza5gw84D3PXcFu5fUcO9y7dz6ZxxfOmimZwxpTzTZYpIiKhnnyXqD7Tyy5e28otl22hobufcWWO57qJZnDNzrA7mishRaRhnhGlqTX0z9/bnqqk/0MoZU8q4/uJZLDq5SqEvIr1S2I9QB9sTPLCqhtue2UzN3hZOPqGE6y6exZLTxutyDCJyBIX9CNeeSPLQmlp+8vQmNtc3MauqmBsvnc2SU8cTUeiLSEBhHxLJpPPIa+9w6183srGukZPGlXDT+2dz2SknKPRFRL9BGxaRiPHB0yfwlxsv4NZPzKM9keQLv1rNB//9eR5/YyfZ+KEtItlDYT/CRCPGVfMm8thNF/B/PzaXprYOPv+LlVz5oxd4an2dQl9E0tIwzgjXkUjyu1fe5odPbKRmbwvzJpfxlctO4rzZFZkuTUSGkYZxQi4WjfCxBZN56isX8Z3/chr1B1r59F3L+dSdy1i7oyHT5YlIllDPPmRaOxLcu2w7P3pqE3ua2rj81BP48mUnMauqONOlicgQ0tk4OaqxtYM7n6vmjmeraWlP8NH5k7nh0tlMKCvMdGkiMgQU9jlud2MrP35qM79atg0MrjlnKl+6aJausikSMgp7AaBmbzO3/HUjv1tdQ1FejKUXzOCz502nKF/XwhMJA4W9dLNh5wH+7dG3eOyNnVQU5/F3i2Zz9cIp5MV0rF5kJFPYS1qrt+/lu39ez/Ite5g8ppAbLzmRD58xUdfdERmhdOqlpHXmlHLuW3o293x2IaUFcb78wFoW3/Isf371HX0xSySEFPY5zMy48MRK/nj9efzkU2fiwBfvXc2HfvQ8T72lb+OKhInCXohEjCWnjefRGy/g+x+dy76Wdv7b3Sv42G0vsbx6d6bLE5FBoDF7OUJbR5L7V+7g35/cyM79rZw/u4KvfuAkTp9UlunSRKQXOkArx+xge4JfvrSNnzy9ib3N7XzgPeP48mUnceK4kkyXJiI9KOzluDW2dvCz57dwx7PVNLZ1cNXcCdxw6YlMryjKdGkiElDYy6DZ29TGbc9W8/MXt9DWkeSK0yfwpYtmMmd8aaZLE8l5x33qpZkVmNnLZrbWzF43s28Hy+eZ2TIzW2NmK81sYS/tF5vZW2a2ycxuPvaXIplWXpTHzZefzHNfW8R/v3AmT62v4/Jbn+Pan69g9fa9mS5PRPpw1J69mRlQ5O6NZhYHngduAP4Z+IG7/9nMlgBfc/eLerSNAhuA9wM1wArgand/o69tqmc/MuxrbucXL23lZy9sYW9zO+fMGMv1i2bxvpljSb1tRGS4HHfP3lMag9l4cPPg1vn3+2igNk3zhcAmd6929zbgPuCqAdQvWWz0qDh/d8lsnv+HRXzrijlU72rkU3cu58M/eZGH19XSkUhmukQRCfRrzD7ooa8CZgE/dvd/MLM5wKOAkfrQeJ+7b+vR7m+Axe7+uWD+M8BZ7n59mm0sBZYCTJkyZf62bdt6riJZrrUjwYOr3ub2ZzezdXcz40cX8LfnTOPqhZMpG6WrbIoMpUG5XIK7J9x9HjAJWGhmpwJfBG5y98nATcBd6baf7ul62cbt7r7A3RdUVlb2pyzJMvmxKJ88awpPfvki7rpmATMqi/juX9Zz9v95gm/8/lU27jyQ6RJFctaArm/r7g1m9jSwGLiG1Ng9wAPAnWma1ACTu8xPIv1wj4RIJGJcMmccl8wZx/p39/PzF7by21U1/Hr5dhZMLecTC6dwxWnjKcyLZrpUkZzRn7NxKs2sLJguBC4F1pMK7QuD1RYBG9M0XwHMNrPpZpYHfAJ4aBDqlhHi5BNK+c5HTmfZ1y/hG0tOZk9TG195YC0L/+Wv/M//fI3Xa/dlukSRnNCfnv144J5g3D4C3O/uD5tZA3CrmcWAgwTj7WY2AbjT3Ze4e4eZXU9qbD8K/MzdXx+KFyLZbUxRHksvmMnnz5/By1v28JuXt/MfK3fwy2XbOPmEEq6aN5EPzR3PpPJRmS5VJJT0pSrJmIbmNv6wppY/rHmb1dsbAHjvtHKunDuBJaeNZ2xxfmYLFBlB9A1aGRG2727mj+tq+c9X3mZjXSPRiLFw2hguPWUcl86pYupYXZpBpC8KexlR3J317x7gj2tr+eubO9mwM/UVj9lVxYeCf97kcv2ilkgPCnsZ0bbvbuavb+7kifU7WV69h46kM6Yoj/fNHMu5syo4d2YFU8ZqnF9EYS+hsa+lnWc21PP0+jpe2LyLnftbAZhUXsi5MytYMK2c+VPLmV5RpMs1SM5R2EsouTub65t4cfMuXti0i2XVe9jX0g5A+ag4Z0xJBf8ZU8qYO6mMovwBfaVEZMQ5Wtjrf4CMSGbGrKpiZlUV87fnTCOZdDbXN7J6+15WbdvL6u0NPLm+LlgXpo0t4pQJpZwyvpRTJpTynvGlVJUWZPhViAwfhb2EQiRizB5XwuxxJXz8vVOA1Kmdr2xv4NW39/FG7X7W1TTwp3XvHGpTUZzHzMpiZlYVM7OymBmVRcyqLGZCWaEOAEvoKOwltMpG5XHxyVVcfHLVoWX7WtpZ/85+Xq/dz5vv7GdzfSN/WvfOoSEggLxYhBkVRUwbW8Sk8kImlRcysXxUcF9IaUE8Ey9H5Lgo7CWnjC6Mc9aMsZw1Y+yhZe7OnqY2Ntc3UV3fyOb6Rqrrm9hQd4Cn3qqjtaP7pZpLC2JMLB/FhNEFVJXmU1mcT2VJl1txAZUl+br2j2QVhb3kPDNjbHE+Y4vzWTh9TLfH3J3dTW3U7G3h7b0t1Oxt5u2GFmr2tlC77yDr3t7H7sZWkmnOcyjOj1FZkk/5qDhlo/IoK4xTWhinbFScssLUstHBspKCGEX5MYryohTlx4hH+3VBWpF+U9iL9MHMqCjOp6I4n3mTy9Kuk0im/jKoP9BKfWNr6j641R04SENzO3UHDrKx7gANze0cONhx1O3mxSIU58cYlRelOD/4IAg+DEblxSiIRyiIR1P3seih6fx4MB1LPZ4XixCPRsiLRoJpIx6NkB8sj8dSj8WjptNVQ05hL3KcohE7NITTHx2JJPsPdtDQ3EZDSzv7mttpbO2gua2DxtYETa0dqVtbB02tCRqD+X0t7dQ2tNDSluBge3DrSJJI92fFMYhHLRX8wQdBLGJEI9blPpK6j6ZfHj1ifSMaCZ4n2svyzvmoEbWu871tP/UBHDEjYqkD84emj1ieWjcaLLdgnWiXxyJGMH/48YgZkUi65zu8HTO6Pe9I+KBU2IsMs1g0wpiiPMYUDc6vd7UnkkH4p+5bOw5PtyWStCec9o5kMJ2krXO6I/VY2xHLUtOJpNOR9MP3ic75ZLflLe2Jw8sTqeUJDx5PdGnfo10iuIVBz/Dv/FA5/AES3Ed6fDB1+VAxg4qifO7/wjlDUqPCXmSEi0dTPfGSEfi1AXc/4kMl2W3+8IdOR8JJeurmnho+S80ffp7O6aRD4tC6TjLJobZJ5/B9svuy1PMcnk522Y4H6yQ6p5Ndn6vHcye7PmfQrlt9qe14j3YlBUMXyQp7EckYs9SwUEwnLg05HfIXEckBCnsRkRygsBcRyQEKexGRHKCwFxHJAQp7EZEcoLAXEckBCnsRkRyQlT9LaGb1wLZjbF4B7BrEcgZLttYF2VtbttYF2Vub6hq4bK1toHVNdffK3h7MyrA/Hma2sq/fYcyUbK0Lsre2bK0Lsrc21TVw2VrbYNelYRwRkRygsBcRyQFhDPvbM11AL7K1Lsje2rK1Lsje2lTXwGVrbYNaV+jG7EVE5Ehh7NmLiEgPCnsRkRwQmrA3s8Vm9paZbTKzmzOw/a1m9qqZrTGzlcGyMWb2uJltDO7Lu6z/9aDWt8zsA4Ncy8/MrM7MXuuybMC1mNn84DVtMrMf2nH+0GYvdf2Tmb0d7Lc1ZrZkuOsKnnOymT1lZm+a2etmdkOwPKP7rY+6MrrfzKzAzF42s7VBXd8OlmfD+6y32rLlvRY1s1fM7OFgfnj2mQc/vzWSb0AU2AzMAPKAtcApw1zDVqCix7LvATcH0zcD3w2mTwlqzAemB7VHB7GWC4AzgdeOpxbgZeAcwIA/A5cPQV3/BHwlzbrDVlfwnOOBM4PpEmBDUENG91sfdWV0vwXPURxMx4HlwNmZ3l9HqS1b3mv/A/g18PBw/t8MS89+IbDJ3avdvQ24D7gqwzVBqoZ7gul7gA93WX6fu7e6+xZgE6nXMCjc/Vlgz/HUYmbjgVJ3f8lT765fdGkzmHX1ZtjqCmp7x91XB9MHgDeBiWR4v/VRV2+Gqy5398ZgNh7cnOx4n/VWW2+GrTYzmwRcAdzZY/tDvs/CEvYTgR1d5mvo+z/EUHDgMTNbZWZLg2Xj3P0dSP2nBaqC5Zmod6C1TAymh6PG681snaWGeTr/hM1YXWY2DTiDVI8wa/Zbj7ogw/stGI5YA9QBj7t71uyvXmqDzL/XbgG+BiS7LBuWfRaWsE83XjXc55Se6+5nApcD15nZBX2smw31duqtluGq8f8BM4F5wDvA9zNZl5kVAw8CN7r7/r5W7aWOIakvTV0Z32/unnD3ecAkUj3OU/tYfVj3Vy+1ZXSfmdkHgTp3X9XfJoNZV1jCvgaY3GV+ElA7nAW4e21wXwf8ntSwzM7gTy6C+7pg9UzUO9BaaoLpIa3R3XcG/zGTwB0cHs4a9rrMLE4qUO91998FizO+39LVlU37zd0bgKeBxWTB/uqttizYZ+cCV5rZVlJDzYvM7FcM1z473oMN2XADYkA1qYMYnQdo3zOM2y8CSrpMv0jqjf+vdD/w8r1g+j10P/BSzSAeoA22MY3uB0IHXAuwgtSBrc6DQEuGoK7xXaZvIjVGmYm6jNTY5y09lmd0v/VRV0b3G1AJlAXThcBzwAczvb+OUltWvNeC572Iwwdoh2WfDVq4ZPoGLCF1psJm4JvDvO0ZwT/KWuD1zu0DY4EngI3B/Zgubb4Z1PoWg3CEv0c9vyH1Z2o7qV7AtcdSC7AAeC147EcE37ge5Lp+CbwKrAMe6vEfcljqCp7zPFJ/Cq8D1gS3JZneb33UldH9BpwOvBJs/zXgH4/1PT8E77PeasuK91rwvBdxOOyHZZ/pcgkiIjkgLGP2IiLSB4W9iEgOUNiLiOQAhb2ISA5Q2IuI5ACFvYhIDlDYi4jkgP8PTL/7nVVVKlMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_mse[40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "aa0b632d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7f9f8a9730>]"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "9688cf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1373438/387021926.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[\"pred\"] = valid_out_list\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(best_weights)\n",
    "out_val = model(val_batch.x, val_batch.edge_index, val_batch.edge_attr)\n",
    "valid_out_list = [item for sublist in out_val.tolist() for item in sublist]\n",
    "valid_df = trade_df[trade_df.year.isin(val_years)]\n",
    "valid_df[\"pred\"] = valid_out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "7317fa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>country_code</th>\n",
       "      <th>prev_gdp_growth</th>\n",
       "      <th>current_gdp_growth</th>\n",
       "      <th>future_gdp_growth</th>\n",
       "      <th>prev_gdp</th>\n",
       "      <th>current_gdp</th>\n",
       "      <th>prev_gdp_per_cap</th>\n",
       "      <th>current_gdp_per_cap</th>\n",
       "      <th>future_gdp_per_cap</th>\n",
       "      <th>...</th>\n",
       "      <th>resource_3</th>\n",
       "      <th>resource_4</th>\n",
       "      <th>resource_5</th>\n",
       "      <th>resource_6</th>\n",
       "      <th>resource_7</th>\n",
       "      <th>resource_8</th>\n",
       "      <th>resource_9</th>\n",
       "      <th>node_numbers</th>\n",
       "      <th>year</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>AGO</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.200001</td>\n",
       "      <td>6.000002</td>\n",
       "      <td>-0.344494</td>\n",
       "      <td>-0.316559</td>\n",
       "      <td>619.959903</td>\n",
       "      <td>623.440047</td>\n",
       "      <td>637.715098</td>\n",
       "      <td>...</td>\n",
       "      <td>9.669948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.007459</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.024973</td>\n",
       "      <td>0</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.480781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>AND</td>\n",
       "      <td>1.246461</td>\n",
       "      <td>1.770118</td>\n",
       "      <td>1.784687</td>\n",
       "      <td>-1.235474</td>\n",
       "      <td>-1.267204</td>\n",
       "      <td>9610.020616</td>\n",
       "      <td>8025.207641</td>\n",
       "      <td>7728.906695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055203</td>\n",
       "      <td>0.003099</td>\n",
       "      <td>0.094196</td>\n",
       "      <td>2.465849</td>\n",
       "      <td>2.272600</td>\n",
       "      <td>1.914061</td>\n",
       "      <td>0.124961</td>\n",
       "      <td>1</td>\n",
       "      <td>1983</td>\n",
       "      <td>-0.063039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ARE</td>\n",
       "      <td>-6.719316</td>\n",
       "      <td>-4.745821</td>\n",
       "      <td>4.016951</td>\n",
       "      <td>0.359799</td>\n",
       "      <td>0.346322</td>\n",
       "      <td>40025.822624</td>\n",
       "      <td>34843.159626</td>\n",
       "      <td>32309.832713</td>\n",
       "      <td>...</td>\n",
       "      <td>9.244663</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.028120</td>\n",
       "      <td>0.349803</td>\n",
       "      <td>0.124311</td>\n",
       "      <td>0.102205</td>\n",
       "      <td>0.038545</td>\n",
       "      <td>2</td>\n",
       "      <td>1983</td>\n",
       "      <td>1.248557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>ARG</td>\n",
       "      <td>-0.735659</td>\n",
       "      <td>4.349093</td>\n",
       "      <td>1.570739</td>\n",
       "      <td>0.555838</td>\n",
       "      <td>0.640285</td>\n",
       "      <td>2927.897357</td>\n",
       "      <td>3553.377509</td>\n",
       "      <td>2659.708242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080409</td>\n",
       "      <td>0.719742</td>\n",
       "      <td>0.575831</td>\n",
       "      <td>0.959789</td>\n",
       "      <td>0.516924</td>\n",
       "      <td>0.132742</td>\n",
       "      <td>0.047231</td>\n",
       "      <td>3</td>\n",
       "      <td>1983</td>\n",
       "      <td>1.429152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>ATG</td>\n",
       "      <td>-0.084012</td>\n",
       "      <td>5.364016</td>\n",
       "      <td>10.164996</td>\n",
       "      <td>-1.509220</td>\n",
       "      <td>-1.461880</td>\n",
       "      <td>2660.558091</td>\n",
       "      <td>2948.317275</td>\n",
       "      <td>3372.602070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330333</td>\n",
       "      <td>1.409294</td>\n",
       "      <td>1.302109</td>\n",
       "      <td>6.096207</td>\n",
       "      <td>0.004043</td>\n",
       "      <td>4</td>\n",
       "      <td>1983</td>\n",
       "      <td>-1.455333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>179</td>\n",
       "      <td>VEN</td>\n",
       "      <td>-2.071006</td>\n",
       "      <td>-3.764817</td>\n",
       "      <td>1.442165</td>\n",
       "      <td>0.483416</td>\n",
       "      <td>0.497463</td>\n",
       "      <td>4228.902648</td>\n",
       "      <td>4108.481006</td>\n",
       "      <td>3555.806233</td>\n",
       "      <td>...</td>\n",
       "      <td>7.977507</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.384118</td>\n",
       "      <td>1.012529</td>\n",
       "      <td>0.072326</td>\n",
       "      <td>0.027783</td>\n",
       "      <td>0.116498</td>\n",
       "      <td>128</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.269555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>180</td>\n",
       "      <td>VUT</td>\n",
       "      <td>1.979883</td>\n",
       "      <td>3.009629</td>\n",
       "      <td>9.574991</td>\n",
       "      <td>-1.628860</td>\n",
       "      <td>-1.607376</td>\n",
       "      <td>943.194390</td>\n",
       "      <td>945.066573</td>\n",
       "      <td>1137.138278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.897550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>182</td>\n",
       "      <td>ZAF</td>\n",
       "      <td>-0.383419</td>\n",
       "      <td>-1.846558</td>\n",
       "      <td>5.099152</td>\n",
       "      <td>0.562046</td>\n",
       "      <td>0.614545</td>\n",
       "      <td>2849.180570</td>\n",
       "      <td>3103.983020</td>\n",
       "      <td>2665.386369</td>\n",
       "      <td>...</td>\n",
       "      <td>1.182575</td>\n",
       "      <td>0.022561</td>\n",
       "      <td>0.724303</td>\n",
       "      <td>3.171675</td>\n",
       "      <td>0.185134</td>\n",
       "      <td>0.641042</td>\n",
       "      <td>0.538092</td>\n",
       "      <td>130</td>\n",
       "      <td>1983</td>\n",
       "      <td>1.163298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>183</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>-2.812783</td>\n",
       "      <td>-1.966689</td>\n",
       "      <td>-0.336835</td>\n",
       "      <td>-0.453336</td>\n",
       "      <td>-0.510946</td>\n",
       "      <td>637.545690</td>\n",
       "      <td>496.197754</td>\n",
       "      <td>408.777920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020058</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>9.282008</td>\n",
       "      <td>0.043878</td>\n",
       "      <td>0.025820</td>\n",
       "      <td>0.008154</td>\n",
       "      <td>131</td>\n",
       "      <td>1983</td>\n",
       "      <td>1.393661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>184</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>2.634297</td>\n",
       "      <td>1.585305</td>\n",
       "      <td>-1.907360</td>\n",
       "      <td>-0.201914</td>\n",
       "      <td>-0.219070</td>\n",
       "      <td>1073.064116</td>\n",
       "      <td>940.557953</td>\n",
       "      <td>741.875001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056610</td>\n",
       "      <td>3.653744</td>\n",
       "      <td>0.125775</td>\n",
       "      <td>0.087979</td>\n",
       "      <td>0.044107</td>\n",
       "      <td>132</td>\n",
       "      <td>1983</td>\n",
       "      <td>1.091719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index country_code  prev_gdp_growth  current_gdp_growth  \\\n",
       "0        2          AGO         0.000000            4.200001   \n",
       "1        3          AND         1.246461            1.770118   \n",
       "2        5          ARE        -6.719316           -4.745821   \n",
       "3        6          ARG        -0.735659            4.349093   \n",
       "4        7          ATG        -0.084012            5.364016   \n",
       "..     ...          ...              ...                 ...   \n",
       "128    179          VEN        -2.071006           -3.764817   \n",
       "129    180          VUT         1.979883            3.009629   \n",
       "130    182          ZAF        -0.383419           -1.846558   \n",
       "131    183          ZMB        -2.812783           -1.966689   \n",
       "132    184          ZWE         2.634297            1.585305   \n",
       "\n",
       "     future_gdp_growth  prev_gdp  current_gdp  prev_gdp_per_cap  \\\n",
       "0             6.000002 -0.344494    -0.316559        619.959903   \n",
       "1             1.784687 -1.235474    -1.267204       9610.020616   \n",
       "2             4.016951  0.359799     0.346322      40025.822624   \n",
       "3             1.570739  0.555838     0.640285       2927.897357   \n",
       "4            10.164996 -1.509220    -1.461880       2660.558091   \n",
       "..                 ...       ...          ...               ...   \n",
       "128           1.442165  0.483416     0.497463       4228.902648   \n",
       "129           9.574991 -1.628860    -1.607376        943.194390   \n",
       "130           5.099152  0.562046     0.614545       2849.180570   \n",
       "131          -0.336835 -0.453336    -0.510946        637.545690   \n",
       "132          -1.907360 -0.201914    -0.219070       1073.064116   \n",
       "\n",
       "     current_gdp_per_cap  future_gdp_per_cap  ...  resource_3  resource_4  \\\n",
       "0             623.440047          637.715098  ...    9.669948    0.000000   \n",
       "1            8025.207641         7728.906695  ...    0.055203    0.003099   \n",
       "2           34843.159626        32309.832713  ...    9.244663    0.002089   \n",
       "3            3553.377509         2659.708242  ...    0.080409    0.719742   \n",
       "4            2948.317275         3372.602070  ...    0.000000    0.000000   \n",
       "..                   ...                 ...  ...         ...         ...   \n",
       "128          4108.481006         3555.806233  ...    7.977507    0.022214   \n",
       "129           945.066573         1137.138278  ...    0.000000    0.000000   \n",
       "130          3103.983020         2665.386369  ...    1.182575    0.022561   \n",
       "131           496.197754          408.777920  ...    0.020058    0.000733   \n",
       "132           940.557953          741.875001  ...    0.030102    0.000000   \n",
       "\n",
       "     resource_5  resource_6  resource_7  resource_8  resource_9  node_numbers  \\\n",
       "0      0.000519    0.005412    0.007459    0.002400    0.024973             0   \n",
       "1      0.094196    2.465849    2.272600    1.914061    0.124961             1   \n",
       "2      0.028120    0.349803    0.124311    0.102205    0.038545             2   \n",
       "3      0.575831    0.959789    0.516924    0.132742    0.047231             3   \n",
       "4      0.330333    1.409294    1.302109    6.096207    0.004043             4   \n",
       "..          ...         ...         ...         ...         ...           ...   \n",
       "128    0.384118    1.012529    0.072326    0.027783    0.116498           128   \n",
       "129    0.000000    0.007115    0.000000    0.040526    0.000000           129   \n",
       "130    0.724303    3.171675    0.185134    0.641042    0.538092           130   \n",
       "131    0.005787    9.282008    0.043878    0.025820    0.008154           131   \n",
       "132    0.056610    3.653744    0.125775    0.087979    0.044107           132   \n",
       "\n",
       "     year      pred  \n",
       "0    1983  0.480781  \n",
       "1    1983 -0.063039  \n",
       "2    1983  1.248557  \n",
       "3    1983  1.429152  \n",
       "4    1983 -1.455333  \n",
       "..    ...       ...  \n",
       "128  1983  0.269555  \n",
       "129  1983  0.897550  \n",
       "130  1983  1.163298  \n",
       "131  1983  1.393661  \n",
       "132  1983  1.091719  \n",
       "\n",
       "[133 rows x 26 columns]"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "d30a3251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7fb3dcf9d0>"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(valid_df[\"pred\"], valid_df[\"future_gdp_per_cap_growth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "1673c681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.82362341641022, 15.33352666269962)"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(valid_df[\"pred\"], valid_df[\"future_gdp_per_cap_growth\"])\n",
    "lower = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "upper = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "ax.set_xlim(lower, upper)\n",
    "ax.set_ylim(lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "bbf5506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                OLS Regression Results                               \n",
      "=====================================================================================\n",
      "Dep. Variable:     future_gdp_per_cap_growth   R-squared:                       0.164\n",
      "Model:                                   OLS   Adj. R-squared:                  0.145\n",
      "Method:                        Least Squares   F-statistic:                     8.456\n",
      "Date:                       Fri, 16 Dec 2022   Prob (F-statistic):           3.59e-05\n",
      "Time:                               15:24:16   Log-Likelihood:                -387.74\n",
      "No. Observations:                        133   AIC:                             783.5\n",
      "Df Residuals:                            129   BIC:                             795.0\n",
      "Df Model:                                  3                                         \n",
      "Covariance Type:                   nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      1.0590      0.539      1.966      0.051      -0.007       2.125\n",
      "pred                           0.3602      0.371      0.972      0.333      -0.373       1.093\n",
      "current_gdp_per_cap_growth     0.2547      0.087      2.934      0.004       0.083       0.426\n",
      "prev_gdp_per_cap_growth        0.1366      0.082      1.669      0.098      -0.025       0.299\n",
      "==============================================================================\n",
      "Omnibus:                       32.985   Durbin-Watson:                   1.930\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               64.286\n",
      "Skew:                          -1.079   Prob(JB):                     1.10e-14\n",
      "Kurtosis:                       5.636   Cond. No.                         10.3\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "result = sm.ols(formula='future_gdp_per_cap_growth ~ pred +current_gdp_per_cap_growth + prev_gdp_per_cap_growth', \n",
    "             data=valid_df).fit()\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "7ca9de0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                OLS Regression Results                               \n",
      "=====================================================================================\n",
      "Dep. Variable:     future_gdp_per_cap_growth   R-squared:                       0.031\n",
      "Model:                                   OLS   Adj. R-squared:                  0.024\n",
      "Method:                        Least Squares   F-statistic:                     4.207\n",
      "Date:                       Fri, 16 Dec 2022   Prob (F-statistic):             0.0422\n",
      "Time:                               15:24:17   Log-Likelihood:                -397.58\n",
      "No. Observations:                        133   AIC:                             799.2\n",
      "Df Residuals:                            131   BIC:                             804.9\n",
      "Df Model:                                  1                                         \n",
      "Covariance Type:                   nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.4338      0.556      0.781      0.436      -0.666       1.533\n",
      "pred           0.7835      0.382      2.051      0.042       0.028       1.539\n",
      "==============================================================================\n",
      "Omnibus:                       19.473   Durbin-Watson:                   1.702\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               28.267\n",
      "Skew:                          -0.761   Prob(JB):                     7.28e-07\n",
      "Kurtosis:                       4.669   Cond. No.                         2.42\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "result = sm.ols(formula='future_gdp_per_cap_growth ~ pred', \n",
    "             data=valid_df).fit()\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "78c8e13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1373438/1946012134.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"pred\"] = test_out_list\n"
     ]
    }
   ],
   "source": [
    "#evaluate on test set\n",
    "#evaluate on test set\n",
    "model.load_state_dict(best_weights)\n",
    "out_val = model(test_batch.x, test_batch.edge_index, test_batch.edge_attr)\n",
    "test_out_list = [item for sublist in out_val.tolist() for item in sublist]\n",
    "test_df = trade_df[trade_df.year.isin(test_years)]\n",
    "test_df[\"pred\"] = test_out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "4d27d190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f7fab7561c0>"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(test_df[\"pred\"], test_df[\"future_gdp_per_cap_growth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "a349bb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-16.26411981185805, 20.15342023789545)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(test_df[\"pred\"], test_df[\"future_gdp_per_cap_growth\"])\n",
    "lower = min(ax.get_xlim()[0], ax.get_ylim()[0])\n",
    "upper = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "ax.set_xlim(lower, upper)\n",
    "ax.set_ylim(lower, upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "01834591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                OLS Regression Results                               \n",
      "=====================================================================================\n",
      "Dep. Variable:     future_gdp_per_cap_growth   R-squared:                       0.012\n",
      "Model:                                   OLS   Adj. R-squared:                  0.005\n",
      "Method:                        Least Squares   F-statistic:                     1.623\n",
      "Date:                       Fri, 16 Dec 2022   Prob (F-statistic):              0.205\n",
      "Time:                               14:58:09   Log-Likelihood:                -410.46\n",
      "No. Observations:                        136   AIC:                             824.9\n",
      "Df Residuals:                            134   BIC:                             830.7\n",
      "Df Model:                                  1                                         \n",
      "Covariance Type:                   nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.1640      0.996      2.173      0.032       0.194       4.134\n",
      "pred          -0.7439      0.584     -1.274      0.205      -1.899       0.411\n",
      "==============================================================================\n",
      "Omnibus:                       10.430   Durbin-Watson:                   1.785\n",
      "Prob(Omnibus):                  0.005   Jarque-Bera (JB):               25.685\n",
      "Skew:                          -0.025   Prob(JB):                     2.65e-06\n",
      "Kurtosis:                       5.128   Cond. No.                         5.14\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "result = sm.ols(formula='future_gdp_per_cap_growth ~ pred', \n",
    "             data=test_df).fit()\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "445f7a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                OLS Regression Results                               \n",
      "=====================================================================================\n",
      "Dep. Variable:     future_gdp_per_cap_growth   R-squared:                       0.163\n",
      "Model:                                   OLS   Adj. R-squared:                  0.144\n",
      "Method:                        Least Squares   F-statistic:                     8.581\n",
      "Date:                       Fri, 16 Dec 2022   Prob (F-statistic):           3.02e-05\n",
      "Time:                               14:58:09   Log-Likelihood:                -399.16\n",
      "No. Observations:                        136   AIC:                             806.3\n",
      "Df Residuals:                            132   BIC:                             818.0\n",
      "Df Model:                                  3                                         \n",
      "Covariance Type:                   nonrobust                                         \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "Intercept                      2.5841      0.938      2.756      0.007       0.729       4.439\n",
      "current_gdp_per_cap_growth     0.2740      0.088      3.106      0.002       0.100       0.448\n",
      "prev_gdp_per_cap_growth        0.1941      0.081      2.398      0.018       0.034       0.354\n",
      "pred                          -1.1586      0.548     -2.114      0.036      -2.243      -0.074\n",
      "==============================================================================\n",
      "Omnibus:                       11.565   Durbin-Watson:                   1.720\n",
      "Prob(Omnibus):                  0.003   Jarque-Bera (JB):               26.726\n",
      "Skew:                           0.217   Prob(JB):                     1.57e-06\n",
      "Kurtosis:                       5.128   Cond. No.                         16.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "result = sm.ols(formula='future_gdp_per_cap_growth ~ current_gdp_per_cap_growth+prev_gdp_per_cap_growth +pred', \n",
    "             data=test_df).fit()\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1df262",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning of a Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "7f0ad1a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3182354760.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1373438/3182354760.py\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0005, 0.0001, 0.001]\n",
    "attn_drop_rates = [0, 0.1]\n",
    "feat_drop_rates = [0, 0.1]\n",
    "num_heads_list = [10, 20, 50, 100]\n",
    "num_feat_list = [5, 10, 20, 50]\n",
    "num_epochs_list = [500, 10000]\n",
    "\n",
    "list_model_params = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for attn_drop in attn_drop_rates:\n",
    "        for feat_drop in feat_drop_rates:\n",
    "            for num_heads in num_heads_list:\n",
    "                for num_feats in num_feat_list:\n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0791204e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a338637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212de0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec66eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefb0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231abb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c2a70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeRegressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, reg_input_dim, hidden_dim = 25, attribute_type = \"attributes\", \n",
    "                 num_heads_hidden = 10, num_heads_out = 10):\n",
    "        super(GraphRegressor, self).__init__()\n",
    "        \n",
    "        #set up for GNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attribute_type = attribute_type\n",
    "        self.graph_conv1 = GATConv(input_dim, hidden_dim, num_heads_hidden, attn_drop = 0.2, \n",
    "                                     feat_drop = 0.2)\n",
    "        self.graph_conv2 = GATConv(hidden_dim*num_heads_hidden, out_feats = 1)\n",
    "        #self.pooling = AvgPooling()\n",
    "        #self.MLP = nn.Linear(hidden_dim*num_heads_out, 1)\n",
    "        \n",
    "        #set up for penalized linear regression\n",
    "        #self.reg_input_dim = reg_input_dim\n",
    "        #self.dropout = nn.Dropout(0.8)\n",
    "        #self.linear_reg = nn.Linear(reg_input_dim, 1)\n",
    "        \n",
    "        #combine output from \n",
    "        #self.linear_comb = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self, graph, reg_data):\n",
    "        \n",
    "        #run GNN\n",
    "        x = graph.ndata[self.attribute_type] # extract node attributes in an n x d tensor\n",
    "        x = torch.relu(self.graph_conv1(graph, x))\n",
    "        #reshapes to combine heads: https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch \n",
    "        # -1 singifies don't know how many rows are needed, but know how many columns \n",
    "        x = x.view(-1, x.size(1)* x.size(2))\n",
    "        #print(x.shape)\n",
    "        x = torch.relu(self.graph_conv2(graph, x))\n",
    "        x = x.view(-1, x.size(1)* x.size(2)) #reshape second time with multiple heads\n",
    "        x = self.pooling(graph, x)\n",
    "        x = self.MLP(x)\n",
    "        \n",
    "        #run regression\n",
    "        #reg_data = self.dropout(reg_data)\n",
    "        out = self.linear_reg(reg_data)\n",
    "        \n",
    "        #combine regression and GNN output\n",
    "        x = torch.cat((x,out),1)\n",
    "        #return x, reg_data\n",
    "        x = self.linear_comb(x)\n",
    "        #print(x.dtype)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111f302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab63e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b72386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6da7a9b",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Experimenting with Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to make graphs compatible with regression tables\n",
    "def pred_group_for_graphs(gnn_table, lookbacks):\n",
    "    \"\"\" gnn_table: graph table, as generated by gm.submissions_agg \n",
    "        lookbacks: number of lookbacks in gnn_table\n",
    "        \n",
    "        In order to merge with dependent variable and to ensure a walk-between equivalently structured regression, \n",
    "        need to create \"pred_group\" column. \"pred_group\" and \"ticker\" are two columns that enable joining\n",
    "        between regression tables and graph tables. \n",
    "        \n",
    "        \"ticker\" is a column already included\n",
    "        \"pred_group\" needds to be created. If we have only one lookback window - \"pred_group\" = \"ind_group\"\n",
    "        otherwise: we build a longer graph which spans all relevant ind_groups (all lookback periods are \n",
    "        combined into longer time frame)\n",
    "        \n",
    "        function returns gnn_table of submissions and pred_group\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    #if we have a single lookback period - ind_group is equivalent to pred_group\n",
    "    if(lookbacks == 1):\n",
    "        gnn_table[\"pred_group\"] = gnn_table[\"ind_group\"]\n",
    "        return(gnn_table)\n",
    "    #if we have multiple lookback period - we combine all nodes within all lookback periods\n",
    "    else:\n",
    "        #we create long table \n",
    "        \n",
    "        #find all indicator columns\n",
    "        ind_group_cols = []\n",
    "        for lookback in range(lookbacks):\n",
    "            ig =f'ind_group_{lookback}'\n",
    "            ind_group_cols.append(ig)\n",
    "\n",
    "        long_gnn_table = pd.DataFrame()\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        #for each indicator group - append rows\n",
    "        for ind_group_col in ind_group_cols:\n",
    "            \n",
    "            if (i == 0):\n",
    "                gnn_table[\"pred_group\"] = gnn_table[ind_group_col]\n",
    "                long_gnn_table = gnn_table.drop(columns = ind_group_cols)\n",
    "            else:\n",
    "                gnn_table[\"pred_group\"] = gnn_table[ind_group_col]\n",
    "                long_gnn_table = long_gnn_table.append(gnn_table.drop(columns = ind_group_cols), \n",
    "                                                      ignore_index = True)\n",
    "            i = i+1\n",
    "        \n",
    "        long_gnn_table = long_gnn_table.drop_duplicates()\n",
    "        return(long_gnn_table)\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f46ec1",
   "metadata": {},
   "source": [
    "# Apply Log Transforms to y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform(table, y_col = \"num_submissions_future\"):\n",
    "    new_col = y_col + \"_log\"\n",
    "    table[new_col] = np.log(table[y_col]+1)\n",
    "    return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ed70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reg_module(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \n",
    "    #remove NA - first from y, then from X\n",
    "    y_train = y_train[~y_train.index.isin(X_train[X_train.isna().any(axis = 1)].index)]\n",
    "    y_val = y_val[~y_val.index.isin(X_val[X_val.isna().any(axis = 1)].index)]\n",
    "    y_test = y_test[~y_test.index.isin(X_test[X_test.isna().any(axis = 1)].index)]\n",
    "    \n",
    "    X_train = X_train[~X_train.index.isin(X_train[X_train.isna().any(axis = 1)].index)]\n",
    "    X_val = X_val[~X_val.index.isin(X_val[X_val.isna().any(axis = 1)].index)]\n",
    "    X_test = X_test[~X_test.index.isin(X_test[X_test.isna().any(axis = 1)].index)]\n",
    "    \n",
    "    #combine train, val for full model fitting\n",
    "    X_train_val = X_train.append(X_val)\n",
    "    y_train_val = y_train.append(y_val)\n",
    "    \n",
    "    #scale\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    \n",
    "    X_train = ss.transform(X_train)\n",
    "    X_val = ss.transform(X_val)\n",
    "    X_test = ss.transform(X_test)\n",
    "    X_train_val = ss.transform(X_train_val)\n",
    "    \n",
    "    # Add constant \n",
    "    X_train = sm.add_constant(X_train)\n",
    "    X_val = sm.add_constant(X_val)\n",
    "    X_test = sm.add_constant(X_test)\n",
    "    X_train_val = sm.add_constant(X_train_val)\n",
    "    \n",
    "    #run regularized OLS\n",
    "    alphas = [0.1,1,10,100]\n",
    "    l1s = np.linspace(0,1,11)\n",
    "    models = {'model':[],'alpha':[],'l1_wt':[], 'r2':[]}\n",
    "    for a in alphas:\n",
    "        for l1 in l1s:\n",
    "            models['alpha'].append(a)\n",
    "            models['l1_wt'].append(l1)\n",
    "            model = sm.OLS(y_train, X_train, missing = \"drop\").fit_regularized(alpha=a, L1_wt=l1, refit=False)\n",
    "            models['model'].append(model)\n",
    "            pred = model.predict(X_val)\n",
    "            r2 = r2_score(y_val, pred)\n",
    "            models['r2'].append(r2)\n",
    "    \n",
    "    models = pd.DataFrame(models).sort_values('r2', ascending=False).reset_index()\n",
    "    \n",
    "    #extract optimal alpha, l1\n",
    "    alpha_opt = models.iloc[0][\"alpha\"]\n",
    "    l1_opt = models.iloc[0][\"l1_wt\"]\n",
    "    \n",
    "    #l1_opt = 0\n",
    "    \n",
    "    #retrain optimal model on train, val \n",
    "    if(l1_opt == 0): \n",
    "        opt_model = sm.OLS(y_train_val, X_train_val).fit()\n",
    "    else:\n",
    "        opt_model = sm.OLS(y_train_val, X_train_val, missing = \"drop\").fit_regularized(alpha=alpha_opt, \n",
    "                                                                               L1_wt=l1_opt, refit=False)\n",
    "    #evaluate performance on test set\n",
    "    pred = opt_model.predict(X_test)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    \n",
    "    return r2, opt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9dc16",
   "metadata": {},
   "source": [
    "### Check Graph Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvt_split(reg_table, graph_table,\n",
    "    x_cols_to_elim_reg = ['pred_group', 'pred_group_start', 'pred_group_end', \"ticker\", \n",
    "                      'num_submissions_future', 'num_comments_future', 'num_authors_future',\n",
    "                      'ncf_bin', 'nearest_bday', 'source'],\n",
    "    y_col = \"num_submissions_future\", \n",
    "    y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin'],\n",
    "    x_cols_graph = ['avg_level','max_level', 'total_com', 'num_auth', 'pct_ch',\n",
    "           'mkt_cap', 'log_return_mean', 'log_return_std', 'num_authors','num_posts', 'created_utc'],\n",
    "    test_frac = 0.2, val_frac = 0.2, rand_seed= 192,\n",
    "             stratify = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    reg_table: regression table built by dl.create_regression_table\n",
    "    graph_table: graph learning table created by gm.submission_agg and pred_group columns created using \n",
    "        pred_group_for_graphs function\n",
    "    x_cols_to_elim_reg - x columns eliminated from regression table\n",
    "    y_col - y column chosen as dependent variable for regression\n",
    "    y_cols_to_elim_reg - y columns to be eliminated\n",
    "    x_cols_graph - x columns selected for graph table (to be normalized as denep)\n",
    "    test_frac - fraction of data for test set\n",
    "    val_frac - fraction of data for val set\n",
    "    \n",
    "    Function produces train, val, test tables from a regression table; it returns a re-scaled graph table \n",
    "    with indicators of whether a node belongs to train, val or test\n",
    "    \n",
    "    \"\"\"\n",
    "####Create regression tables\n",
    "    cols = reg_table.columns \n",
    "    cols = [col for col in cols if col not in x_cols_to_elim_reg]\n",
    "    cols = [col for col in cols if col not in y_cols_to_elim_reg]\n",
    "    \n",
    "    #create reg train, val, test split\n",
    "    if(stratify):\n",
    "        regX_train, regX_val, regX_test, regy_train, regy_val, regy_test = DL.stratify_train_test_split(data = reg_table,                                   \n",
    "                            x= cols, y=y_col, test_size=test_frac, val_size = val_frac, random_state = rand_seed)\n",
    "    else: \n",
    "        regX_train, regX_val, regX_test, regy_train, regy_val, regy_test = DL.forecasting_split(data = reg_table,                                   \n",
    "                            x= cols, y=y_col, test_size=test_frac, val_size = val_frac, random_state = rand_seed)\n",
    "        \n",
    "####Infrastructure for combining with graph data\n",
    "    #combine into single dataframe to combine this with graph table on \"pred_group\" and \"ticker\" columns\n",
    "    #create new column tracking which submissions belong to train, val and test\n",
    "    \n",
    "    temp_train = regX_train.copy().reset_index()\n",
    "    temp_train[\"train_val_test\"] = \"train\"\n",
    "    temp_val = regX_val.copy().reset_index()\n",
    "    temp_val[\"train_val_test\"] = \"val\"\n",
    "    temp_test = regX_test.copy().reset_index()\n",
    "    temp_test[\"train_val_test\"] = \"test\"\n",
    "    \n",
    "    combined = temp_train.append(temp_val, ignore_index = True)\n",
    "    combined = combined.append(temp_test, ignore_index = True)\n",
    "    \n",
    "    combined = combined.dropna()\n",
    "    \n",
    "    #scaler = StandardScaler()\n",
    "    #scaler.fit(combined[combined[\"train_val_test\"] == \"train\"][cols])\n",
    "    #rescale reg\n",
    "    #combined[cols] = scaler.transform(combined[cols])\n",
    "    \n",
    "    cols_new = [val if val not in x_cols_graph else val+\"_reg\" for val in cols]\n",
    "    \n",
    "    #x_cols_graph.extend(cols)\n",
    "    \n",
    "    #print(x_cols_graph)\n",
    "    \n",
    "    graph_table = graph_table.merge(combined,\n",
    "                               on = [\"pred_group\", \"ticker\"],\n",
    "                                suffixes = (\"\",\"_reg\"))\n",
    "    \n",
    "    # train scaler only on train set \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(graph_table[graph_table[\"train_val_test\"] == \"train\"][x_cols_graph])\n",
    "    #rescale graph\n",
    "    graph_table[x_cols_graph] = scaler.transform(graph_table[x_cols_graph])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(graph_table[graph_table[\"train_val_test\"] == \"train\"].drop_duplicates().copy()[cols_new])\n",
    "    #rescale graph\n",
    "    graph_table[cols_new] = scaler.transform(graph_table[cols_new])\n",
    "    \n",
    "    #create column for future idenfitication of specific pred_ticker combos\n",
    "    graph_table[\"pred_ticker\"] = graph_table.apply(lambda row: (row[\"pred_group\"], row[\"ticker\"]), axis = 1)\n",
    "    \n",
    "    return(regX_train, regX_val, regX_test, regy_train, regy_val, regy_test, graph_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75513b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_create(graph_table, pred_group_select, ticker_select,\n",
    "                 \n",
    "                y_col = \"num_submissions_future\",\n",
    "                reg_columns = ['total_sub',\n",
    "       'avg_level_reg', 'max_level_reg', 'total_com_reg', 'max_num_com',\n",
    "       'max_num_auth', 'avg_num_auth', 'total_auth', 'avg_num_com',\n",
    "       'overall_posts', 'overall_engagements', 'overall_authors',\n",
    "       'return_mean', 'return_std', 'vol_mean', 'vol_std',\n",
    "       'log_return_mean_reg', 'log_return_std_reg', 'return_var', 'vol_var', 'mkt_cap_reg']):\n",
    "    \n",
    "    \"\"\"\n",
    "    graph_table: graph learning table created by gm.submission_agg and pred_group columns created using \n",
    "        pred_group_for_graphs function\n",
    "    pred_group_select: pred_group identifier\n",
    "    ticker_select: selected ticker\n",
    "    -- pred_group,ticker combination regers to unique graph \n",
    "    y_col: dependent variable selected to create graph label\n",
    "    \n",
    "    Function takes in graph table with specific pred_group, ticker - creates a graph \n",
    "    (storing all data from independent variables in a graph structure for learning) and a label - encoding dependt var\n",
    "    \n",
    "    graphs can be fed for learning into GNN\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #filter data to the data we are trying to use\n",
    "    graph = graph_table[((graph_table[\"pred_group\"] == pred_group_select) & \\\n",
    "                        (graph_table[\"ticker\"] == ticker_select))].copy()\n",
    "    \n",
    "    #label is the dependent variable - for each pred_group, ticker combination - label is constant\n",
    "        #if not - we print a warning\n",
    "    temp = list(set(graph[y_col]))\n",
    "    if(len(temp) > 1): print(\"too many dependent variables for \" + ticker_select + str(pred_group_select))\n",
    "    label = torch.tensor(np.float32(temp[0]))\n",
    "    \n",
    "    #extract reg- columns \n",
    "    \n",
    "    \n",
    "    #create list of all submissions - these are all the nodes in our graph\n",
    "    nodes = list(set(graph[\"submission_id\"]))\n",
    "    node_dict = {}\n",
    "    node_num = []\n",
    "    i = 0\n",
    "\n",
    "    node_attributes = []\n",
    "    node_attributes_no_mkt = []\n",
    "    node_attributes_only_mkt = []\n",
    "\n",
    "    #create a dict where we can go between numbers for node and name of node\n",
    "    #store node attributes here in NxD dimentional array - row number corresponds to node\n",
    "    for index, row in graph.iterrows():\n",
    "        \n",
    "        if(i == 0):\n",
    "            reg_vars = np.array(row[reg_columns].values, dtype = np.float32)\n",
    "            #reg_vars = reg_vars.reshape(-1,len(reg_vars))\n",
    "            #reg_vars = torch.tensor(np.array(reg_vars))\n",
    "        \n",
    "        node_dict[i] = row[\"submission_id\"]\n",
    "        node_num.append(i)\n",
    "        i = i+1\n",
    "        node_attributes.append([np.float32(row[\"num_auth\"]), np.float32(row[\"avg_level\"]), \n",
    "                                np.float32(row[\"max_level\"]), np.float32(row[\"total_com\"]),\n",
    "                                np.float32(row[\"pct_ch\"]), np.float32(row[\"mkt_cap\"]),\n",
    "                                np.float32(row[\"log_return_mean\"]), np.float32(row[\"log_return_std\"]), \n",
    "                                np.float32(row[\"num_authors\"]), np.float32(row[\"num_posts\"]),\n",
    "                                np.float32(row[\"created_utc\"])])\n",
    "        node_attributes_no_mkt.append([np.float32(row[\"num_auth\"]), np.float32(row[\"avg_level\"]), \n",
    "                                np.float32(row[\"max_level\"]), np.float32(row[\"total_com\"]), \n",
    "                                np.float32(row[\"num_authors\"]), np.float32(row[\"num_posts\"]),\n",
    "                                np.float32(row[\"created_utc\"])])\n",
    "        node_attributes_only_mkt.append([np.float32(row[\"pct_ch\"]), np.float32(row[\"mkt_cap\"]),\n",
    "                                np.float32(row[\"log_return_mean\"]), np.float32(row[\"log_return_std\"])])\n",
    "\n",
    "    #create complete graph from our numerically stored nodes\n",
    "    edges = list(combinations(node_num,2))\n",
    "    #print(edges)\n",
    "\n",
    "    #add self-loops\n",
    "    self_loops = [(x,x) for x in node_num]\n",
    "    edges.extend(self_loops)\n",
    "\n",
    "    adjacency = skn.utils.edgelist2adjacency(edges, undirected=True)\n",
    "\n",
    "    g = dgl.from_scipy(adjacency)\n",
    "    g.ndata[\"attributes\"] = torch.tensor(np.array(node_attributes))\n",
    "    g.ndata[\"attributes_no_mkt\"] = torch.tensor(np.array(node_attributes_no_mkt))\n",
    "    g.ndata[\"attributes_only_mkt\"] = torch.tensor(np.array(node_attributes_only_mkt))\n",
    "    \n",
    "    #reg_vars = torch.from_numpy(reg_vars.reshape(-1,len(reg_vars)))\n",
    "    \n",
    "    return(g,label, reg_vars)\n",
    "\n",
    "def table_to_graphs(graph_table, y_col_select = \"num_submissions_future\"):\n",
    "\n",
    "    pred_ticker_combos = list(set(graph_table[\"pred_ticker\"]))\n",
    "    \n",
    "    dgl_graphs = []\n",
    "    \n",
    "    for pred_ticker_combo in pred_ticker_combos:\n",
    "        g, label, reg_vars = graph_create(graph_table, \n",
    "                                pred_group_select = pred_ticker_combo[0], \n",
    "                                ticker_select = pred_ticker_combo[1],\n",
    "                               y_col = y_col_select,)\n",
    "        dgl_graphs.append((g,label, reg_vars, pred_ticker_combo))\n",
    "    \n",
    "    return(dgl_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e1f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tvt_split_graphs(dataset, graphs_table):\n",
    "    train = list(set(graphs_table[graphs_table[\"train_val_test\"] == \"train\"][\"pred_ticker\"]))\n",
    "    val = list(set(graphs_table[graphs_table[\"train_val_test\"] == \"val\"][\"pred_ticker\"]))\n",
    "    test = list(set(graphs_table[graphs_table[\"train_val_test\"] == \"test\"][\"pred_ticker\"]))\n",
    "  \n",
    "    train_set = [tup for tup in dataset if tup[3] in train]\n",
    "    train_graphs = [tup[0] for tup in train_set]\n",
    "    train_data = [tup[2] for tup in train_set]\n",
    "    train_labels = [tup[1] for tup in train_set]\n",
    "    \n",
    "    val_set = [tup for tup in dataset if tup[3] in val]\n",
    "    val_graphs = [tup[0] for tup in val_set]\n",
    "    val_data = [tup[2] for tup in val_set]\n",
    "    val_labels = [tup[1] for tup in val_set]\n",
    "    \n",
    "    test_set = [tup for tup in dataset if tup[3] in test]\n",
    "    test_graphs = [tup[0] for tup in test_set]\n",
    "    test_data = [tup[2] for tup in test_set]\n",
    "    test_labels = [tup[1] for tup in test_set]\n",
    "    \n",
    "    dataset_a = list(zip(train_graphs, train_data, train_labels, train))\n",
    "    dataset_b = list(zip(val_graphs, val_data, val_labels, val))\n",
    "    dataset_c = list(zip(test_graphs, test_data, test_labels, test))\n",
    "    \n",
    "    return dataset_a, dataset_b, dataset_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up data loader for \n",
    "def collate(samples):\n",
    "    \"\"\"Used to create DGL dataloaders.\"\"\"\n",
    "    graphs, reg_data, labels, tvt = map(list, zip(*samples)) # converts ((g1, l1), (g2, l2)...) to (g1, g2,...), (l1, l2,...) \n",
    "    #print(reg_data)\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    #print(reg_data)\n",
    "    reg_data = np.array(reg_data)\n",
    "    reg_data = reg_data.reshape(-1,len(reg_data[0]))\n",
    "    reg_data = torch.from_numpy(reg_data)\n",
    "    #print(reg_data)\n",
    "    \n",
    "    return batched_graph, reg_data, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b3bab",
   "metadata": {},
   "source": [
    "# Different seed comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f13815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change number of output heads for final layer\n",
    "class GraphRegressor2(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, reg_input_dim, hidden_dim = 25, attribute_type = \"attributes\", \n",
    "                 num_heads_hidden = 10, num_heads_out = 10):\n",
    "        super(GraphRegressor2, self).__init__()\n",
    "        \n",
    "        #set up for GNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attribute_type = attribute_type\n",
    "        self.graph_conv1 = GATConv(input_dim, hidden_dim, num_heads_hidden, attn_drop = 0.2, \n",
    "                                     feat_drop = 0.2)\n",
    "        self.graph_conv2 = GATConv(hidden_dim*num_heads_hidden, hidden_dim, num_heads_out, attn_drop = 0.2, \n",
    "                                     feat_drop = 0.2)\n",
    "        self.pooling = AvgPooling()\n",
    "        self.MLP = nn.Linear(hidden_dim*num_heads_out, 1)\n",
    "        \n",
    "        #set up for penalized linear regression\n",
    "        self.reg_input_dim = reg_input_dim\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.linear_reg = nn.Linear(reg_input_dim, 1)\n",
    "        \n",
    "        #combine output from \n",
    "        self.linear_comb1 = nn.Linear(2,4)\n",
    "        self.linear_comb2 = nn.Linear(4,1)\n",
    "        self.positive = nn.Softplus()\n",
    "        \n",
    "    def forward(self, graph, reg_data):\n",
    "        \n",
    "        #run GNN\n",
    "        x = graph.ndata[self.attribute_type] # extract node attributes in an n x d tensor\n",
    "        x = torch.relu(self.graph_conv1(graph, x))\n",
    "        #reshapes to combine heads: https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch \n",
    "        # -1 singifies don't know how many rows are needed, but know how many columns \n",
    "        x = x.view(-1, x.size(1)* x.size(2))\n",
    "        #print(x.shape)\n",
    "        x = torch.relu(self.graph_conv2(graph, x))\n",
    "        x = x.view(-1, x.size(1)* x.size(2)) #reshape second time with multiple heads\n",
    "        x = self.pooling(graph, x)\n",
    "        x = self.MLP(x)\n",
    "        \n",
    "        #run regression\n",
    "        #reg_data = self.dropout(reg_data)\n",
    "        out = self.linear_reg(reg_data)\n",
    "        \n",
    "        #combine regression and GNN output\n",
    "        x = torch.cat((x,out),1)\n",
    "        #return x, reg_data\n",
    "        x = torch.relu(self.linear_comb1(x))\n",
    "        x = self.linear_comb2(x)\n",
    "        x = self.positive(x)\n",
    "        #print(x.dtype)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda00587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gnn_module2(train, val, test, \n",
    "                   gnn_epochs = 50, batches = 200, \n",
    "                   plot = False, attribute_select = \"attributes\"):\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batches, shuffle=True, collate_fn=collate, drop_last = True)\n",
    "    valid_loader = DataLoader(val, batch_size=batches, collate_fn=collate, drop_last = True)\n",
    "    test_loader = DataLoader(test, batch_size=batches, collate_fn=collate, drop_last = True)\n",
    "    \n",
    "    #set up initial GNN module \n",
    "    #find initial input dimension from attirbutes selected\n",
    "    gnn_input_dim = len(train[0][0].ndata[attribute_select].detach().numpy()[0])\n",
    "    reg_input_dim = len(train[0][1])\n",
    "    model = GraphRegressor2(input_dim=gnn_input_dim, attribute_type = attribute_select, \n",
    "                           reg_input_dim = reg_input_dim)\n",
    "    \n",
    "    loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay = 0.001) #perhaps worth playing with \n",
    "\n",
    "    #optimizer specifying different learning rates: https://discuss.pytorch.org/t/how-to-specify-only-single-module-in-the-per-parameter-options/91008/2\n",
    "    #my_list = ['linear_reg.weight', 'linear_reg.bias']\n",
    "    #optimizer = torch.optim.Adam([{'params': [p for n, p in model.named_parameters() if n not in my_list]},\n",
    "    #            {'params': [p for n, p in model.named_parameters() if n in my_list], 'lr': 0.001}], \n",
    "    #                              lr=0.0001, weight_decay = 0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    \n",
    "    #set up model and parameters tracking performance\n",
    "    best_val_mse = np.inf \n",
    "    best_model = None\n",
    "    training_logs = []\n",
    "    num_epochs = gnn_epochs\n",
    "    \n",
    "    #train NN model\n",
    "    for epoch in trange(num_epochs):\n",
    "    \n",
    "        # training step\n",
    "        model.train()\n",
    "        train_mse = []\n",
    "        \n",
    "        for i, (graphs, reg_data, labels) in enumerate(train_loader):\n",
    "\n",
    "            #print(graphs)\n",
    "            #squeeze operation \n",
    "            predictions = model(graphs, reg_data).squeeze()\n",
    "            #print(predictions)\n",
    "            #print(predictions)\n",
    "            loss = loss_fn(predictions, labels)#+l1_regularizer(model, )\n",
    "            train_mse.append(loss.detach().numpy())\n",
    "            loss = loss.mean()\n",
    "            optimizer.zero_grad()\n",
    "            #loss.backward generates type problem: possibly linked to: https://discuss.pytorch.org/t/strange-error-from-loss-backward/62344\n",
    "            #loss = torch.tensor(loss, dtype = torch.float, requires_grad = True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #s = model.MLP1.weight.data.abs()\n",
    "            #print(s)\n",
    "            #lossl1 = l1_regularizer(model,)\n",
    "            #print(lossl1)\n",
    "        #print(model.linear_reg.weight.data.abs().sum())\n",
    "        train_mse = np.mean(np.concatenate(train_mse))\n",
    "        #print(train_mse)\n",
    "\n",
    "        # validation step\n",
    "        model.eval()\n",
    "        valid_mse = []\n",
    "        for i, (graphs, reg_data,labels) in enumerate(valid_loader):\n",
    "            predictions = model(graphs,reg_data).squeeze()\n",
    "            #print(reg_data)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            valid_mse.append(loss.detach().numpy())\n",
    "        valid_mse = np.mean(np.concatenate(valid_mse))\n",
    "        \n",
    "        #print(valid_mse)\n",
    "\n",
    "        # early stopping \n",
    "        if valid_mse < best_val_mse:\n",
    "            best_val_mse = valid_mse\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "        training_logs.append((train_mse, valid_mse))\n",
    "\n",
    "    training_logs = np.array(training_logs)\n",
    "    \n",
    "    #evaluate model performance\n",
    "    model.load_state_dict(best_weights)\n",
    "    store_test_mse  = []\n",
    "    store_test_l1  = []\n",
    "    store_labels = []\n",
    "    store_predictions = []\n",
    "    \n",
    "    loss_fn_l1 = nn.L1Loss(reduction='none')\n",
    "    loss_fn_mse = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    for i, (graphs, reg_data, labels) in enumerate(test_loader):\n",
    "        predictions = model(graphs, reg_data).squeeze()\n",
    "        #evaluate l1 loss\n",
    "        loss_l1 = loss_fn_l1(predictions, labels)\n",
    "        store_test_l1.append(loss.detach().numpy())\n",
    "        #evaluate mse loss\n",
    "        loss_mse = loss_fn_mse(predictions, labels)\n",
    "        store_test_mse.append(loss.detach().numpy())\n",
    "        #store labels\n",
    "        store_labels.append(labels.detach().numpy())\n",
    "        store_predictions.append(predictions.detach().numpy())\n",
    "    \n",
    "    #store evaluation\n",
    "    r2 = r2_score(np.concatenate(store_labels), \n",
    "                  np.concatenate(store_predictions))\n",
    "    test_mse = np.mean(np.concatenate(store_test_mse))\n",
    "    test_l1 = np.mean(np.concatenate(store_test_l1))\n",
    "    \n",
    "    #print(model.linear_comb.weight.data)\n",
    "    \n",
    "    if (plot == True):\n",
    "        #plot how model performance evovles over time \n",
    "        fig, ax = plt.subplots()\n",
    "        skip = 5\n",
    "        ax.plot(range(skip, num_epochs), training_logs[skip:,0], label='train')\n",
    "        ax.plot(range(skip, num_epochs), training_logs[skip:,1], label='valid')\n",
    "        ax.plot([skip, num_epochs], [test_l1, test_l1], label='test', linestyle='dashed')\n",
    "        ax.legend()\n",
    "        \n",
    "    \n",
    "    return(r2, test_mse, test_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change number of output heads for final layer\n",
    "class GraphRegressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, reg_input_dim, hidden_dim = 25, attribute_type = \"attributes\", \n",
    "                 num_heads_hidden = 10, num_heads_out = 10):\n",
    "        super(GraphRegressor, self).__init__()\n",
    "        \n",
    "        #set up for GNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attribute_type = attribute_type\n",
    "        self.graph_conv1 = GATConv(input_dim, hidden_dim, num_heads_hidden, attn_drop = 0.2, \n",
    "                                     feat_drop = 0.2)\n",
    "        self.graph_conv2 = GATConv(hidden_dim*num_heads_hidden, hidden_dim, num_heads_out, attn_drop = 0.2, \n",
    "                                     feat_drop = 0.2)\n",
    "        self.pooling = AvgPooling()\n",
    "        self.MLP = nn.Linear(hidden_dim*num_heads_out, 1)\n",
    "        \n",
    "        #set up for penalized linear regression\n",
    "        self.reg_input_dim = reg_input_dim\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.linear_reg = nn.Linear(reg_input_dim, 1)\n",
    "        \n",
    "        #combine output from \n",
    "        self.linear_comb = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self, graph, reg_data):\n",
    "        \n",
    "        #run GNN\n",
    "        x = graph.ndata[self.attribute_type] # extract node attributes in an n x d tensor\n",
    "        x = torch.relu(self.graph_conv1(graph, x))\n",
    "        #reshapes to combine heads: https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch \n",
    "        # -1 singifies don't know how many rows are needed, but know how many columns \n",
    "        x = x.view(-1, x.size(1)* x.size(2))\n",
    "        #print(x.shape)\n",
    "        x = torch.relu(self.graph_conv2(graph, x))\n",
    "        x = x.view(-1, x.size(1)* x.size(2)) #reshape second time with multiple heads\n",
    "        x = self.pooling(graph, x)\n",
    "        x = self.MLP(x)\n",
    "        \n",
    "        #run regression\n",
    "        #reg_data = self.dropout(reg_data)\n",
    "        out = self.linear_reg(reg_data)\n",
    "        \n",
    "        #combine regression and GNN output\n",
    "        x = torch.cat((x,out),1)\n",
    "        #return x, reg_data\n",
    "        x = self.linear_comb(x)\n",
    "        #print(x.dtype)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gnn_module(train, val, test, \n",
    "                   gnn_epochs = 50, batches = 200, \n",
    "                   plot = False, attribute_select = \"attributes\"):\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batches, shuffle=True, collate_fn=collate, drop_last = True)\n",
    "    valid_loader = DataLoader(val, batch_size=batches, collate_fn=collate, drop_last = True)\n",
    "    test_loader = DataLoader(test, batch_size=batches, collate_fn=collate, drop_last = True)\n",
    "    \n",
    "    #set up initial GNN module \n",
    "    #find initial input dimension from attirbutes selected\n",
    "    gnn_input_dim = len(train[0][0].ndata[attribute_select].detach().numpy()[0])\n",
    "    reg_input_dim = len(train[0][1])\n",
    "    model = GraphRegressor(input_dim=gnn_input_dim, attribute_type = attribute_select, \n",
    "                           reg_input_dim = reg_input_dim)\n",
    "    \n",
    "    loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay = 0.001) #perhaps worth playing with \n",
    "\n",
    "    #optimizer specifying different learning rates: https://discuss.pytorch.org/t/how-to-specify-only-single-module-in-the-per-parameter-options/91008/2\n",
    "    #my_list = ['linear_reg.weight', 'linear_reg.bias']\n",
    "    #optimizer = torch.optim.Adam([{'params': [p for n, p in model.named_parameters() if n not in my_list]},\n",
    "    #            {'params': [p for n, p in model.named_parameters() if n in my_list], 'lr': 0.001}], \n",
    "    #                              lr=0.0001, weight_decay = 0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    \n",
    "    #set up model and parameters tracking performance\n",
    "    best_val_mse = np.inf \n",
    "    best_model = None\n",
    "    training_logs = []\n",
    "    num_epochs = gnn_epochs\n",
    "    \n",
    "    #train NN model\n",
    "    for epoch in trange(num_epochs):\n",
    "    \n",
    "        # training step\n",
    "        model.train()\n",
    "        train_mse = []\n",
    "        for i, (graphs, reg_data, labels) in enumerate(train_loader):\n",
    "\n",
    "            #print(graphs)\n",
    "            #squeeze operation \n",
    "            predictions = model(graphs, reg_data).squeeze()\n",
    "            print(predictions)\n",
    "            #print(predictions)\n",
    "            loss = loss_fn(predictions, labels)#+l1_regularizer(model, )\n",
    "            train_mse.append(loss.detach().numpy())\n",
    "            loss = loss.mean()\n",
    "            optimizer.zero_grad()\n",
    "            #loss.backward generates type problem: possibly linked to: https://discuss.pytorch.org/t/strange-error-from-loss-backward/62344\n",
    "            #loss = torch.tensor(loss, dtype = torch.float, requires_grad = True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #s = model.MLP1.weight.data.abs()\n",
    "            #print(s)\n",
    "            #lossl1 = l1_regularizer(model,)\n",
    "            #print(lossl1)\n",
    "        #print(model.linear_reg.weight.data.abs().sum())\n",
    "        train_mse = np.mean(np.concatenate(train_mse))\n",
    "        print(train_mse)\n",
    "\n",
    "        # validation step\n",
    "        model.eval()\n",
    "        valid_mse = []\n",
    "        for i, (graphs, reg_data,labels) in enumerate(valid_loader):\n",
    "            predictions = model(graphs,reg_data).squeeze()\n",
    "            #print(reg_data)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            valid_mse.append(loss.detach().numpy())\n",
    "        valid_mse = np.mean(np.concatenate(valid_mse))\n",
    "        \n",
    "        #print(valid_mse)\n",
    "\n",
    "        # early stopping \n",
    "        if valid_mse < best_val_mse:\n",
    "            best_val_mse = valid_mse\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "        training_logs.append((train_mse, valid_mse))\n",
    "\n",
    "    training_logs = np.array(training_logs)\n",
    "    \n",
    "    #evaluate model performance\n",
    "    model.load_state_dict(best_weights)\n",
    "    store_test_mse  = []\n",
    "    store_test_l1  = []\n",
    "    store_labels = []\n",
    "    store_predictions = []\n",
    "    \n",
    "    loss_fn_l1 = nn.L1Loss(reduction='none')\n",
    "    loss_fn_mse = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    for i, (graphs, reg_data, labels) in enumerate(test_loader):\n",
    "        predictions = model(graphs, reg_data).squeeze()\n",
    "        #evaluate l1 loss\n",
    "        loss_l1 = loss_fn_l1(predictions, labels)\n",
    "        store_test_l1.append(loss.detach().numpy())\n",
    "        #evaluate mse loss\n",
    "        loss_mse = loss_fn_mse(predictions, labels)\n",
    "        store_test_mse.append(loss.detach().numpy())\n",
    "        #store labels\n",
    "        store_labels.append(labels.detach().numpy())\n",
    "        store_predictions.append(predictions.detach().numpy())\n",
    "    \n",
    "    #store evaluation\n",
    "    r2 = r2_score(np.concatenate(store_labels), \n",
    "                  np.concatenate(store_predictions))\n",
    "    test_mse = np.mean(np.concatenate(store_test_mse))\n",
    "    test_l1 = np.mean(np.concatenate(store_test_l1))\n",
    "    \n",
    "    #print(model.linear_comb.weight.data)\n",
    "    \n",
    "    if (plot == True):\n",
    "        #plot how model performance evovles over time \n",
    "        fig, ax = plt.subplots()\n",
    "        skip = 5\n",
    "        ax.plot(range(skip, num_epochs), training_logs[skip:,0], label='train')\n",
    "        ax.plot(range(skip, num_epochs), training_logs[skip:,1], label='valid')\n",
    "        ax.plot([skip, num_epochs], [test_l1, test_l1], label='test', linestyle='dashed')\n",
    "        ax.legend()\n",
    "        \n",
    "    \n",
    "    return(r2, test_mse, test_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7757a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with different lookback and lookforward\n",
    "\n",
    "date_combos = [('40D', '30D'), ('60D', '30D'), ('20D', '10D')]\n",
    "\n",
    "\n",
    "for comb in date_combos:\n",
    "    lookback = comb[0]\n",
    "    lookforward = comb[1]\n",
    "    \n",
    "    #########create tables\n",
    "    reg_table, graph_table = create_graph_from_spec(markets, period = lookback, \n",
    "                               lookback = 1, lookforward = lookforward, inspect = False)\n",
    "    \n",
    "    #########find pred group\n",
    "    #recalculate graphs with pred groups\n",
    "    graph_new = pred_group_for_graphs(graph_table, lookbacks = 1)\n",
    "    graph_new = graph_new.merge(reg_table[[\"pred_group\", \"ticker\", 'num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin']],\n",
    "                 on = [\"pred_group\", \"ticker\"]) \n",
    "\n",
    "    #check no strange rows have been generated - implying a bad match between reg and graph table\n",
    "    if(not graph_new[pd.isna(graph_new[\"created_date\"])].empty): print(\"error in \" + str(i))\n",
    "    if(not graph_new[pd.isna(graph_new[\"num_submissions_future\"])].empty): print(\"error in \" + str(i))\n",
    "\n",
    "    #convert created date to utc timestamp\n",
    "    graph_new[\"created_utc\"] = graph_new[\"created_date\"].apply(lambda x: x.timestamp())\n",
    "    \n",
    "    #########log tranform dep variable\n",
    "    graph_new = log_transform(graph_new)\n",
    "    reg_table = log_transform(reg_table)\n",
    "\n",
    "    graph_new = log_transform(graph_new, y_col = \"num_comments_future\")\n",
    "    reg_table = log_transform(reg_table, y_col = \"num_comments_future\")\n",
    "    \n",
    "    ########TVT split\n",
    "    \n",
    "    regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                        graph_new,\n",
    "                                                                                         rand_seed = 10,\n",
    "                                                                                                y_col = \"num_comments_future_log\",\n",
    "                        y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                             'num_submissions_future_log', 'num_comments_future_log'])\n",
    "\n",
    "    graph_table = graph_table.dropna()\n",
    "    \n",
    "    #######Run penalized regression\n",
    "    r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "    \n",
    "    print(\"\\nReg r2 = \" + str(r2_reg))\n",
    "    \n",
    "    #######prepare and run GNN\n",
    "    \n",
    "    dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")\n",
    "    \n",
    "    graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                     graphs_table = graph_table)\n",
    "    \n",
    "    r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                       plot= True, gnn_epochs = 70)\n",
    "    \n",
    "    print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762261a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abefb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########create tables\n",
    "reg_table, graph_table = create_graph_from_spec(markets, period = \"60D\", \n",
    "                           lookback = 1, lookforward = \"30D\", inspect = False)\n",
    "\n",
    "#########find pred group\n",
    "#recalculate graphs with pred groups\n",
    "graph_new = pred_group_for_graphs(graph_table, lookbacks = 1)\n",
    "graph_new = graph_new.merge(reg_table[[\"pred_group\", \"ticker\", 'num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin']],\n",
    "             on = [\"pred_group\", \"ticker\"]) \n",
    "\n",
    "#check no strange rows have been generated - implying a bad match between reg and graph table\n",
    "if(not graph_new[pd.isna(graph_new[\"created_date\"])].empty): print(\"error in \" + str(i))\n",
    "if(not graph_new[pd.isna(graph_new[\"num_submissions_future\"])].empty): print(\"error in \" + str(i))\n",
    "\n",
    "#convert created date to utc timestamp\n",
    "graph_new[\"created_utc\"] = graph_new[\"created_date\"].apply(lambda x: x.timestamp())\n",
    "\n",
    "#########log tranform dep variable\n",
    "graph_new = log_transform(graph_new)\n",
    "reg_table = log_transform(reg_table)\n",
    "\n",
    "graph_new = log_transform(graph_new, y_col = \"num_comments_future\")\n",
    "reg_table = log_transform(reg_table, y_col = \"num_comments_future\")\n",
    "\n",
    "for seed in seeds:\n",
    "    \n",
    "    ########TVT split\n",
    "    \n",
    "    regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                        graph_new,\n",
    "                                                                                         rand_seed = seed,\n",
    "                                                                                                y_col = \"num_comments_future_log\",\n",
    "                        y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                             'num_submissions_future_log', 'num_comments_future_log'])\n",
    "\n",
    "    graph_table = graph_table.dropna()\n",
    "    \n",
    "    #######Run penalized regression\n",
    "    r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "    \n",
    "    print(\"\\nReg r2 = \" + str(r2_reg))\n",
    "    \n",
    "    #######prepare and run GNN\n",
    "    \n",
    "    dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")\n",
    "    \n",
    "    graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                     graphs_table = graph_table)\n",
    "    \n",
    "    r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                       plot= True, gnn_epochs = 70)\n",
    "    r2_gnn2, test_mse, test_l1 = run_gnn_module2(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                       plot= True, gnn_epochs = 70)\n",
    "    \n",
    "    print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "    print(str(seed))\n",
    "    \n",
    "    print(\"Reg r2 = \" + str(r2_reg))\n",
    "    \n",
    "    print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "    \n",
    "\n",
    "    print(\"GNN fully connected final layer r2 = \" + str(r2_gnn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f579ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########create tables\n",
    "reg_table, graph_table = create_graph_from_spec(markets, period = \"60D\", \n",
    "                           lookback = 1, lookforward = \"30D\", inspect = False)\n",
    "\n",
    "#########find pred group\n",
    "#recalculate graphs with pred groups\n",
    "graph_new = pred_group_for_graphs(graph_table, lookbacks = 1)\n",
    "graph_new = graph_new.merge(reg_table[[\"pred_group\", \"ticker\", 'num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin']],\n",
    "             on = [\"pred_group\", \"ticker\"]) \n",
    "\n",
    "#check no strange rows have been generated - implying a bad match between reg and graph table\n",
    "if(not graph_new[pd.isna(graph_new[\"created_date\"])].empty): print(\"error in \" + str(i))\n",
    "if(not graph_new[pd.isna(graph_new[\"num_submissions_future\"])].empty): print(\"error in \" + str(i))\n",
    "\n",
    "#convert created date to utc timestamp\n",
    "graph_new[\"created_utc\"] = graph_new[\"created_date\"].apply(lambda x: x.timestamp())\n",
    "\n",
    "#########log tranform dep variable\n",
    "graph_new = log_transform(graph_new)\n",
    "reg_table = log_transform(reg_table)\n",
    "\n",
    "graph_new = log_transform(graph_new, y_col = \"num_comments_future\")\n",
    "reg_table = log_transform(reg_table, y_col = \"num_comments_future\")\n",
    "    \n",
    "regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                    graph_new,\n",
    "                                                                                     rand_seed = 10,\n",
    "                                                                                            y_col = \"num_comments_future_log\",\n",
    "                    y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                         'num_submissions_future_log', 'num_comments_future_log'],\n",
    "                    stratify = False)\n",
    "\n",
    "graph_table = graph_table.dropna()\n",
    "\n",
    "#######Run penalized regression\n",
    "r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "\n",
    "#######prepare and run GNN\n",
    "\n",
    "dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")\n",
    "\n",
    "graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                 graphs_table = graph_table)\n",
    "\n",
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "r2_gnn2, test_mse, test_l1 = run_gnn_module2(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "\n",
    "print(\"Reg r2 = \" + str(r2_reg))\n",
    "\n",
    "print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "\n",
    "\n",
    "print(\"GNN fully connected final layer r2 = \" + str(r2_gnn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8f9233",
   "metadata": {},
   "source": [
    "# Additional GNN architecture experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c10c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change number of output heads for final layer\n",
    "class GraphRegressor(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, reg_input_dim, hidden_dim = 25, attribute_type = \"attributes\", \n",
    "                 num_heads_hidden = 10, num_heads_out = 10):\n",
    "        super(GraphRegressor, self).__init__()\n",
    "        \n",
    "        #set up for GNN\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attribute_type = attribute_type\n",
    "        self.graph_conv1 = GATConv(input_dim, hidden_dim, num_heads_hidden, attn_drop = 0.2, \n",
    "                                     feat_drop = 0.2)\n",
    "        self.graph_conv2 = GATConv(hidden_dim*num_heads_hidden, hidden_dim, num_heads_out, attn_drop = 0.2, \n",
    "                                     feat_drop = 0.2)\n",
    "        self.pooling1 = AvgPooling()\n",
    "        self.pooling2 = MaxPooling()\n",
    "        self.pooling3 = SumPooling()\n",
    "        self.MLP = nn.Linear(hidden_dim*num_heads_out, 1)\n",
    "        \n",
    "        #set up for penalized linear regression\n",
    "        self.reg_input_dim = reg_input_dim\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.linear_reg = nn.Linear(reg_input_dim, 1)\n",
    "        \n",
    "        #combine output from \n",
    "        self.linear_comb = nn.Linear(4,1)\n",
    "        \n",
    "    def forward(self, graph, reg_data):\n",
    "        \n",
    "        #run GNN\n",
    "        x = graph.ndata[self.attribute_type] # extract node attributes in an n x d tensor\n",
    "        x = torch.relu(self.graph_conv1(graph, x))\n",
    "        #reshapes to combine heads: https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch \n",
    "        # -1 singifies don't know how many rows are needed, but know how many columns \n",
    "        x = x.view(-1, x.size(1)* x.size(2))\n",
    "        #print(x.shape)\n",
    "        x = torch.relu(self.graph_conv2(graph, x))\n",
    "        x = x.view(-1, x.size(1)* x.size(2)) #reshape second time with multiple heads\n",
    "        x1 = self.pooling1(graph, x)\n",
    "        x1 = self.MLP(x1)\n",
    "        x2 = self.pooling2(graph, x)\n",
    "        x2 = self.MLP(x2)\n",
    "        x3 = self.pooling3(graph, x)\n",
    "        x3 = self.MLP(x3)\n",
    "        \n",
    "        #run regression\n",
    "        #reg_data = self.dropout(reg_data)\n",
    "        out = self.linear_reg(reg_data)\n",
    "        \n",
    "        #combine regression and GNN output\n",
    "        #x_fin = torch.cat((x3,out),1)\n",
    "        x_fin = torch.cat((x1,x2,x3,out),1)\n",
    "        #print(x3)\n",
    "        #print(x2)\n",
    "        #print(x3)\n",
    "        #return x, reg_data\n",
    "        x_fin = self.linear_comb(x_fin)\n",
    "        #print(x.dtype)\n",
    "        return x_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gnn_module(train, val, test, \n",
    "                   gnn_epochs = 50, batches = 200, \n",
    "                   plot = False, attribute_select = \"attributes\"):\n",
    "\n",
    "    train_loader = DataLoader(train, batch_size=batches, shuffle=True, collate_fn=collate, drop_last = True)\n",
    "    valid_loader = DataLoader(val, batch_size=batches, collate_fn=collate, drop_last = True)\n",
    "    test_loader = DataLoader(test, batch_size=batches, collate_fn=collate, drop_last = True)\n",
    "    \n",
    "    #set up initial GNN module \n",
    "    #find initial input dimension from attirbutes selected\n",
    "    gnn_input_dim = len(train[0][0].ndata[attribute_select].detach().numpy()[0])\n",
    "    reg_input_dim = len(train[0][1])\n",
    "    model = GraphRegressor(input_dim=gnn_input_dim, attribute_type = attribute_select, \n",
    "                           reg_input_dim = reg_input_dim)\n",
    "    \n",
    "    loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay = 0.001) #perhaps worth playing with \n",
    "\n",
    "    #optimizer specifying different learning rates: https://discuss.pytorch.org/t/how-to-specify-only-single-module-in-the-per-parameter-options/91008/2\n",
    "    #my_list = ['linear_reg.weight', 'linear_reg.bias']\n",
    "    #optimizer = torch.optim.Adam([{'params': [p for n, p in model.named_parameters() if n not in my_list]},\n",
    "    #            {'params': [p for n, p in model.named_parameters() if n in my_list], 'lr': 0.001}], \n",
    "    #                              lr=0.0001, weight_decay = 0.001)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "    \n",
    "    #set up model and parameters tracking performance\n",
    "    best_val_mse = np.inf \n",
    "    best_model = None\n",
    "    training_logs = []\n",
    "    num_epochs = gnn_epochs\n",
    "    \n",
    "    #train NN model\n",
    "    for epoch in trange(num_epochs):\n",
    "    \n",
    "        # training step\n",
    "        model.train()\n",
    "        train_mse = []\n",
    "        for i, (graphs, reg_data, labels) in enumerate(train_loader):\n",
    "\n",
    "            #print(graphs)\n",
    "            #squeeze operation \n",
    "            predictions = model(graphs, reg_data).squeeze()\n",
    "            #print(predictions)\n",
    "            #print(predictions)\n",
    "            loss = loss_fn(predictions, labels)#+l1_regularizer(model, )\n",
    "            train_mse.append(loss.detach().numpy())\n",
    "            loss = loss.mean()\n",
    "            optimizer.zero_grad()\n",
    "            #loss.backward generates type problem: possibly linked to: https://discuss.pytorch.org/t/strange-error-from-loss-backward/62344\n",
    "            #loss = torch.tensor(loss, dtype = torch.float, requires_grad = True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #s = model.MLP1.weight.data.abs()\n",
    "            #print(s)\n",
    "            #lossl1 = l1_regularizer(model,)\n",
    "            #print(lossl1)\n",
    "        #print(model.linear_reg.weight.data.abs().sum())\n",
    "        train_mse = np.mean(np.concatenate(train_mse))\n",
    "        #print(train_mse)\n",
    "\n",
    "        # validation step\n",
    "        model.eval()\n",
    "        valid_mse = []\n",
    "        for i, (graphs, reg_data,labels) in enumerate(valid_loader):\n",
    "            predictions = model(graphs,reg_data).squeeze()\n",
    "            #print(reg_data)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            valid_mse.append(loss.detach().numpy())\n",
    "        valid_mse = np.mean(np.concatenate(valid_mse))\n",
    "        \n",
    "        #print(valid_mse)\n",
    "\n",
    "        # early stopping \n",
    "        if valid_mse < best_val_mse:\n",
    "            best_val_mse = valid_mse\n",
    "            best_weights = model.state_dict()\n",
    "\n",
    "        training_logs.append((train_mse, valid_mse))\n",
    "\n",
    "    training_logs = np.array(training_logs)\n",
    "    \n",
    "    #evaluate model performance\n",
    "    model.load_state_dict(best_weights)\n",
    "    store_test_mse  = []\n",
    "    store_test_l1  = []\n",
    "    store_labels = []\n",
    "    store_predictions = []\n",
    "    \n",
    "    loss_fn_l1 = nn.L1Loss(reduction='none')\n",
    "    loss_fn_mse = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    for i, (graphs, reg_data, labels) in enumerate(test_loader):\n",
    "        predictions = model(graphs, reg_data).squeeze()\n",
    "        #evaluate l1 loss\n",
    "        loss_l1 = loss_fn_l1(predictions, labels)\n",
    "        store_test_l1.append(loss.detach().numpy())\n",
    "        #evaluate mse loss\n",
    "        loss_mse = loss_fn_mse(predictions, labels)\n",
    "        store_test_mse.append(loss.detach().numpy())\n",
    "        #store labels\n",
    "        store_labels.append(labels.detach().numpy())\n",
    "        store_predictions.append(predictions.detach().numpy())\n",
    "    \n",
    "    #store evaluation\n",
    "    r2 = r2_score(np.concatenate(store_labels), \n",
    "                  np.concatenate(store_predictions))\n",
    "    test_mse = np.mean(np.concatenate(store_test_mse))\n",
    "    test_l1 = np.mean(np.concatenate(store_test_l1))\n",
    "    \n",
    "    #print(model.linear_comb.weight.data)\n",
    "    \n",
    "    if (plot == True):\n",
    "        #plot how model performance evovles over time \n",
    "        fig, ax = plt.subplots()\n",
    "        skip = 5\n",
    "        ax.plot(range(skip, num_epochs), training_logs[skip:,0], label='train')\n",
    "        ax.plot(range(skip, num_epochs), training_logs[skip:,1], label='valid')\n",
    "        ax.plot([skip, num_epochs], [test_l1, test_l1], label='test', linestyle='dashed')\n",
    "        ax.legend()\n",
    "        \n",
    "    \n",
    "    return(r2, test_mse, test_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########create tables\n",
    "reg_table, graph_table = create_graph_from_spec(markets, period = \"60D\", \n",
    "                           lookback = 1, lookforward = \"30D\", inspect = False)\n",
    "\n",
    "#########find pred group\n",
    "#recalculate graphs with pred groups\n",
    "graph_new = pred_group_for_graphs(graph_table, lookbacks = 1)\n",
    "graph_new = graph_new.merge(reg_table[[\"pred_group\", \"ticker\", 'num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin']],\n",
    "             on = [\"pred_group\", \"ticker\"]) \n",
    "\n",
    "#check no strange rows have been generated - implying a bad match between reg and graph table\n",
    "if(not graph_new[pd.isna(graph_new[\"created_date\"])].empty): print(\"error in \" + str(i))\n",
    "if(not graph_new[pd.isna(graph_new[\"num_submissions_future\"])].empty): print(\"error in \" + str(i))\n",
    "\n",
    "#convert created date to utc timestamp\n",
    "graph_new[\"created_utc\"] = graph_new[\"created_date\"].apply(lambda x: x.timestamp())\n",
    "\n",
    "#########log tranform dep variable\n",
    "graph_new = log_transform(graph_new)\n",
    "reg_table = log_transform(reg_table)\n",
    "\n",
    "graph_new = log_transform(graph_new, y_col = \"num_comments_future\")\n",
    "reg_table = log_transform(reg_table, y_col = \"num_comments_future\")\n",
    "    \n",
    "regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                    graph_new,\n",
    "                                                                                     rand_seed = 10,\n",
    "                                                                                            y_col = \"num_comments_future_log\",\n",
    "                    y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                         'num_submissions_future_log', 'num_comments_future_log'],\n",
    "                    stratify = False)\n",
    "\n",
    "graph_table = graph_table.dropna()\n",
    "\n",
    "#######Run penalized regression\n",
    "r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "\n",
    "#######prepare and run GNN\n",
    "\n",
    "dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")\n",
    "\n",
    "graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                 graphs_table = graph_table)\n",
    "\n",
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "#r2_gnn2, test_mse, test_l1 = run_gnn_module2(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "#                                   plot= True, gnn_epochs = 70)\n",
    "\n",
    "print(\"Reg r2 = \" + str(r2_reg))\n",
    "\n",
    "print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "\n",
    "\n",
    "#print(\"GNN fully connected final layer r2 = \" + str(r2_gnn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e80dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_gnn1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cf0c9",
   "metadata": {},
   "source": [
    "# Experiment with data truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c3a8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########create tables\n",
    "reg_table, graph_table = create_graph_from_spec(markets, period = \"60D\", \n",
    "                           lookback = 1, lookforward = \"30D\", inspect = False)\n",
    "\n",
    "#########find pred group\n",
    "#recalculate graphs with pred groups\n",
    "graph_new = pred_group_for_graphs(graph_table, lookbacks = 1)\n",
    "graph_new = graph_new.merge(reg_table[[\"pred_group\", \"ticker\", 'num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin']],\n",
    "             on = [\"pred_group\", \"ticker\"]) \n",
    "\n",
    "#check no strange rows have been generated - implying a bad match between reg and graph table\n",
    "if(not graph_new[pd.isna(graph_new[\"created_date\"])].empty): print(\"error in \" + str(i))\n",
    "if(not graph_new[pd.isna(graph_new[\"num_submissions_future\"])].empty): print(\"error in \" + str(i))\n",
    "\n",
    "#convert created date to utc timestamp\n",
    "graph_new[\"created_utc\"] = graph_new[\"created_date\"].apply(lambda x: x.timestamp())\n",
    "\n",
    "#########log tranform dep variable\n",
    "graph_new = log_transform(graph_new)\n",
    "reg_table = log_transform(reg_table)\n",
    "\n",
    "graph_new = log_transform(graph_new, y_col = \"num_comments_future\")\n",
    "reg_table = log_transform(reg_table, y_col = \"num_comments_future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762081dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########truncate data for at least 5 submissions in prior periods\n",
    "reg_table = reg_table[(reg_table[\"total_sub\"] > 5)].copy()\n",
    "graph_new= graph_new.merge(reg_table[[\"pred_group\", \"ticker\"]],\n",
    "                                    on = [\"pred_group\", \"ticker\"],\n",
    "                                    how = \"right\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter over 5 ticker mentions\n",
    "\n",
    "regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                    graph_new,\n",
    "                                                                                     rand_seed = 10,\n",
    "                                                                                            y_col = \"num_comments_future_log\",\n",
    "                    y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                         'num_submissions_future_log', 'num_comments_future_log'],\n",
    "                    stratify = False)\n",
    "print(\"done\")\n",
    "graph_table = graph_table.dropna()\n",
    "\n",
    "#######Run penalized regression\n",
    "r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "\n",
    "#######prepare and run GNN\n",
    "\n",
    "dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")\n",
    "\n",
    "graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                 graphs_table = graph_table)\n",
    "\n",
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "r2_gnn2, test_mse, test_l1 = run_gnn_module2(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "\n",
    "print(\"Reg r2 = \" + str(r2_reg))\n",
    "\n",
    "print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "\n",
    "\n",
    "print(\"GNN fully connected final layer r2 = \" + str(r2_gnn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558b2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter over 10 ticker mentions\n",
    "\n",
    "regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                    graph_new,\n",
    "                                                                                     rand_seed = 10,\n",
    "                                                                                            y_col = \"num_comments_future_log\",\n",
    "                    y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                         'num_submissions_future_log', 'num_comments_future_log'],\n",
    "                    stratify = True)\n",
    "print(\"done\")\n",
    "graph_table = graph_table.dropna()\n",
    "\n",
    "#######Run penalized regression\n",
    "r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "\n",
    "#######prepare and run GNN\n",
    "\n",
    "dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")\n",
    "\n",
    "graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                 graphs_table = graph_table)\n",
    "\n",
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "r2_gnn2, test_mse, test_l1 = run_gnn_module2(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "\n",
    "print(\"Reg r2 = \" + str(r2_reg))\n",
    "\n",
    "print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "\n",
    "\n",
    "print(\"GNN fully connected final layer r2 = \" + str(r2_gnn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ca4f3",
   "metadata": {},
   "source": [
    "# Log dependent as well as independent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77739b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform2(table, col):\n",
    "    table[col] = np.log(table[col]+1)\n",
    "    return(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37193605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########create tables\n",
    "reg_table, graph_table = create_graph_from_spec(markets, period = \"60D\", \n",
    "                           lookback = 1, lookforward = \"30D\", inspect = False)\n",
    "\n",
    "#########find pred group\n",
    "#recalculate graphs with pred groups\n",
    "graph_new = pred_group_for_graphs(graph_table, lookbacks = 1)\n",
    "graph_new = graph_new.merge(reg_table[[\"pred_group\", \"ticker\", 'num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin']],\n",
    "             on = [\"pred_group\", \"ticker\"]) \n",
    "\n",
    "#check no strange rows have been generated - implying a bad match between reg and graph table\n",
    "if(not graph_new[pd.isna(graph_new[\"created_date\"])].empty): print(\"error in \" + str(i))\n",
    "if(not graph_new[pd.isna(graph_new[\"num_submissions_future\"])].empty): print(\"error in \" + str(i))\n",
    "\n",
    "#convert created date to utc timestamp\n",
    "graph_new[\"created_utc\"] = graph_new[\"created_date\"].apply(lambda x: x.timestamp())\n",
    "\n",
    "#########log tranform dep variable\n",
    "graph_new = log_transform(graph_new)\n",
    "reg_table = log_transform(reg_table)\n",
    "\n",
    "graph_new = log_transform(graph_new, y_col = \"num_comments_future\")\n",
    "reg_table = log_transform(reg_table, y_col = \"num_comments_future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table = reg_table[(reg_table[\"total_sub\"] > 5)].copy()\n",
    "graph_new= graph_new.merge(reg_table[[\"pred_group\", \"ticker\"]],\n",
    "                                    on = [\"pred_group\", \"ticker\"],\n",
    "                                    how = \"right\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols_reg_transform = ['total_sub', 'total_com', 'max_num_com',\n",
    "       'max_num_auth', 'overall_posts', 'overall_engagements', 'overall_authors',\n",
    "       'mkt_cap', 'vol_mean', 'vol_std']\n",
    "\n",
    "x_cols_graph_transform = ['total_com', 'mkt_cap', 'num_authors','num_posts']\n",
    "\n",
    "for col in x_cols_reg_transform:\n",
    "    reg_table = log_transform2(reg_table, col = col)\n",
    "    \n",
    "for col in x_cols_graph_transform:\n",
    "    graph_new = log_transform2(graph_new, col = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005427b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter over 5 ticker mentions\n",
    "\n",
    "regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                    graph_new,\n",
    "                                                                                     rand_seed = 10,\n",
    "                                                                                            y_col = \"num_comments_future_log\",\n",
    "                    y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                         'num_submissions_future_log', 'num_comments_future_log'],\n",
    "                    stratify = False)\n",
    "print(\"done\")\n",
    "graph_table = graph_table.dropna()\n",
    "\n",
    "#######Run penalized regression\n",
    "r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "\n",
    "#######prepare and run GNN\n",
    "\n",
    "dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")\n",
    "\n",
    "graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                 graphs_table = graph_table)\n",
    "\n",
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "r2_gnn2, test_mse, test_l1 = run_gnn_module2(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "\n",
    "print(\"Reg r2 = \" + str(r2_reg))\n",
    "\n",
    "print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "\n",
    "\n",
    "print(\"GNN fully connected final layer r2 = \" + str(r2_gnn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table = reg_table.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62630715",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_new = graph_new.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945bbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no filter to number of ticker mentions\n",
    "\n",
    "regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                    graph_new,\n",
    "                                                                                     rand_seed = 10,\n",
    "                                                                                            y_col = \"num_comments_future_log\",\n",
    "                    y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                         'num_submissions_future_log', 'num_comments_future_log'],\n",
    "                    stratify = False)\n",
    "print(\"done\")\n",
    "graph_table = graph_table.dropna()\n",
    "\n",
    "#######Run penalized regression\n",
    "#r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "\n",
    "#######prepare and run GNN\n",
    "\n",
    "dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")\n",
    "\n",
    "graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                 graphs_table = graph_table)\n",
    "\n",
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "r2_gnn2, test_mse, test_l1 = run_gnn_module2(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 70)\n",
    "\n",
    "print(\"Reg r2 = \" + str(r2_reg))\n",
    "\n",
    "print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "\n",
    "\n",
    "print(\"GNN fully connected final layer r2 = \" + str(r2_gnn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ca41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50414a55",
   "metadata": {},
   "source": [
    "# Ingest author features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d37f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprocessor import GraphMaker as GM\n",
    "\n",
    "dl = GM(load_data=False)\n",
    "\n",
    "subs = dl.load_data('submissions')\n",
    "coms = dl.load_data('comments')\n",
    "markets = dl.load_data('market_data')\n",
    "act = dl.load_data('activity')\n",
    "\n",
    "markets = dl.create_market_variables(markets, inplace=False, window=30, min_periods=10)\n",
    "subs, coms, act = dl.apply_filters(subs, coms, act, inplace=False, lookback_freq='60D',lookforward_freq='30D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f671c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table = dl.create_regression_table(subs, coms, act, markets)\n",
    "graph_table = dl.submission_agg(subs, coms, markets, act, get_sub_age=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b56aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join sub writer specific columns to graph table\n",
    "author_columns = ['sub_count_auth_all',\n",
    "       'sub_score_auth_all', 'sub_replies_auth_all', 'com_count_auth_all',\n",
    "       'com_score_auth_all', 'com_replies_auth_all', 'age_auth_all',\n",
    "       'sub_count_auth_ticker', 'sub_score_auth_ticker',\n",
    "       'sub_replies_auth_ticker', 'com_count_auth_ticker',\n",
    "       'com_score_auth_ticker', 'com_replies_auth_ticker', 'deleted_auth', \n",
    "                 \"id\"]\n",
    "graph_table = pd.merge(graph_table, subs[author_columns],\n",
    "                      left_on = \"submission_id\",\n",
    "                      right_on = \"id\",\n",
    "                      how = \"left\")\n",
    "graph_table = graph_table.drop(columns = [\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ccb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_new = pred_group_for_graphs(graph_table, lookbacks = 1)\n",
    "graph_new = graph_new.merge(reg_table[[\"pred_group\", \"ticker\", 'num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin']],\n",
    "             on = [\"pred_group\", \"ticker\"]) \n",
    "\n",
    "#check no strange rows have been generated - implying a bad match between reg and graph table\n",
    "if(not graph_new[pd.isna(graph_new[\"created_date\"])].empty): print(\"error in \" + str(i))\n",
    "if(not graph_new[pd.isna(graph_new[\"num_submissions_future\"])].empty): print(\"error in \" + str(i))\n",
    "\n",
    "#convert created date to utc timestamp\n",
    "graph_new[\"created_utc\"] = graph_new[\"created_date\"].apply(lambda x: x.timestamp())\n",
    "\n",
    "#########log tranform dep variable\n",
    "graph_new = log_transform(graph_new)\n",
    "reg_table = log_transform(reg_table)\n",
    "\n",
    "graph_new = log_transform(graph_new, y_col = \"num_comments_future\")\n",
    "reg_table = log_transform(reg_table, y_col = \"num_comments_future\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3d2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_table = reg_table[(reg_table[\"total_sub\"] > 5)].copy()\n",
    "graph_new= graph_new.merge(reg_table[[\"pred_group\", \"ticker\"]],\n",
    "                                    on = [\"pred_group\", \"ticker\"],\n",
    "                                    how = \"right\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0400acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transform certain independent variables \n",
    "x_cols_reg_transform = ['total_sub', 'total_com', 'max_num_com',\n",
    "       'max_num_auth', 'avg_num_auth', 'total_auth', 'overall_posts', 'overall_engagements', 'overall_authors',\n",
    "        'overall_posts',\n",
    "       'mkt_cap', 'vol_mean', 'vol_std',\n",
    "        'avg_sub_count_auth_all',\n",
    "       'max_sub_count_auth_all',\n",
    "       'max_sub_score_auth_all', 'avg_sub_replies_auth_all',\n",
    "       'max_sub_replies_auth_all', 'avg_com_count_auth_all',\n",
    "       'max_com_count_auth_all',\n",
    "       'max_com_score_auth_all', 'avg_com_replies_auth_all',\n",
    "       'max_com_replies_auth_all', 'avg_age_auth_all', 'max_age_auth_all',\n",
    "       'avg_sub_count_auth_ticker', 'max_sub_count_auth_ticker', 'max_sub_score_auth_ticker',\n",
    "       'avg_sub_replies_auth_ticker', 'max_sub_replies_auth_ticker',\n",
    "       'avg_com_count_auth_ticker', 'max_com_count_auth_ticker', 'max_com_score_auth_ticker',\n",
    "       'avg_com_replies_auth_ticker', 'max_com_replies_auth_ticker',\n",
    "       'count_deleted_auth', 'avg_num_com']\n",
    "\n",
    "x_cols_graph_transform = ['total_com', 'mkt_cap', 'num_authors','num_posts',\n",
    "                         'sub_count_auth_all', \n",
    "       'sub_replies_auth_all', 'com_count_auth_all', \n",
    "       'com_replies_auth_all', 'age_auth_all', 'sub_count_auth_ticker', 'sub_replies_auth_ticker',\n",
    "       'com_count_auth_ticker', \n",
    "       'com_replies_auth_ticker']\n",
    "\n",
    "for col in x_cols_reg_transform:\n",
    "    reg_table = log_transform2(reg_table, col = col)\n",
    "    \n",
    "for col in x_cols_graph_transform:\n",
    "    graph_new = log_transform2(graph_new, col = col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6968d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log, graph_table = tvt_split(reg_table, \n",
    "                                                                                                    graph_new,\n",
    "                                                                                     rand_seed = 10,\n",
    "                                                                                            y_col = \"num_comments_future_log\",\n",
    "                    y_cols_to_elim_reg = ['num_submissions_future', 'num_comments_future', 'num_authors_future','ncf_bin',\n",
    "                                         'num_submissions_future_log', 'num_comments_future_log'],\n",
    "                    x_cols_graph = ['avg_level','max_level', 'total_com', 'num_auth', 'pct_ch',\n",
    "           'mkt_cap', 'log_return_mean', 'log_return_std', 'num_authors','num_posts', 'num_engagements',\n",
    "           'sub_age', 'sub_count_auth_all', 'sub_score_auth_all', 'deleted_auth'],\n",
    "                    stratify = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbbea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "                reg_columns = ['total_sub',\n",
    " 'avg_level_reg',\n",
    " 'max_level_reg',\n",
    " 'total_com_reg',\n",
    " 'max_num_com',\n",
    " 'max_num_auth',\n",
    " 'avg_num_auth',\n",
    " 'total_auth',\n",
    " 'avg_sub_count_auth_all_reg',\n",
    " 'max_sub_count_auth_all_reg',\n",
    " 'avg_sub_replies_auth_all_reg',\n",
    " 'max_sub_replies_auth_all_reg',\n",
    " 'avg_com_count_auth_all_reg',\n",
    " 'max_com_count_auth_all_reg',\n",
    " 'avg_com_replies_auth_all_reg',\n",
    " 'max_com_replies_auth_all_reg',\n",
    " 'avg_age_auth_all_reg',\n",
    " 'max_age_auth_all_reg',\n",
    " 'avg_sub_count_auth_ticker_reg',\n",
    " 'max_sub_count_auth_ticker_reg',\n",
    " 'avg_sub_replies_auth_ticker_reg',\n",
    " 'max_sub_replies_auth_ticker_reg',\n",
    " 'avg_com_count_auth_ticker_reg',\n",
    " 'max_com_count_auth_ticker_reg',\n",
    " 'avg_com_replies_auth_ticker_reg',\n",
    " 'max_com_replies_auth_ticker_reg',\n",
    " 'count_deleted_auth_reg',\n",
    " 'avg_num_com',\n",
    " 'overall_posts',\n",
    " 'overall_engagements',\n",
    " 'overall_authors',\n",
    " 'return_mean',\n",
    " 'return_std',\n",
    " 'vol_mean',\n",
    " 'vol_std',\n",
    " 'log_return_mean_reg',\n",
    " 'log_return_std_reg',\n",
    " 'return_var',\n",
    " 'vol_var',\n",
    " 'mkt_cap_reg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2888e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = ['avg_level','max_level', 'total_com', 'num_auth', 'pct_ch',\n",
    "           'mkt_cap', 'log_return_mean', 'log_return_std', 'num_authors','num_posts', 'num_engagements',\n",
    "           'sub_age', 'sub_count_auth_all',\n",
    "       'sub_replies_auth_all', 'com_count_auth_all',\n",
    "       'com_replies_auth_all', 'age_auth_all', 'sub_count_auth_ticker', 'sub_replies_auth_ticker',\n",
    "       'com_count_auth_ticker',\n",
    "       'com_replies_auth_ticker', 'deleted_auth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87026bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_create(graph_table, pred_group_select, ticker_select,\n",
    "                 \n",
    "                y_col = \"num_submissions_future\",\n",
    "                reg_columns = ['total_sub',\n",
    " 'avg_level_reg',\n",
    " 'max_level_reg',\n",
    " 'total_com_reg',\n",
    " 'max_num_com',\n",
    " 'max_num_auth',\n",
    " 'avg_num_auth',\n",
    " 'total_auth',\n",
    " 'avg_num_com',\n",
    " 'overall_posts',\n",
    " 'overall_engagements',\n",
    " 'overall_authors',\n",
    " 'return_mean',\n",
    " 'return_std',\n",
    " 'vol_mean',\n",
    " 'vol_std',\n",
    " 'log_return_mean_reg',\n",
    " 'log_return_std_reg',\n",
    " 'return_var',\n",
    " 'vol_var',\n",
    " 'mkt_cap_reg'],\n",
    "                x_cols = ['avg_level','max_level', 'total_com', 'num_auth', 'pct_ch',\n",
    "           'mkt_cap', 'log_return_mean', 'log_return_std', 'num_authors','num_posts', 'num_engagements',\n",
    "           'sub_age', 'deleted_auth']):\n",
    "    \n",
    "    \"\"\"\n",
    "    graph_table: graph learning table created by gm.submission_agg and pred_group columns created using \n",
    "        pred_group_for_graphs function\n",
    "    pred_group_select: pred_group identifier\n",
    "    ticker_select: selected ticker\n",
    "    -- pred_group,ticker combination regers to unique graph \n",
    "    y_col: dependent variable selected to create graph label\n",
    "    \n",
    "    Function takes in graph table with specific pred_group, ticker - creates a graph \n",
    "    (storing all data from independent variables in a graph structure for learning) and a label - encoding dependt var\n",
    "    \n",
    "    graphs can be fed for learning into GNN\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #filter data to the data we are trying to use\n",
    "    graph = graph_table[((graph_table[\"pred_group\"] == pred_group_select) & \\\n",
    "                        (graph_table[\"ticker\"] == ticker_select))].copy()\n",
    "    \n",
    "    #label is the dependent variable - for each pred_group, ticker combination - label is constant\n",
    "        #if not - we print a warning\n",
    "    temp = list(set(graph[y_col]))\n",
    "    if(len(temp) > 1): print(\"too many dependent variables for \" + ticker_select + str(pred_group_select))\n",
    "    label = torch.tensor(np.float32(temp[0]))\n",
    "    \n",
    "    #extract reg- columns \n",
    "    \n",
    "    \n",
    "    #create list of all submissions - these are all the nodes in our graph\n",
    "    nodes = list(set(graph[\"submission_id\"]))\n",
    "    node_dict = {}\n",
    "    node_num = []\n",
    "    i = 0\n",
    "\n",
    "    node_attributes = []\n",
    "\n",
    "    #create a dict where we can go between numbers for node and name of node\n",
    "    #store node attributes here in NxD dimentional array - row number corresponds to node\n",
    "    for index, row in graph.iterrows():\n",
    "        \n",
    "        if(i == 0):\n",
    "            reg_vars = np.array(row[reg_columns].values, dtype = np.float32)\n",
    "            #reg_vars = reg_vars.reshape(-1,len(reg_vars))\n",
    "            #reg_vars = torch.tensor(np.array(reg_vars))\n",
    "        \n",
    "        node_dict[i] = row[\"submission_id\"]\n",
    "        node_num.append(i)\n",
    "        i = i+1\n",
    "        node_attributes.append(np.array(list(row[x_cols].values)).astype(np.float32))\n",
    "\n",
    "    #create complete graph from our numerically stored nodes\n",
    "    edges = list(combinations(node_num,2))\n",
    "    #print(edges)\n",
    "\n",
    "    #add self-loops\n",
    "    self_loops = [(x,x) for x in node_num]\n",
    "    edges.extend(self_loops)\n",
    "\n",
    "    adjacency = skn.utils.edgelist2adjacency(edges, undirected=True)\n",
    "\n",
    "    g = dgl.from_scipy(adjacency)\n",
    "    g.ndata[\"attributes\"] = torch.tensor(np.array(node_attributes))\n",
    "    \n",
    "    return(g,label, reg_vars)\n",
    "\n",
    "def table_to_graphs(graph_table, y_col_select = \"num_submissions_future\"):\n",
    "\n",
    "    pred_ticker_combos = list(set(graph_table[\"pred_ticker\"]))\n",
    "    \n",
    "    dgl_graphs = []\n",
    "    \n",
    "    for pred_ticker_combo in pred_ticker_combos:\n",
    "        g, label, reg_vars = graph_create(graph_table, \n",
    "                                pred_group_select = pred_ticker_combo[0], \n",
    "                                ticker_select = pred_ticker_combo[1],\n",
    "                               y_col = y_col_select,)\n",
    "        dgl_graphs.append((g,label, reg_vars, pred_ticker_combo))\n",
    "    \n",
    "    return(dgl_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c218fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_table = graph_table.dropna()\n",
    "\n",
    "#######Run penalized regression\n",
    "r2_reg, opt_model = run_reg_module(regX_train_log, regX_val_log, regX_test_log, regy_train_log, regy_val_log, regy_test_log)\n",
    "\n",
    "print(r2_reg)\n",
    "#######prepare and run GNN\n",
    "\n",
    "dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl_graphs_logy = table_to_graphs(graph_table, y_col_select = \"num_comments_future_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edebfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_train_logy, graph_val_logy, graph_test_logy = tvt_split_graphs(dataset = dgl_graphs_logy, \n",
    "                                                                 graphs_table = graph_table)\n",
    "\n",
    "r2_gnn1, test_mse, test_l1 = run_gnn_module(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 120)\n",
    "r2_gnn2, test_mse, test_l1 = run_gnn_module2(train = graph_train_logy, val = graph_val_logy, test = graph_test_logy, \n",
    "                                   plot= True, gnn_epochs = 120)\n",
    "\n",
    "\n",
    "print(\"GNN non fully connected final layer r2 = \" + str(r2_gnn1))\n",
    "\n",
    "\n",
    "print(\"GNN fully connected final layer r2 = \" + str(r2_gnn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d31ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
