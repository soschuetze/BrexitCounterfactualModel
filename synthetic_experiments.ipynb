{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Experiments using Stochastic Block Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from torch_geometric.loader import DataLoader\n",
    "from itertools import combinations\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from src.utils.CreateFeatures import CreateFeatures\n",
    "from src.pygcn.GCN_batched import GraphSiamese\n",
    "from torch_geometric.utils import to_networkx\n",
    "from src.utils.embedding import GCN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from typing import Union\n",
    "from src.utils.graphs import laplacian_embeddings, random_walk_embeddings, degree_matrix, identity\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from src.utils.sample import sample_pairs\n",
    "from src.utils.misc import collate\n",
    "from detect import detect_change_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def run_model(model, train_loader, val_loader):\n",
    "    torch.manual_seed(42)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Changed to BCEWithLogitsLoss for numerical stability\n",
    "\n",
    "    for epoch in tqdm(range(30)):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for data1, data2, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data1, data2)\n",
    "    \n",
    "            labels = labels.float().view(-1, 1)  # Ensure labels are of the shape (batch_size, 1)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "\n",
    "            val_pred = []\n",
    "            val_truth = []\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data1, data2, labels in val_loader:\n",
    "                out = model(data1, data2)\n",
    "\n",
    "                labels = labels.float().view(-1, 1)  # Ensure labels are of the shape (batch_size, 1)\n",
    "                val_loss = criterion(out, labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                predictions = torch.round(out)\n",
    "\n",
    "                val_pred.extend(predictions.cpu().numpy())\n",
    "                val_truth.extend(labels.cpu().numpy())\n",
    "\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "            val_loss = sum(val_losses) / len(val_losses)\n",
    "            val_accuracy = correct / total\n",
    "\n",
    "        val_f1 = f1_score(val_truth, val_pred)\n",
    "        print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}, Validation F1 Score: {val_f1}')\n",
    "    return val_accuracy, val_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clique Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Assuming root_dir is the path to your root directory\n",
    "root_dir = 'results/synthetic/'\n",
    "\n",
    "clique_data = {}\n",
    "cp_times = {}\n",
    "label_data = {}\n",
    "\n",
    "# Walk through all directories and files in root_dir\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # If there's a data.p file in this directory, read it\n",
    "    args_file = os.path.join(dirpath, 'args.json')\n",
    "    if os.path.isfile(args_file):\n",
    "        with open(args_file, 'rb') as f:\n",
    "            arg_data = json.load(f)\n",
    "            clique_size = arg_data['size_clique']\n",
    "\n",
    "    data_file = os.path.join(dirpath, 'data.p')\n",
    "    if os.path.isfile(data_file):\n",
    "        with open(data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            clique_data[clique_size] = data\n",
    "\n",
    "    # If there's a time.json file in this directory, read it\n",
    "    time_file = os.path.join(dirpath, 'time.json')\n",
    "    if os.path.isfile(time_file):\n",
    "        with open(time_file, 'r') as f:\n",
    "            time_data = json.load(f)\n",
    "            cp_times[clique_size] = time_data\n",
    "\n",
    "    label_file = os.path.join(dirpath, 'labels.p')\n",
    "    if os.path.isfile(label_file):\n",
    "        with open(label_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            label_data[clique_size] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [20, 30, 40, 50, 60, 70, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 positive and 1000 negative examples\n",
      "1000 positive and 1000 negative examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [02:36<1:15:47, 156.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.777213262957196, Validation Loss: 0.7506718858178839, Validation Accuracy: 0.6272590361445783, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [05:13<1:13:11, 156.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.7407366896784583, Validation Loss: 0.7265052860041699, Validation Accuracy: 0.6276355421686747, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [07:49<1:10:19, 156.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.7221748038779857, Validation Loss: 0.7145954248416855, Validation Accuracy: 0.6283885542168675, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [10:25<1:07:41, 156.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.7124104596847711, Validation Loss: 0.7078666155596813, Validation Accuracy: 0.6276355421686747, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [13:02<1:05:10, 156.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.7069445065287656, Validation Loss: 0.7040091401123139, Validation Accuracy: 0.6265060240963856, Validation F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "for s in [20]:\n",
    "    for j, i in enumerate(clique_data[s]):\n",
    "        edge_index = i.edge_index.to(torch.int64)\n",
    "        networkx_graph = to_networkx(i)\n",
    "        adjacency = nx.adjacency_matrix(networkx_graph)\n",
    "        \n",
    "        attributes = np.eye(adjacency.shape[0])\n",
    "        clique_data[s][j].x = attributes\n",
    "    \n",
    "    train = clique_data[s][:1000]\n",
    "    train_labels = label_data[s][:1000]\n",
    "\n",
    "    val = clique_data[s][1000:2000]\n",
    "    val_labels = label_data[s][1000:2000]\n",
    "\n",
    "    test = clique_data[s][2000:]\n",
    "    test_labels = label_data[s][2000:]\n",
    "\n",
    "    graph_pairs_train = sample_pairs(train,train_labels,nsamples=2000)\n",
    "    graph_pairs_val = sample_pairs(train,val_labels,nsamples=2000)\n",
    "\n",
    "    for j in graph_pairs_train:\n",
    "        j[2] = int(j[2].item())\n",
    "    for j in graph_pairs_val:\n",
    "        j[2] = int(j[2].item())\n",
    "\n",
    "    training_data_pairs = DataLoader(graph_pairs_train, batch_size=32, shuffle=True, collate_fn=collate,\n",
    "                               drop_last=True)\n",
    "    validation_data_pairs = DataLoader(graph_pairs_val, batch_size=32, shuffle=True, collate_fn=collate,\n",
    "                               drop_last=True)\n",
    "\n",
    "    input_dim = training_data_pairs.dataset[0][0].x.shape[1]\n",
    "\n",
    "    # Define hyperparameter grids\n",
    "    learning_rates = [0.01]\n",
    "    dropout_rates = [0.05]\n",
    "    sort_k_values = [30]\n",
    "    hidden_units_values = [16]\n",
    "\n",
    "    # Create combinations of hyperparameters\n",
    "    hyperparameter_combinations = list(itertools.product(learning_rates, dropout_rates, sort_k_values, hidden_units_values))\n",
    "\n",
    "    for lr, dropout_rate, sort_k, hidden_units in hyperparameter_combinations:\n",
    "        embedding = GCN(input_dim=input_dim, hidden_dim=hidden_units, layers=3, dropout=dropout_rate)\n",
    "        model = GraphSiamese(embedding, sort_k, nlinear = 2, nhidden=hidden_units, dropout = dropout_rate)\n",
    "        val_accuracy, val_f1 = run_model(model, training_data_pairs, validation_data_pairs)\n",
    "\n",
    "    model_name = f\"models/sgnn-topk{sort_k}-64hidden-{s}clique.pt\"\n",
    "    torch.save(model.state_dict(), model_name)\n",
    "\n",
    "    time_test = [t-2000 for t in cp_times[s] if t>=2000]\n",
    "\n",
    "    with open(f'results/test_synthetic/{s}-data.p', 'wb') as f:\n",
    "        pickle.dump(test, f)\n",
    "\n",
    "    with open(f'results/test_synthetic/{s}-labels.p', 'wb') as f:\n",
    "        pickle.dump(test_labels, f)\n",
    "\n",
    "    with open(f'results/test_synthetic/{s}-time.json', 'w') as f:\n",
    "        json.dump(time_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
