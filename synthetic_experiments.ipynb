{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Experiments using Stochastic Block Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from torch_geometric.data import DataLoader\n",
    "from itertools import combinations\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from src.utils.CreateFeatures import CreateFeatures\n",
    "from src.pygcn.GCN_synthetic import SiameseGNN\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "\n",
    "from src.utils.graphs import laplacian_embeddings, random_walk_embeddings, degree_matrix, identity\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_pairs(data, cp_time):\n",
    "    all_pairs = list(combinations(range(200), 2))\n",
    "    random_pairs = random.sample(all_pairs, 1000)\n",
    "\n",
    "    graph_pairs = []\n",
    "    for i in random_pairs:\n",
    "        first, second = i[0], i[1]\n",
    "\n",
    "        if first < cp_time and second < cp_time:\n",
    "            y_label = 1\n",
    "        elif first >= cp_time and second >= cp_time:\n",
    "            y_label = 1\n",
    "        else:\n",
    "            y_label = 0\n",
    "\n",
    "        graph_pairs.append((data[first], data[second], y_label))\n",
    "\n",
    "    flattened_train, flattened_test = train_test_split(graph_pairs, test_size=0.40, random_state=42)\n",
    "    flattened_test, flattened_val = train_test_split(graph_pairs, test_size=0.5, random_state=42)\n",
    "\n",
    "    return flattened_train, flattened_test, flattened_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_data, val_data, lr, dropout_rate, sort_k, hidden_units):\n",
    "    torch.manual_seed(42)\n",
    "    model = SiameseGNN(hidden_units, sort_k, dropout_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in tqdm(range(10)):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for data1, data2, label in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data1, data2)\n",
    "            label = torch.tensor(label).view(1).float()\n",
    "            loss = criterion(out.squeeze(0), label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        scheduler.step()  # Add this line to update the learning rate\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "\n",
    "            val_pred = []\n",
    "            val_truth = []\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data1, data2, label in val_data:\n",
    "                out = model(data1, data2)\n",
    "                label = torch.tensor(label).view(1).float()\n",
    "                val_loss = criterion(out.squeeze(0), label)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                predictions = torch.round(out.squeeze())\n",
    "\n",
    "                val_pred.append(predictions)\n",
    "                val_truth.append(label)\n",
    "\n",
    "                correct += (predictions == label).sum().item()\n",
    "                total += 1\n",
    "\n",
    "            val_loss = sum(val_losses) / len(val_losses)\n",
    "            val_accuracy = correct / total\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}, Validation F1 Score: {f1_score(val_truth, val_pred)}')\n",
    "    return val_accuracy, f1_score(val_truth, val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Assuming root_dir is the path to your root directory\n",
    "root_dir = 'results/synthetic/merge'\n",
    "\n",
    "merge_data = {}\n",
    "cp_times = {}\n",
    "\n",
    "# Walk through all directories and files in root_dir\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # If there's a data.p file in this directory, read it\n",
    "    args_file = os.path.join(dirpath, 'args.json')\n",
    "    if os.path.isfile(args_file):\n",
    "        with open(args_file, 'rb') as f:\n",
    "            arg_data = json.load(f)\n",
    "            p_value = arg_data['p']\n",
    "\n",
    "    data_file = os.path.join(dirpath, 'data.p')\n",
    "    if os.path.isfile(data_file):\n",
    "        with open(data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            merge_data[p_value] = data\n",
    "\n",
    "    # If there's a time.json file in this directory, read it\n",
    "    time_file = os.path.join(dirpath, 'time.json')\n",
    "    if os.path.isfile(time_file):\n",
    "        with open(time_file, 'r') as f:\n",
    "            time_data = json.load(f)\n",
    "            cp_times[p_value] = time_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with lr=0.001, dropout_rate=0.1, sort_k=40, hidden_units=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:27<04:08, 27.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.6963474320868651, Validation Loss: 0.6749369359016418, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:54<03:35, 26.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6881913221875826, Validation Loss: 0.6731768381595612, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:19<03:03, 26.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6878298337260882, Validation Loss: 0.6728987216949462, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:45<02:36, 26.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6858611965676149, Validation Loss: 0.6678843367099762, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:10<02:09, 25.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6866415205597878, Validation Loss: 0.6671185410022735, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:36<01:42, 25.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.682774517138799, Validation Loss: 0.6678065538406373, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:01<01:16, 25.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.6831856133540471, Validation Loss: 0.6664223670959473, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:27<00:51, 25.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.6828666445116202, Validation Loss: 0.6666052949428558, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:52<00:25, 25.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.6841998573640983, Validation Loss: 0.6663987874984741, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:18<00:00, 25.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.682914386789004, Validation Loss: 0.6665736401081085, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n",
      "Running with lr=0.01, dropout_rate=0.1, sort_k=40, hidden_units=64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:25<03:51, 25.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7029932231083512, Validation Loss: 0.6732045084238052, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:52<03:33, 26.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6873510731756687, Validation Loss: 0.6672911652326584, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:18<03:04, 26.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6813986180226008, Validation Loss: 0.6654541726112366, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:44<02:36, 26.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6813455215593179, Validation Loss: 0.6653825563192367, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:09<02:08, 25.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6813181965549787, Validation Loss: 0.6653651654720306, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [02:35<01:42, 25.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.6813281909128031, Validation Loss: 0.6653626537322999, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:00<01:16, 25.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.6817549201349418, Validation Loss: 0.6653983318805694, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [03:26<00:51, 25.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.6814154241482416, Validation Loss: 0.6653938090801239, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [03:51<00:25, 25.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.6816403838992119, Validation Loss: 0.6665417976379394, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:17<00:00, 25.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.681710265527169, Validation Loss: 0.6653460693359375, Validation Accuracy: 0.62, Validation F1 Score: 0.7654320987654321\n",
      "Best Hyperparameters: Learning Rate: 0.001, Dropout Rate: 0.1, Sort-k: 40, Hidden Units: 64\n",
      "Best Validation F1 Score: 0.7654320987654321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(merge_data[0.3]):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = np.diag(degree_matrix(adjacency).todense(), k=0).reshape(-1,1)\n",
    "    merge_data[0.3][j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(merge_data[0.3], cp_times[0.3])\n",
    "\n",
    "positive_samples = [item for item in flattened_train if item[2] == 1]\n",
    "negative_samples = [item for item in flattened_train if item[2] == 0]\n",
    "\n",
    "# Calculate the difference in count\n",
    "diff = len(negative_samples) - len(positive_samples)\n",
    "\n",
    "# Upsample positive samples\n",
    "if diff > 0:\n",
    "    positive_samples_upsampled = positive_samples * (diff // len(positive_samples)) + random.sample(positive_samples, diff % len(positive_samples))\n",
    "    balanced_data = negative_samples + positive_samples + positive_samples_upsampled\n",
    "else:\n",
    "    balanced_data = flattened_train\n",
    "\n",
    "random.shuffle(balanced_data)\n",
    "\n",
    "# Define hyperparameter grids\n",
    "learning_rates = [0.001, 0.01]\n",
    "dropout_rates = [0.01, 0.05, 0.1]\n",
    "sort_k_values = [20, 40, 100]\n",
    "hidden_units_values = [16, 32, 64]\n",
    "\n",
    "# Best params: lr=0.001, dropout_rate=0.1, sort_k=40, hidden_units=64\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, dropout_rates, sort_k_values, hidden_units_values))\n",
    "\n",
    "best_hyperparams = None\n",
    "best_val_score = 0\n",
    "\n",
    "for lr, dropout_rate, sort_k, hidden_units in hyperparameter_combinations:\n",
    "    print(f\"Running with lr={lr}, dropout_rate={dropout_rate}, sort_k={sort_k}, hidden_units={hidden_units}\")\n",
    "    val_accuracy, val_f1 = run_model(balanced_data, flattened_val, lr, dropout_rate, sort_k, hidden_units)\n",
    "    \n",
    "    # Update the best hyperparameters based on validation F1 score\n",
    "    if val_f1 > best_val_score:\n",
    "        best_val_score = val_f1\n",
    "        best_hyperparams = (lr, dropout_rate, sort_k, hidden_units)\n",
    "\n",
    "print(f\"Best Hyperparameters: Learning Rate: {best_hyperparams[0]}, Dropout Rate: {best_hyperparams[1]}, Sort-k: {best_hyperparams[2]}, Hidden Units: {best_hyperparams[3]}\")\n",
    "print(f\"Best Validation F1 Score: {best_val_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random-Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:28<01:54, 28.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7054354806492726, Validation Loss: 0.6984284114837647, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:57<01:25, 28.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6988908539215724, Validation Loss: 0.6956806327104569, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:25<00:57, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6965483835836251, Validation Loss: 0.6966160855293274, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:53<00:28, 28.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6961831471323967, Validation Loss: 0.6942015937566757, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:21<00:00, 28.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6951501431067785, Validation Loss: 0.6936068749427795, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(merge_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = random_walk_embeddings(adjacency, k=1)\n",
    "    merge_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(merge_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:29<01:56, 29.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7086347258090973, Validation Loss: 0.6929218556880951, Validation Accuracy: 0.51, Validation F1 Score: 0.6754966887417219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:58<01:27, 29.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6984774100780488, Validation Loss: 0.6929834187030792, Validation Accuracy: 0.51, Validation F1 Score: 0.6754966887417219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:27<00:58, 29.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6966335082550844, Validation Loss: 0.694138139128685, Validation Accuracy: 0.51, Validation F1 Score: 0.6754966887417219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:57<00:29, 29.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6972869330644608, Validation Loss: 0.6941545931100845, Validation Accuracy: 0.51, Validation F1 Score: 0.6754966887417219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:27<00:00, 29.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6956800158818562, Validation Loss: 0.6944538348913193, Validation Accuracy: 0.51, Validation F1 Score: 0.6754966887417219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(merge_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = laplacian_embeddings(adjacency, k=1)\n",
    "    merge_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(merge_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch_geometric/data/storage.py:450: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n",
      " 20%|██        | 1/5 [00:29<01:57, 29.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7082885177433491, Validation Loss: 0.6926153062582016, Validation Accuracy: 0.508, Validation F1 Score: 0.6737400530503979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:58<01:27, 29.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.7004396438598632, Validation Loss: 0.6938644955158234, Validation Accuracy: 0.492, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:28<00:58, 29.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6983883129556974, Validation Loss: 0.6947316192388534, Validation Accuracy: 0.492, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:58<00:29, 29.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6970296726624171, Validation Loss: 0.6948082302808761, Validation Accuracy: 0.492, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:26<00:00, 29.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6975281210740407, Validation Loss: 0.6932941085100174, Validation Accuracy: 0.492, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(merge_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = identity(400)\n",
    "    merge_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(merge_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clique Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/synthetic/06_04_11:13:29_clique_cp_1_T_200_n_400_p_0.2_q_0.05_20_0/data.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/synthetic/06_04_11:13:29_clique_cp_1_T_200_n_400_p_0.2_q_0.05_20_0/data.p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     clique_data \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m cp_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m133\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/synthetic/06_04_11:13:29_clique_cp_1_T_200_n_400_p_0.2_q_0.05_20_0/data.p'"
     ]
    }
   ],
   "source": [
    "with open(\"results/synthetic/06_04_11:13:29_clique_cp_1_T_200_n_400_p_0.2_q_0.05_20_0/data.p\", \"rb\") as f:\n",
    "    clique_data = pkl.load(f)\n",
    "\n",
    "cp_time = 133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch_geometric/data/storage.py:450: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n",
      " 20%|██        | 1/5 [00:15<01:02, 15.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.6992556133369605, Validation Loss: 0.6923871096372605, Validation Accuracy: 0.516, Validation F1 Score: 0.6807387862796834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:30<00:45, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6929134220878284, Validation Loss: 0.689760978102684, Validation Accuracy: 0.516, Validation F1 Score: 0.6807387862796834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:44<00:29, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6868213406205177, Validation Loss: 0.6814346568584442, Validation Accuracy: 0.516, Validation F1 Score: 0.6807387862796834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:59<00:14, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6622537118693193, Validation Loss: 0.6425437939763069, Validation Accuracy: 0.67, Validation F1 Score: 0.7433903576982893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:12<00:00, 14.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.5708100362246236, Validation Loss: 0.5006951180100441, Validation Accuracy: 0.79, Validation F1 Score: 0.8037383177570093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(clique_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = np.diag(degree_matrix(adjacency).todense(), k=0).reshape(-1,1)\n",
    "    clique_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(clique_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:14<00:58, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7019147059818109, Validation Loss: 0.6959783440828323, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:28<00:43, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6939904629190763, Validation Loss: 0.6951362727880478, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:43<00:28, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6924663008749485, Validation Loss: 0.6945376887321472, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:56<00:14, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6916149450341861, Validation Loss: 0.6942118445634842, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:11<00:00, 14.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6918824293216069, Validation Loss: 0.694335107088089, Validation Accuracy: 0.522, Validation F1 Score: 0.6859395532194481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(clique_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = random_walk_embeddings(adjacency, k=1)\n",
    "    clique_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(clique_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:13<00:55, 13.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7026044877370199, Validation Loss: 0.7127146036624908, Validation Accuracy: 0.532, Validation F1 Score: 0.6945169712793734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:28<00:42, 14.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.698015845467647, Validation Loss: 0.7064780175685883, Validation Accuracy: 0.532, Validation F1 Score: 0.6945169712793734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:42<00:28, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6962834357221921, Validation Loss: 0.7058776593208314, Validation Accuracy: 0.532, Validation F1 Score: 0.6945169712793734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:56<00:14, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6937986141443253, Validation Loss: 0.7009501012563706, Validation Accuracy: 0.532, Validation F1 Score: 0.6945169712793734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:10<00:00, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6910516557594141, Validation Loss: 0.7095978164672851, Validation Accuracy: 0.532, Validation F1 Score: 0.6945169712793734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(clique_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = laplacian_embeddings(adjacency, k=1)\n",
    "    clique_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(clique_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:14<00:58, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7043468555559714, Validation Loss: 0.6890924557447433, Validation Accuracy: 0.536, Validation F1 Score: 0.6979166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:28<00:42, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6917967011034488, Validation Loss: 0.6863912776708603, Validation Accuracy: 0.536, Validation F1 Score: 0.6979166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:42<00:28, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6876791834831237, Validation Loss: 0.6763330624103546, Validation Accuracy: 0.536, Validation F1 Score: 0.6979166666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:56<00:14, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6813007913529873, Validation Loss: 0.6608687014579773, Validation Accuracy: 0.73, Validation F1 Score: 0.7906976744186046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:10<00:00, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6493846730391184, Validation Loss: 0.5793172103762627, Validation Accuracy: 0.736, Validation F1 Score: 0.7480916030534351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(clique_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = identity(400)\n",
    "    clique_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(clique_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
