{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Experiments using Stochastic Block Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from torch_geometric.data import DataLoader\n",
    "from itertools import combinations\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from src.utils.CreateFeatures import CreateFeatures\n",
    "from src.pygcn.GCN_synthetic import SiameseGNN\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "\n",
    "from src.utils.graphs import laplacian_embeddings, random_walk_embeddings, degree_matrix\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_pairs(data, cp_time):\n",
    "    all_pairs = list(combinations(range(200), 2))\n",
    "    random_pairs = random.sample(all_pairs, 1000)\n",
    "\n",
    "    graph_pairs = []\n",
    "    for i in random_pairs:\n",
    "        first, second = i[0], i[1]\n",
    "\n",
    "        if first < cp_time and second < cp_time:\n",
    "            y_label = 1\n",
    "        elif first >= cp_time and second >= cp_time:\n",
    "            y_label = 1\n",
    "        else:\n",
    "            y_label = 0\n",
    "\n",
    "        graph_pairs.append((data[first], data[second], y_label))\n",
    "\n",
    "    flattened_train, flattened_test = train_test_split(graph_pairs, test_size=0.40, random_state=42)\n",
    "    flattened_test, flattened_val = train_test_split(graph_pairs, test_size=0.5, random_state=42)\n",
    "\n",
    "    return flattened_train, flattened_test, flattened_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train_data, val_data, type):\n",
    "    torch.manual_seed(42)\n",
    "    model = SiameseGNN()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in tqdm(range(5)):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for data1, data2, label in train_data:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data1, data2, type)\n",
    "            label = torch.tensor(label).view(1).float()\n",
    "            loss = criterion(out.squeeze(0), label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        scheduler.step()  # Add this line to update the learning rate\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "\n",
    "            val_pred = []\n",
    "            val_truth = []\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data1, data2, label in val_data:\n",
    "                out = model(data1, data2, type)\n",
    "                label = torch.tensor(label).view(1).float()\n",
    "                val_loss = criterion(out.squeeze(0), label)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                predictions = torch.round(out.squeeze())\n",
    "\n",
    "                val_pred.append(predictions)\n",
    "                val_truth.append(label)\n",
    "\n",
    "                correct += (predictions == label).sum().item()\n",
    "                total += 1\n",
    "\n",
    "            val_loss = sum(val_losses) / len(val_losses)\n",
    "            val_accuracy = correct / total\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}, Validation F1 Score: {f1_score(val_truth, val_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/synthetic/06_04_10:47:54_merge_T_200_n_400_k1_4_k2_2_p_0.5_q_0.2_0/data.p\", \"rb\") as f:\n",
    "    merge_data = pkl.load(f)\n",
    "    \n",
    "cp_time = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degree Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:30<02:00, 30.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.713399851967891, Validation Loss: 0.6954973387718201, Validation Accuracy: 0.478, Validation F1 Score: 0.6468200270635994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:00<01:31, 30.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6990391464034716, Validation Loss: 0.6961053496599198, Validation Accuracy: 0.478, Validation F1 Score: 0.6468200270635994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:31<01:00, 30.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6974425973494848, Validation Loss: 0.697055667757988, Validation Accuracy: 0.478, Validation F1 Score: 0.6468200270635994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:00<00:30, 30.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6973512395222982, Validation Loss: 0.6973099544048309, Validation Accuracy: 0.478, Validation F1 Score: 0.6468200270635994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [02:30<00:00, 30.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6966807777682941, Validation Loss: 0.6976537685394287, Validation Accuracy: 0.478, Validation F1 Score: 0.6468200270635994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(merge_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = np.diag(degree_matrix(adjacency).todense(), k=0).reshape(-1,1)\n",
    "    merge_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(merge_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random-Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/Users/sinclaireschuetze/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch_geometric/data/storage.py:450: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/5 [05:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflattened_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 33\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(train_data, val_data, type)\u001b[0m\n\u001b[1;32m     31\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data1, data2, label \u001b[38;5;129;01min\u001b[39;00m val_data:\n\u001b[0;32m---> 33\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(label)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     35\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), label)\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/src/pygcn/GCN_synthetic.py:55\u001b[0m, in \u001b[0;36mSiameseGNN.forward\u001b[0;34m(self, data1, data2, type)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data1, data2, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m     54\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn(data1, \u001b[38;5;28mtype\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m     out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcdist(out1, out2, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Euclidean distance\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_aggr(out, data1\u001b[38;5;241m.\u001b[39mbatch) \u001b[38;5;66;03m#Sort-k pooling layer\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/src/pygcn/GCN_synthetic.py:20\u001b[0m, in \u001b[0;36mGNN.forward\u001b[0;34m(self, data, type)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Use node degrees as features\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m---> 20\u001b[0m     networkx_graph \u001b[38;5;241m=\u001b[39m \u001b[43mto_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     adjacency \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mto_scipy_sparse_array(networkx_graph, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch_geometric/utils/convert.py:188\u001b[0m, in \u001b[0;36mto_networkx\u001b[0;34m(data, node_attrs, edge_attrs, graph_attrs, to_undirected, to_multi, remove_self_loops)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m edge_attrs \u001b[38;5;129;01mor\u001b[39;00m []:\n\u001b[1;32m    186\u001b[0m             edge_kwargs[key] \u001b[38;5;241m=\u001b[39m to_networkx_value(edge_store[key][i])\n\u001b[0;32m--> 188\u001b[0m         \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43medge_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[0;32m~/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/networkx/classes/digraph.py:648\u001b[0m, in \u001b[0;36mDiGraph.add_edge\u001b[0;34m(self, u_of_edge, v_of_edge, **attr)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# silent failure on remove\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     nx\u001b[38;5;241m.\u001b[39m_clear_cache(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_edge\u001b[39m(\u001b[38;5;28mself\u001b[39m, u_of_edge, v_of_edge, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattr):\n\u001b[1;32m    649\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add an edge between u and v.\u001b[39;00m\n\u001b[1;32m    650\u001b[0m \n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    The nodes u and v will be automatically added if they are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;124;03m    >>> G.edges[1, 2].update({0: 5})\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     u, v \u001b[38;5;241m=\u001b[39m u_of_edge, v_of_edge\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j, i in enumerate(merge_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = random_walk_embeddings(adjacency, k=1)\n",
    "    merge_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(merge_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, i in enumerate(merge_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = laplacian_embeddings(adjacency, k=1)\n",
    "    merge_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(merge_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, i in enumerate(merge_data):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.to_scipy_sparse_array(networkx_graph, format='csr')\n",
    "            \n",
    "    x = np.eye(adjacency.shape[0])\n",
    "    merge_data[j].x = x\n",
    "\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(merge_data, cp_time)\n",
    "run_model(flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clique Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/Documents/GitHub/Trade-GNN-ChangePoint/.env/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      " 20%|██        | 1/5 [00:14<00:58, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7027358784526586, Validation Loss: 0.6949385805130005, Validation Accuracy: 0.524, Validation F1 Score: 0.6876640419947506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:28<00:42, 14.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6930854049821694, Validation Loss: 0.6904357953071594, Validation Accuracy: 0.524, Validation F1 Score: 0.6876640419947506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:42<00:28, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6918158697585265, Validation Loss: 0.6914281842708587, Validation Accuracy: 0.524, Validation F1 Score: 0.6876640419947506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:56<00:14, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6835549014310042, Validation Loss: 0.6739447337388992, Validation Accuracy: 0.622, Validation F1 Score: 0.46458923512747874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:11<00:00, 14.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6490362334251404, Validation Loss: 0.6374697284698486, Validation Accuracy: 0.614, Validation F1 Score: 0.4469914040114613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"results/synthetic/06_04_11:13:29_clique_cp_1_T_200_n_400_p_0.2_q_0.05_20_0/data.p\", \"rb\") as f:\n",
    "    clique_data = pkl.load(f)\n",
    "\n",
    "cp_time = 133\n",
    "flattened_train, flattened_test, flattened_val = create_synthetic_pairs(clique_data, cp_time)\n",
    "model = SiameseGNN()\n",
    "run_model(model, flattened_train, flattened_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
