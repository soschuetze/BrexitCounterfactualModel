{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Experiments using Stochastic Block Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from torch_geometric.data import DataLoader\n",
    "from itertools import combinations\n",
    "import random\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from src.utils.CreateFeatures import CreateFeatures\n",
    "from src.pygcn.GCN_synthetic import SiameseGNN\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch_geometric.data as data\n",
    "from typing import Union\n",
    "from src.utils.graphs import laplacian_embeddings, random_walk_embeddings, degree_matrix, identity\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from src.utils.sample import sample_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def adjusted_f1_score(y_true, y_pred, beta=1.0):\n",
    "    \"\"\"\n",
    "    Calculate the adjusted F1 score.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (list or array): True labels.\n",
    "    y_pred (list or array): Predicted labels.\n",
    "    beta (float): Weight factor.\n",
    "    \n",
    "    Returns:\n",
    "    float: Adjusted F1 score.\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    adjusted_f1 = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "    return adjusted_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_data, val_data, lr):\n",
    "    torch.manual_seed(42)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    criterion = nn.BCELoss()  # Changed to BCEWithLogitsLoss for numerical stability\n",
    "\n",
    "    for epoch in tqdm(range(10)):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for data1, data2, label in train_data:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data1, data2)\n",
    "\n",
    "            label = torch.tensor(label).view(1).float()\n",
    "            loss = criterion(out.squeeze(0), label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_losses = []\n",
    "\n",
    "            val_pred = []\n",
    "            val_truth = []\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for data1, data2, label in val_data:\n",
    "                out = model(data1, data2)\n",
    "                label = torch.tensor(label).view(1).float()\n",
    "                val_loss = criterion(out.squeeze(0), label)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "                predictions = torch.round(out.squeeze())\n",
    "\n",
    "                val_pred.append(predictions)\n",
    "                val_truth.append(label)\n",
    "\n",
    "                correct += (predictions == label).sum().item()\n",
    "                total += 1\n",
    "\n",
    "            val_loss = sum(val_losses) / len(val_losses)\n",
    "            val_accuracy = correct / total\n",
    "\n",
    "        val_f1 = f1_score(val_truth, val_pred)\n",
    "        print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}, Validation F1 Score: {val_f1}')\n",
    "    return val_accuracy, val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clique Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Assuming root_dir is the path to your root directory\n",
    "root_dir = 'results/synthetic/'\n",
    "\n",
    "clique_data = {}\n",
    "cp_times = {}\n",
    "label_data = {}\n",
    "\n",
    "# Walk through all directories and files in root_dir\n",
    "for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "    # If there's a data.p file in this directory, read it\n",
    "    args_file = os.path.join(dirpath, 'args.json')\n",
    "    if os.path.isfile(args_file):\n",
    "        with open(args_file, 'rb') as f:\n",
    "            arg_data = json.load(f)\n",
    "            clique_size = arg_data['size_clique']\n",
    "\n",
    "    data_file = os.path.join(dirpath, 'data.p')\n",
    "    if os.path.isfile(data_file):\n",
    "        with open(data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            clique_data[clique_size] = data\n",
    "\n",
    "    # If there's a time.json file in this directory, read it\n",
    "    time_file = os.path.join(dirpath, 'time.json')\n",
    "    if os.path.isfile(time_file):\n",
    "        with open(time_file, 'r') as f:\n",
    "            time_data = json.load(f)\n",
    "            cp_times[clique_size] = time_data\n",
    "\n",
    "    label_file = os.path.join(dirpath, 'labels.p')\n",
    "    if os.path.isfile(label_file):\n",
    "        with open(label_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            label_data[clique_size] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [20, 30, 40, 50, 60, 70, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 positive and 1000 negative examples\n",
      "1000 positive and 1000 negative examples\n",
      "Running with lr=0.001, dropout_rate=0.1, sort_k=30, hidden_units=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:09<10:21, 69.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.6488162154513123, Validation Loss: 0.6768332703160156, Validation Accuracy: 0.6011168727562824, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:17<09:11, 68.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.6456865868027151, Validation Loss: 0.6743163139543354, Validation Accuracy: 0.6011168727562824, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:26<08:00, 68.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6453274700351926, Validation Loss: 0.674170755607748, Validation Accuracy: 0.6011168727562824, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [04:35<06:53, 68.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6440438593809421, Validation Loss: 0.6739354103649096, Validation Accuracy: 0.6011168727562824, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [05:41<05:40, 68.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6446300898688111, Validation Loss: 0.6738682423614628, Validation Accuracy: 0.6011168727562824, Validation F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [06:48<04:30, 67.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.6437767698214605, Validation Loss: 0.6823341381302572, Validation Accuracy: 0.580374950139609, Validation F1 Score: 0.05565529622980251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [07:56<03:23, 67.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.6401357015656971, Validation Loss: 0.7013331853539809, Validation Accuracy: 0.562026326286398, Validation F1 Score: 0.14084507042253522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [09:05<02:16, 68.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.6324059018184329, Validation Loss: 0.7634722070362067, Validation Accuracy: 0.5416832867969685, Validation F1 Score: 0.25918762088974856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [10:14<01:08, 68.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.6216219229873465, Validation Loss: 0.8910496097650954, Validation Accuracy: 0.5105704028719585, Validation F1 Score: 0.3806158505805149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:25<00:00, 68.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.6094959310710512, Validation Loss: 0.9289528059891652, Validation Accuracy: 0.5185480654168328, Validation F1 Score: 0.45947156291983876\n",
      "Best Hyperparameters: Learning Rate: 0.001, Dropout Rate: 0.1, Sort-k: 30, Hidden Units: 16\n",
      "Best Validation F1 Score: 0.45947156291983876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for s in [20]:\n",
    "    for j, i in enumerate(clique_data[s]):\n",
    "        edge_index = i.edge_index.to(torch.int64)\n",
    "        networkx_graph = to_networkx(i)\n",
    "        adjacency = nx.adjacency_matrix(networkx_graph)\n",
    "        \n",
    "        attributes = np.eye(adjacency.shape[0])\n",
    "        clique_data[s][j].x = attributes\n",
    "    \n",
    "    train = clique_data[s][:1000]\n",
    "    train_labels = label_data[s][:1000]\n",
    "\n",
    "    val = clique_data[s][1000:2000]\n",
    "    val_labels = label_data[s][1000:2000]\n",
    "\n",
    "    test = clique_data[s][2000:]\n",
    "    test_labels = label_data[s][2000:]\n",
    "\n",
    "    graph_pairs_train = sample_pairs(train,train_labels,nsamples=2000)\n",
    "    graph_pairs_val = sample_pairs(train,val_labels,nsamples=2000)\n",
    "\n",
    "    for j in graph_pairs_train:\n",
    "        j[2] = int(j[2].item())\n",
    "    for j in graph_pairs_val:\n",
    "        j[2] = int(j[2].item())\n",
    "\n",
    "    # Define hyperparameter grids\n",
    "    learning_rates = [0.001]\n",
    "    dropout_rates = [0.1]\n",
    "    sort_k_values = [30]\n",
    "    hidden_units_values = [16]\n",
    "\n",
    "    # Best params: lr=0.001, dropout_rate=0.1, sort_k=40, hidden_units=64\n",
    "\n",
    "    # Create combinations of hyperparameters\n",
    "    hyperparameter_combinations = list(itertools.product(learning_rates, dropout_rates, sort_k_values, hidden_units_values))\n",
    "\n",
    "    best_hyperparams = None\n",
    "    best_val_score = 0\n",
    "\n",
    "    for lr, dropout_rate, sort_k, hidden_units in hyperparameter_combinations:\n",
    "        print(f\"Running with lr={lr}, dropout_rate={dropout_rate}, sort_k={sort_k}, hidden_units={hidden_units}\")\n",
    "        model = SiameseGNN(hidden_units, sort_k, dropout_rate)\n",
    "        val_accuracy, val_f1 = run_model(model, graph_pairs_train, graph_pairs_val, lr)\n",
    "        \n",
    "        # Update the best hyperparameters based on validation F1 score\n",
    "        if val_f1 > best_val_score:\n",
    "            best_val_score = val_f1\n",
    "            best_hyperparams = (lr, dropout_rate, sort_k, hidden_units)\n",
    "\n",
    "    print(f\"Best Hyperparameters: Learning Rate: {best_hyperparams[0]}, Dropout Rate: {best_hyperparams[1]}, Sort-k: {best_hyperparams[2]}, Hidden Units: {best_hyperparams[3]}\")\n",
    "    print(f\"Best Validation F1 Score: {best_val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/sgnn-topk30-16hidden-20clique.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_labels_to_changepoint_labels_adjusted(labels: Union[np.ndarray, list], tolerance=2):\n",
    "    \"\"\"\n",
    "    Convert graph distribution labels (phase) to change-point labels (0 or 1) using adjustment mechanism with level of tolerance\n",
    "\n",
    "    :param labels:\n",
    "    :param tolerance (int): flag as change points the timestamps at +/- tolerance around a change-point\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(labels, list):\n",
    "        labels = np.array(labels)\n",
    "\n",
    "    cps = np.concatenate([np.zeros(1).astype(int), (abs(labels[1:] - labels[:-1]) > 0).astype(int)],axis=0)\n",
    "\n",
    "    for i in range(1,tolerance+1):\n",
    "        cps = (cps + np.concatenate([np.zeros(i), cps[:-i]], axis=0) + np.concatenate([cps[i:], np.zeros(i)], axis=0) > 0)\n",
    "\n",
    "    return cps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, i in enumerate(clique_data[20]):\n",
    "    edge_index = i.edge_index.to(torch.int64)\n",
    "    networkx_graph = to_networkx(i)\n",
    "    adjacency = nx.adjacency_matrix(networkx_graph)\n",
    "    \n",
    "    attributes = np.eye(adjacency.shape[0])\n",
    "    clique_data[20][j].x = attributes\n",
    "    \n",
    "data = clique_data[s]\n",
    "labels = label_data[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/test_synthetic/data.p', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "with open('results/test_synthetic/labels.p', 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
