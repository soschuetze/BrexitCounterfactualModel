{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import datetime as datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as sm\n",
    "import dgl.function as fn\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from src.utils.TradeNetwork import TradeNetwork\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from src.pygcn import SiameseGNN\n",
    "\n",
    "#imports for graph creation\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#imports for graph learning\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.data as data\n",
    "import torch_geometric.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1962,2019)\n",
    "\n",
    "train_years = [2005, 1969, 2002, 1997, 1993, 1982, 2001, 2000, 1962, 1985, 1978, 2016, 1986, 1987, 1989, 1971, 2013, 1996, 1995, 1967, 2017, 1974, 1990, 1977, 1980, 2014, 1965, 1984, 2006, 1973, 1968, 1981, 1970, 1991]\n",
    "val_years = [1975, 1983, 2009, 1966, 1999, 1988, 2007, 1979, 1972, 2015, 2003]\n",
    "test_years = [1963, 1964, 1976, 1992, 1994, 1998, 2004, 2008, 2010, 2011, 2012, 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12 features\n",
    "import pickle as pkl\n",
    "with open(\"src/pygcn/train_graphs.pickle\", \"rb\") as f:\n",
    "    train_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"src/pygcn/val_graphs.pickle\", \"rb\") as f:  \n",
    "    val_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"src/pygcn/test_graphs.pickle\", \"rb\") as f:         \n",
    "    test_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test_graphs, batch_size=4)\n",
    "train_loader = DataLoader(train_graphs, batch_size=4)\n",
    "val_loader = DataLoader(val_graphs, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sGNN with GCN Encoder and 3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crisis_years(year_pairs, crisis_years):\n",
    "    result = []\n",
    "    for pair in year_pairs:\n",
    "        start, end = pair\n",
    "        # Check if any crisis year is between the pair or equals the later year\n",
    "        if any(start < year <= end for year in crisis_years):\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1983, 1982, 2008, 2002, 2016, 1967, 1962, 1989, 2012, 1963, 1993, 1986, 1996,1978]\n",
    "\n",
    "def get_year_pairs(year_range):\n",
    "    return [(year1, year2) for year1 in year_range for year2 in year_range if year2 >= year1]\n",
    "\n",
    "def get_loader_pairs(dataset):\n",
    "    return [(dataset[i], dataset[j]) for i in range(len(dataset)) for j in range(len(dataset)) if j >= i]\n",
    "\n",
    "def get_graph_pairs(graphs):\n",
    "    return [(graphs[i], graphs[j]) for i in range(len(graphs)) for j in range(len(graphs)) if j >= i]\n",
    "\n",
    "train_pairs = get_year_pairs(train_years)\n",
    "val_pairs = get_year_pairs(val_years)\n",
    "\n",
    "train_y = check_crisis_years(train_pairs, crisis_years)\n",
    "val_y = check_crisis_years(val_pairs, crisis_years)\n",
    "\n",
    "train_loader_pairs = get_loader_pairs(train_loader.dataset)\n",
    "val_loader_pairs = get_loader_pairs(val_loader.dataset)\n",
    "\n",
    "train_graph_pairs = get_graph_pairs(train_graphs)\n",
    "val_graph_pairs = get_graph_pairs(val_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_y = torch.tensor(np.array(train_y))\n",
    "val_torch_y = torch.tensor(np.array(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_pairs_train = list(zip(train_loader_pairs, train_y))\n",
    "labeled_pairs_val = list(zip(val_loader_pairs, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_train = [(a, b, c) for ((a, b), c) in labeled_pairs_train]\n",
    "flattened_val  = [(a, b, c) for ((a, b), c) in labeled_pairs_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "positive_samples = [item for item in flattened_train if item[2] == 1]\n",
    "negative_samples = [item for item in flattened_train if item[2] == 0]\n",
    "\n",
    "# Calculate the difference in count\n",
    "diff = len(negative_samples) - len(positive_samples)\n",
    "\n",
    "# Upsample positive samples\n",
    "if diff > 0:\n",
    "    positive_samples_upsampled = positive_samples * (diff // len(positive_samples)) + random.sample(positive_samples, diff % len(positive_samples))\n",
    "    balanced_data = negative_samples + positive_samples + positive_samples_upsampled\n",
    "else:\n",
    "    balanced_data = data\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "random.shuffle(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:22<07:09, 22.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.7069368174782505, Validation Loss: 0.6644339471152334, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:44<06:35, 21.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.7007183937708072, Validation Loss: 0.6638694134625521, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [01:04<06:03, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.6884700077028535, Validation Loss: 0.7348903051831506, Validation Accuracy: 0.5151515151515151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [01:25<05:37, 21.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.6593805215163537, Validation Loss: 0.6667289880640579, Validation Accuracy: 0.5606060606060606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:46<05:13, 20.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6341424704934189, Validation Loss: 0.6880699423226443, Validation Accuracy: 0.5606060606060606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [02:06<04:51, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.6164361875373543, Validation Loss: 0.6692583533850583, Validation Accuracy: 0.5909090909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [02:27<04:32, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.5951411732568698, Validation Loss: 0.6672850864741838, Validation Accuracy: 0.5606060606060606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [02:50<04:18, 21.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.5732985908977077, Validation Loss: 0.649898157483249, Validation Accuracy: 0.5757575757575758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [03:11<03:55, 21.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.5661149462049584, Validation Loss: 0.6404500199820508, Validation Accuracy: 0.5606060606060606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [03:32<03:31, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.5478807543467941, Validation Loss: 0.7244498655905552, Validation Accuracy: 0.5454545454545454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [03:53<03:09, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training Loss: 0.5182998887114372, Validation Loss: 0.7226670713333244, Validation Accuracy: 0.5606060606060606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [04:15<02:51, 21.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training Loss: 0.49552465096746023, Validation Loss: 0.7331494244069539, Validation Accuracy: 0.5757575757575758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [04:38<02:32, 21.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training Loss: 0.4816634640230146, Validation Loss: 0.7546161954770937, Validation Accuracy: 0.5757575757575758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [05:00<02:12, 22.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Training Loss: 0.46910015609819755, Validation Loss: 0.7864336175632409, Validation Accuracy: 0.5757575757575758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [05:23<01:51, 22.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Training Loss: 0.45681527108121955, Validation Loss: 0.8081607802270093, Validation Accuracy: 0.6060606060606061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [05:44<01:27, 21.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Training Loss: 0.4452178052955145, Validation Loss: 0.8395179241585234, Validation Accuracy: 0.6060606060606061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [06:07<01:06, 22.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Training Loss: 0.43254965263964396, Validation Loss: 0.8726071839884036, Validation Accuracy: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [06:28<00:43, 21.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Training Loss: 0.41787379529199165, Validation Loss: 0.8951684424678343, Validation Accuracy: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [06:50<00:21, 21.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training Loss: 0.40456365760822904, Validation Loss: 0.91365475038765, Validation Accuracy: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [07:12<00:00, 21.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Training Loss: 0.39105719769102126, Validation Loss: 0.9060935187118241, Validation Accuracy: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "model = SiameseGNN(num_features=balanced_data[0][0].num_node_features)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for data1, data2, label in balanced_data:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data1, data2)\n",
    "        label = torch.tensor(label).view(1).float()\n",
    "        loss = criterion(out.squeeze(0), label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()  # Add this line to update the learning rate\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data1, data2, label in flattened_val:\n",
    "            out = model(data1, data2)\n",
    "            label = torch.tensor(label).view(1).float()\n",
    "            val_loss = criterion(out.squeeze(0), label)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            predictions = torch.round(out.squeeze())\n",
    "            correct += (predictions == label).sum().item()\n",
    "            total += 1\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sGNN with Feature Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_dicts/filtered_features_dict.pkl\", \"rb\") as f:\n",
    "    feat_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1962, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test_graphs, batch_size=32)\n",
    "train_loader = DataLoader(train_graphs, batch_size=32)\n",
    "val_loader = DataLoader(val_graphs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(years, graphs):\n",
    "\n",
    "    zeros = torch.zeros(59)\n",
    "\n",
    "    for i in range(len(years)):\n",
    "        new_x = torch.empty(0, 59)\n",
    "        year = years[i]\n",
    "\n",
    "        feat_dict_year = feat_dict[year].combined_features\n",
    "\n",
    "        for j, country in enumerate(all_nodes):\n",
    "            if j == 0:\n",
    "                new_x = torch.stack([zeros])\n",
    "\n",
    "            elif country in feat_dict_year[\"country_code\"].values:\n",
    "                tensor_before = graphs[i].x[j]\n",
    "                country_row = feat_dict_year[feat_dict_year[\"country_code\"] == country]\n",
    "                country_row = country_row.drop(columns = [\"prev_gdp_growth\", \"country_code\", \"current_gdp_growth\"])\n",
    "                row_values = country_row.values.tolist()\n",
    "                row_tensor = torch.tensor(row_values)[0]\n",
    "                combined_values = torch.cat((tensor_before, row_tensor))\n",
    "\n",
    "                new_x = torch.cat((new_x, combined_values.unsqueeze(0)), dim=0)\n",
    "\n",
    "            else:\n",
    "                new_x = torch.cat((new_x, zeros.unsqueeze(0)), dim=0)\n",
    "\n",
    "        graphs[i].x = new_x\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_years' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_graphs \u001b[38;5;241m=\u001b[39m add_features(train_years, train_graphs)\n\u001b[1;32m      2\u001b[0m val_graphs \u001b[38;5;241m=\u001b[39m add_features(val_years, val_graphs)\n\u001b[1;32m      3\u001b[0m test_graphs \u001b[38;5;241m=\u001b[39m add_features(test_years, test_graphs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_years' is not defined"
     ]
    }
   ],
   "source": [
    "train_graphs = add_features(train_years, train_graphs)\n",
    "val_graphs = add_features(val_years, val_graphs)\n",
    "test_graphs = add_features(test_years, test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1983, 1982, 2008, 2002, 2016, 1967, 1962, 1989, 2012, 1963, 1993, 1986, 1996,1978]\n",
    "\n",
    "def get_year_pairs(year_range):\n",
    "    return [(year1, year2) for year1 in year_range for year2 in year_range if year2 >= year1]\n",
    "\n",
    "def get_loader_pairs(dataset):\n",
    "    return [(dataset[i], dataset[j]) for i in range(len(dataset)) for j in range(len(dataset)) if j >= i]\n",
    "\n",
    "train_pairs = get_year_pairs(train_years)\n",
    "val_pairs = get_year_pairs(val_years)\n",
    "\n",
    "train_y = check_crisis_years(train_pairs, crisis_years)\n",
    "val_y = check_crisis_years(val_pairs, crisis_years)\n",
    "\n",
    "train_loader_pairs = get_loader_pairs(train_loader.dataset)\n",
    "val_loader_pairs = get_loader_pairs(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_y = torch.tensor(np.array(train_y))\n",
    "val_torch_y = torch.tensor(np.array(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:13<01:57, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.4229666137895664, Validation Loss: 0.4994966710607211, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:25<01:42, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.41427947403628285, Validation Loss: 0.5009946116443836, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:39<01:33, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.4123781653074156, Validation Loss: 0.5026700783408049, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:34<02:57, 29.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.4100686021462208, Validation Loss: 0.5035527772975691, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:29<05:02, 60.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.4088507873921835, Validation Loss: 0.5042239278554916, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:43<02:58, 44.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.41012620629758395, Validation Loss: 0.5044262244394331, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:55<01:42, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.40759952013232126, Validation Loss: 0.5053463794968345, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:07<00:54, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.4073875575068117, Validation Loss: 0.5056198169336175, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [04:19<00:22, 22.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.4068174197393305, Validation Loss: 0.5069572094715002, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:31<00:00, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.4066063172676984, Validation Loss: 0.508396189095396, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = train_loader_pairs\n",
    "val_loader = val_loader_pairs\n",
    "\n",
    "model = SiameseGNN(num_features=train_loader[0][0].num_node_features)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i in range(len(train_loader)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_loader[i][0], train_loader[i][1])\n",
    "        loss = criterion(out.squeeze(), train_torch_y[i].to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()  # Add this line to update the learning rate\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(val_loader)):\n",
    "            out = model(val_loader[i][0], val_loader[i][1])\n",
    "            val_loss = criterion(out.squeeze(), val_torch_y[i].to(torch.float))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            predictions = torch.round(out.squeeze())\n",
    "            correct += (predictions == val_torch_y[i]).sum().item()\n",
    "            total += 1\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Feature Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/57 [00:05<04:47,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1963\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2/57 [00:10<04:47,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1964\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/57 [00:15<04:50,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1965\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/57 [00:21<04:52,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1966\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/57 [00:27<05:00,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1967\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6/57 [00:34<05:01,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1968\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/57 [00:40<05:03,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1969\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8/57 [00:47<05:09,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1970\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 9/57 [00:54<05:14,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1971\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 10/57 [01:01<05:19,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1972\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 11/57 [01:09<05:32,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1973\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 12/57 [01:18<05:42,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1974\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 13/57 [01:27<05:50,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1975\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 14/57 [01:36<05:56,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 15/57 [01:40<04:59,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1977\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 16/57 [01:45<04:23,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1978\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 17/57 [01:51<04:16,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1979\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 18/57 [01:58<04:09,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1980\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 19/57 [02:04<04:06,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1981\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 20/57 [02:11<04:04,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1982\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 21/57 [02:18<04:03,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1983\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 22/57 [02:25<03:59,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1984\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 23/57 [02:33<03:58,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1985\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 24/57 [02:41<04:00,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1986\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 25/57 [02:48<03:51,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1987\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 26/57 [02:56<03:52,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1988\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 27/57 [03:05<03:57,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1989\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 28/57 [03:14<03:57,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1990\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 29/57 [03:23<03:57,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1991\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 30/57 [03:32<03:53,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1992\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 31/57 [03:43<04:06,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1993\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 32/57 [03:54<04:06,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1994\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 33/57 [04:06<04:10, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1995\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 34/57 [04:17<04:07, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1996\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 35/57 [04:30<04:06, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1997\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 36/57 [04:42<04:02, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1998\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 37/57 [04:55<04:00, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1999\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 38/57 [05:09<03:59, 12.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 39/57 [05:24<03:57, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2001\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 40/57 [05:39<03:55, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2002\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 41/57 [05:53<03:44, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2003\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 42/57 [06:09<03:37, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2004\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 43/57 [06:26<03:31, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2005\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 44/57 [06:46<03:35, 16.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2006\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 45/57 [07:04<03:24, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2007\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 46/57 [07:22<03:13, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2008\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 47/57 [07:42<03:00, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2009\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 48/57 [08:00<02:43, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2010\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 49/57 [08:18<02:25, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2011\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 50/57 [08:36<02:06, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2012\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 51/57 [08:55<01:49, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2013\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 52/57 [09:14<01:32, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2014\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 53/57 [09:33<01:14, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2015\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 54/57 [09:52<00:56, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2016\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 55/57 [10:12<00:38, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2017\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 56/57 [10:31<00:19, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2018\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [10:51<00:00, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "years = range(1962,2019)\n",
    "\n",
    "train_years = [2005, 1969, 2002, 1997, 1993, 1982, 2001, 2000, 1962, 1985, 1978, 2016, 1986, 1987, 1989, 1971, 2013, 1996, 1995, 1967, 2017, 1974, 1990, 1977, 1980, 2014, 1965, 1984, 2006, 1973, 1968, 1981, 1970, 1991]\n",
    "val_years = [1975, 1983, 2009, 1966, 1999, 1988, 2007, 1979, 1972, 2015, 2003]\n",
    "test_years = [1963, 1964, 1976, 1992, 1994, 1998, 2004, 2008, 2010, 2011, 2012, 2018]\n",
    "\n",
    "train_graphs = []\n",
    "val_graphs = []\n",
    "test_graphs = []\n",
    "i = 0\n",
    "\n",
    "for year in tqdm(years):\n",
    "    print(str(year), end='\\r')\n",
    "    \n",
    "    trade = TradeNetwork(year = year)\n",
    "    trade.prepare_features()\n",
    "    trade.prepare_network()\n",
    "    trade.graph_create(node_features = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth',\n",
    "    'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "       'resource_8', 'resource_9'],\n",
    "        node_labels = 'future_gdp_per_cap_growth')\n",
    "    \n",
    "    if(year in val_years):\n",
    "        val_graphs.append(trade.pyg_graph)\n",
    "    elif(year in test_years):\n",
    "        test_graphs.append(trade.pyg_graph)\n",
    "    else: \n",
    "        train_graphs.append(trade.pyg_graph)\n",
    "        \n",
    "    trade.features[\"year\"] = year\n",
    "    \n",
    "    if(i == 0):\n",
    "        trade_df = trade.features\n",
    "    else: \n",
    "        trade_df = pd.concat([trade_df, trade.features])\n",
    "        \n",
    "    i = i+1\n",
    "    print(trade.node_attributes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pygcn/train_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(train_graphs, f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(val_graphs, f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(test_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_dicts/random_features_dict.pkl\", \"rb\") as f:\n",
    "    feat_dict_random = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(years, graphs, feat_dict):\n",
    "\n",
    "    zeros = torch.zeros(434)\n",
    "\n",
    "    for i in range(len(years)):\n",
    "        new_x = torch.empty(0, 434)\n",
    "        year = years[i]\n",
    "\n",
    "        feat_dict_year = feat_dict[year].combined_features\n",
    "\n",
    "        for j, country in enumerate(all_nodes):\n",
    "            if j == 0:\n",
    "                new_x = torch.stack([zeros])\n",
    "\n",
    "            elif country in feat_dict_year[\"country_code\"].values:\n",
    "                tensor_before = graphs[i].x[j]\n",
    "                country_row = feat_dict_year[feat_dict_year[\"country_code\"] == country]\n",
    "                country_row = country_row.drop(columns = [\"prev_gdp_growth\", \"country_code\", \"current_gdp_growth\"])\n",
    "                row_values = country_row.values.tolist()\n",
    "                row_tensor = torch.tensor(row_values)[0]\n",
    "                combined_values = torch.cat((tensor_before, row_tensor))\n",
    "\n",
    "                new_x = torch.cat((new_x, combined_values.unsqueeze(0)), dim=0)\n",
    "\n",
    "            else:\n",
    "                new_x = torch.cat((new_x, zeros.unsqueeze(0)), dim=0)\n",
    "\n",
    "        graphs[i].x = new_x\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = add_features(train_years, train_graphs, feat_dict_random)\n",
    "val_graphs = add_features(val_years, val_graphs, feat_dict_random)\n",
    "test_graphs = add_features(test_years, test_graphs, feat_dict_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test_graphs, batch_size=32)\n",
    "train_loader = DataLoader(train_graphs, batch_size=32)\n",
    "val_loader = DataLoader(val_graphs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crisis_years(year_pairs, crisis_years):\n",
    "    result = []\n",
    "    for pair in year_pairs:\n",
    "        start, end = pair\n",
    "        # Check if any crisis year is between the pair or equals the later year\n",
    "        if any(start < year <= end for year in crisis_years):\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1983, 1982, 2008, 2002, 2016, 1967, 1962, 1989, 2012, 1963, 1993, 1986, 1996,1978]\n",
    "\n",
    "def get_year_pairs(year_range):\n",
    "    return [(year1, year2) for year1 in year_range for year2 in year_range if year2 >= year1]\n",
    "\n",
    "def get_loader_pairs(dataset):\n",
    "    return [(dataset[i], dataset[j]) for i in range(len(dataset)) for j in range(len(dataset)) if j >= i]\n",
    "\n",
    "train_pairs = get_year_pairs(train_years)\n",
    "val_pairs = get_year_pairs(val_years)\n",
    "\n",
    "train_y = check_crisis_years(train_pairs, crisis_years)\n",
    "val_y = check_crisis_years(val_pairs, crisis_years)\n",
    "\n",
    "train_loader_pairs = get_loader_pairs(train_loader.dataset)\n",
    "val_loader_pairs = get_loader_pairs(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_y = torch.tensor(np.array(train_y))\n",
    "val_torch_y = torch.tensor(np.array(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_neg = torch.eq(train_torch_y, 0).sum().item()\n",
    "N_pos = torch.eq(train_torch_y, 1).sum().item()\n",
    "\n",
    "# Calculate the weight for the positive class\n",
    "weight_for_positive_class = N_neg / N_pos\n",
    "pos_weight = torch.tensor([weight_for_positive_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
