{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import datetime as datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as sm\n",
    "import dgl.function as fn\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "#imports for graph creation\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#imports for graph learning\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.data as data\n",
    "import torch_geometric.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_sitc_codes():\n",
    "    # URL of the JSON file\n",
    "    url = 'https://comtradeapi.un.org/files/v1/app/reference/S4.json'\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL and fetch the data\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check that the request was successful\n",
    "        \n",
    "        # Load the JSON data\n",
    "        data = response.json()\n",
    "\n",
    "        # Since the JSON data might be nested, use json_normalize with appropriate arguments\n",
    "        if isinstance(data, list):\n",
    "            # If the top level is a list\n",
    "            df = pd.json_normalize(data)\n",
    "        else:\n",
    "            # If the top level is a dictionary\n",
    "            # Identify the key that holds the main data (adjust the path as necessary)\n",
    "            main_data_key = 'results'  # Adjust this based on the actual structure\n",
    "            df = pd.json_normalize(data[main_data_key])\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing JSON structure: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeNetwork:\n",
    "    \"\"\"\n",
    "    We define a class which computes the MST trade network for a given year \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"data\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_features(self, filter_gdp = True):\n",
    "        \n",
    "        ###IMPORT GDP###\n",
    "        #prepare GDP as a set of features \n",
    "        with open('data/all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "\n",
    "        self.gdp = features_dict['NY.GDP.MKTP.CD']\n",
    "        scaler = StandardScaler()\n",
    "        self.gdp[[\"prev_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year-1)]]))\n",
    "        self.gdp[[\"current_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year)]]))\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp[\"country_code\"] = self.gdp[\"economy\"]\n",
    "        self.gdp = self.gdp[[\"country_code\", \"prev_gdp\", \"current_gdp\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP GROWTH###\n",
    "        #prepare GDP growth\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth['YR'+str(self.year)] \n",
    "        self.gdp_growth[\"future_gdp_growth\"] = self.gdp_growth['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_growth[\"country_code\"] = self.gdp_growth[\"economy\"]\n",
    "        self.gdp_growth = self.gdp_growth[[\"country_code\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\", \"future_gdp_growth\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA###\n",
    "        self.gdp_per_capita = features_dict['NY.GDP.PCAP.CD']\n",
    "        self.gdp_per_capita[\"prev_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita[\"current_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year)]\n",
    "        self.gdp_per_capita[\"future_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita[\"country_code\"] = self.gdp_per_capita[\"economy\"]\n",
    "        self.gdp_per_capita = self.gdp_per_capita[[\"country_code\", \"prev_gdp_per_cap\",\n",
    "                                \"current_gdp_per_cap\", \"future_gdp_per_cap\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA GROWTH###\n",
    "        self.gdp_per_capita_growth = features_dict['NY.GDP.PCAP.KD.ZG']\n",
    "        self.gdp_per_capita_growth[\"prev_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita_growth[\"current_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year)]\n",
    "        self.gdp_per_capita_growth[\"future_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year+1)]\n",
    "        \n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita_growth[\"country_code\"] = self.gdp_per_capita_growth[\"economy\"]\n",
    "        self.gdp_per_capita_growth = self.gdp_per_capita_growth[[\"country_code\", \"prev_gdp_per_cap_growth\",\n",
    "                                \"current_gdp_per_cap_growth\", \"future_gdp_per_cap_growth\"]].dropna()\n",
    "        \n",
    "        ###MERGE ALL DATA FEATURES###\n",
    "        self.features = pd.merge(self.gdp_growth, self.gdp, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita_growth, on = \"country_code\").dropna()\n",
    "\n",
    "    def prepare_network(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(\"data/ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"/country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        #trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        #trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(self.export_types, exporter_total, on = \"exporter_code\")\n",
    "        #multiply by 100 to allow weights to scale better in GNN\n",
    "        self.export_types[\"category_fraction\"] = self.export_types.import_value/self.export_types.export_total*10\n",
    "        ss = StandardScaler()\n",
    "        columns = list(set(self.export_types[\"product_category\"]))\n",
    "        self.export_types = self.export_types[[\"exporter_code\", \"product_category\", \"category_fraction\"]]\\\n",
    "        .pivot(index = [\"exporter_code\"], columns = [\"product_category\"], values = \"category_fraction\")\\\n",
    "        .reset_index().fillna(0)\n",
    "        #rename columns\n",
    "        rename_columns = []\n",
    "        for col in self.export_types.columns:\n",
    "            if(col == \"exporter_code\"):\n",
    "                rename_columns.append(col)\n",
    "            else:\n",
    "                rename_columns.append(\"resource_\" + col)\n",
    "        self.export_types.columns = rename_columns\n",
    "        self.export_types = self.export_types.rename(columns = {\"exporter_code\": \"country_code\"})\n",
    "        self.features = pd.merge(self.features, self.export_types, \n",
    "                                on = \"country_code\", how = \"left\")\n",
    "        \n",
    "        #look at fraction of goods traded with each counterparty\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, exporter_total, how = \"left\")\n",
    "        imports_table_grouped[\"export_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"export_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"export_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"export_percent\"]]))\n",
    "        imports_table_grouped[\"export_percent_feature\"] = imports_table_grouped[\"export_percent_feature\"] + abs(min(imports_table_grouped[\"export_percent_feature\"]))\n",
    "        \n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, how = \"left\")\n",
    "        imports_table_grouped[\"import_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"import_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"import_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"import_percent\"]]))\n",
    "        imports_table_grouped[\"import_percent_feature\"] = imports_table_grouped[\"import_percent_feature\"] + abs(min(imports_table_grouped[\"import_percent_feature\"]))\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "\n",
    "    def graph_create(self, exporter = True,\n",
    "            node_features = ['prev_gdp_growth', 'current_gdp_growth','prev_gdp','current_gdp'],\n",
    "            node_labels = 'future_gdp_growth'):\n",
    "        \n",
    "        if(exporter):\n",
    "            center_node = \"exporter_code\"\n",
    "            neighbors = \"importer_code\"\n",
    "            edge_features = 'export_percent'\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        # list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "        #                 list(self.trade_data [\"exporter_code\"])))\n",
    "\n",
    "        list_active_countries = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']\n",
    "\n",
    "        # Create a new DataFrame with the list of country codes\n",
    "        df_new = pd.DataFrame(list_active_countries, columns=['country_code'])\n",
    "        self.features = pd.merge(df_new, self.features, on='country_code', how='left')\n",
    "        self.features = self.features.fillna(0)\n",
    "\n",
    "        # self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        # self.features.fillna(0, inplace = True)\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "\n",
    "        #create lookup dictionary making node number / node features combatible with ordering of nodes\n",
    "        #in our edge table\n",
    "\n",
    "        self.node_lookup1 = self.features.set_index('node_numbers').to_dict()['country_code']\n",
    "        self.node_lookup2 = self.features.set_index('country_code').to_dict()['node_numbers']\n",
    "        \n",
    "        #get individual country's features\n",
    "        self.regression_table = pd.merge(self.features, self.trade_data,\n",
    "                        left_on = \"country_code\",\n",
    "                        right_on = center_node, how = 'right')\n",
    "        #get features for trade partners\n",
    "        self.regression_table = pd.merge(self.features, self.regression_table,\n",
    "                                        left_on = \"country_code\",\n",
    "                                        right_on = neighbors, how = \"right\",\n",
    "                                        suffixes = (\"_neighbors\", \"\"))\n",
    "        \n",
    "        self.trade_data = self.trade_data[self.trade_data[neighbors].isin(self.node_lookup2)]\n",
    "        self.trade_data = self.trade_data[self.trade_data[center_node].isin(self.node_lookup2)]\n",
    "\n",
    "        self.regression_table[\"source\"] = self.trade_data[neighbors].apply(lambda x: self.node_lookup2[x])\n",
    "        self.regression_table[\"target\"] = self.trade_data[center_node].apply(lambda x: self.node_lookup2[x])    \n",
    "\n",
    "        self.regression_table = self.regression_table.dropna()\n",
    "        #filter only to relevant columns\n",
    "        self.relevant_columns = [\"source\", \"target\"]\n",
    "        self.relevant_columns.extend(node_features)\n",
    "        self.relevant_columns.append(node_labels)\n",
    "        self.graph_table = self.regression_table[self.relevant_columns]\n",
    "        \n",
    "        if(self.graph_table.isnull().values.any()): print(\"edges contain null / inf values\")\n",
    "\n",
    "        self.node_attributes = torch.tensor(np.array(self.features[node_features]))\\\n",
    "        .to(torch.float)\n",
    "        self.source_nodes = list(self.graph_table[\"source\"])\n",
    "        self.target_nodes = list(self.graph_table[\"target\"])\n",
    "\n",
    "        self.edge_attributes = list(self.trade_data[edge_features])\n",
    "        \n",
    "        self.pyg_graph = data.Data(x = self.node_attributes,\n",
    "                                   edge_index = torch.tensor([self.source_nodes, self.target_nodes]),\n",
    "                                   edge_attr = torch.tensor(self.edge_attributes).to(torch.float),\n",
    "                                   y = torch.tensor(list(self.features[node_labels])).to(torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1962,2019)\n",
    "\n",
    "train_years = [2005, 1969, 2002, 1997, 1993, 1982, 2001, 2000, 1962, 1985, 1978, 2016, 1986, 1987, 1989, 1971, 2013, 1996, 1995, 1967, 2017, 1974, 1990, 1977, 1980, 2014, 1965, 1984, 2006, 1973, 1968, 1981, 1970, 1991]\n",
    "val_years = [1975, 1983, 2009, 1966, 1999, 1988, 2007, 1979, 1972, 2015, 2003]\n",
    "test_years = [1963, 1964, 1976, 1992, 1994, 1998, 2004, 2008, 2010, 2011, 2012, 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/57 [00:05<05:34,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1963\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2/57 [00:11<05:14,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1964\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/57 [00:17<05:07,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1965\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/57 [00:23<05:08,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1966\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/57 [00:29<05:09,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1967\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6/57 [00:35<05:06,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1968\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/57 [00:42<05:09,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1969\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8/57 [00:48<05:10,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1970\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 9/57 [00:55<05:16,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1971\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 10/57 [01:03<05:27,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1972\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 11/57 [01:11<05:35,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1973\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 12/57 [01:20<05:50,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1974\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 13/57 [01:29<06:00,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1975\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 14/57 [01:38<06:05,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 15/57 [01:43<05:00,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1977\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 16/57 [01:48<04:26,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1978\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 17/57 [01:54<04:19,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1979\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 18/57 [02:01<04:13,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1980\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 19/57 [02:08<04:17,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1981\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 20/57 [02:15<04:14,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1982\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 21/57 [02:22<04:07,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1983\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 22/57 [02:29<04:02,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1984\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 23/57 [02:36<03:55,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1985\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 24/57 [02:43<03:53,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1986\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 25/57 [02:52<03:59,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1987\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 26/57 [03:01<04:08,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1988\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 27/57 [03:10<04:10,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1989\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 28/57 [03:19<04:10,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1990\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 29/57 [03:29<04:11,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1991\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 30/57 [03:40<04:15,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1992\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 31/57 [03:51<04:16,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1993\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 32/57 [04:02<04:16, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1994\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 33/57 [04:14<04:16, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1995\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 34/57 [04:25<04:13, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1996\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 35/57 [04:39<04:18, 11.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1997\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 36/57 [04:52<04:18, 12.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1998\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 37/57 [05:06<04:14, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1999\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 38/57 [05:21<04:12, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 39/57 [05:37<04:15, 14.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2001\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 40/57 [05:54<04:18, 15.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2002\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 41/57 [06:09<04:01, 15.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2003\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 42/57 [06:25<03:48, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2004\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 43/57 [06:42<03:39, 15.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2005\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 44/57 [07:00<03:35, 16.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2006\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 45/57 [07:19<03:25, 17.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2007\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 46/57 [07:37<03:11, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2008\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 47/57 [07:57<03:01, 18.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2009\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 48/57 [08:16<02:47, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2010\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 49/57 [08:36<02:31, 18.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2011\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 50/57 [08:56<02:14, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2012\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 51/57 [09:16<01:56, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2013\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 52/57 [09:37<01:40, 20.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2014\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 53/57 [09:59<01:21, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2015\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 54/57 [10:19<01:01, 20.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2016\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 55/57 [10:40<00:41, 20.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2017\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 56/57 [11:00<00:20, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2018\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [11:22<00:00, 11.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_graphs = []\n",
    "val_graphs = []\n",
    "test_graphs = []\n",
    "i = 0\n",
    "\n",
    "for year in tqdm(years):\n",
    "    print(str(year), end='\\r')\n",
    "    \n",
    "    trade = TradeNetwork(year = year)\n",
    "    trade.prepare_features()\n",
    "    trade.prepare_network()\n",
    "    trade.graph_create(node_features = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth',\n",
    "    'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "       'resource_8', 'resource_9'],\n",
    "        node_labels = 'future_gdp_per_cap_growth')\n",
    "    \n",
    "    if(year in val_years):\n",
    "        val_graphs.append(trade.pyg_graph)\n",
    "    elif(year in test_years):\n",
    "        test_graphs.append(trade.pyg_graph)\n",
    "    else: \n",
    "        train_graphs.append(trade.pyg_graph)\n",
    "        \n",
    "    trade.features[\"year\"] = year\n",
    "    \n",
    "    if(i == 0):\n",
    "        trade_df = trade.features\n",
    "    else: \n",
    "        trade_df = pd.concat([trade_df, trade.features])\n",
    "        \n",
    "    i = i+1\n",
    "    print(trade.node_attributes.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# years = list(range(1962,2021))\n",
    "\n",
    "# # Determine the sizes of the train, validation, and test sets\n",
    "# train_size = int(len(years) * 0.6)\n",
    "# val_size = int(len(years) * 0.20)\n",
    "\n",
    "# # Get a random subset for the train set\n",
    "# train_years = random.sample(years, train_size)\n",
    "\n",
    "# # Remove the years in the train set from the list of years\n",
    "# years = [year for year in years if year not in train_years]\n",
    "\n",
    "# # Get a random subset for the validation set\n",
    "# val_years = random.sample(years, val_size)\n",
    "\n",
    "# # Remove the years in the validation set from the list of years\n",
    "# test_years = [year for year in years if year not in val_years]\n",
    "\n",
    "# print(\"Train years:\", train_years)\n",
    "# print(\"Validation years:\", val_years)\n",
    "# print(\"Test years:\", test_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pygcn/train_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(train_graphs, f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(val_graphs, f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(test_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pygcn/train_graphs.pickle\", \"rb\") as f:\n",
    "    train_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"rb\") as f:  \n",
    "    val_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"rb\") as f:         \n",
    "    test_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test_graphs, batch_size=4)\n",
    "train_loader = DataLoader(train_graphs, batch_size=4)\n",
    "val_loader = DataLoader(val_graphs, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sGNN with GCN Encoder and 3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crisis_years(year_pairs, crisis_years):\n",
    "    result = []\n",
    "    for pair in year_pairs:\n",
    "        start, end = pair\n",
    "        # Check if any crisis year is between the pair or equals the later year\n",
    "        if any(start < year <= end for year in crisis_years):\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1983, 1982, 2008, 2002, 2016, 1967, 1962, 1989, 2012, 1963, 1993, 1986, 1996,1978]\n",
    "\n",
    "def get_year_pairs(year_range):\n",
    "    return [(year1, year2) for year1 in year_range for year2 in year_range if year2 >= year1]\n",
    "\n",
    "def get_loader_pairs(dataset):\n",
    "    return [(dataset[i], dataset[j]) for i in range(len(dataset)) for j in range(len(dataset)) if j >= i]\n",
    "\n",
    "train_pairs = get_year_pairs(train_years)\n",
    "val_pairs = get_year_pairs(val_years)\n",
    "\n",
    "train_y = check_crisis_years(train_pairs, crisis_years)\n",
    "val_y = check_crisis_years(val_pairs, crisis_years)\n",
    "\n",
    "train_loader_pairs = get_loader_pairs(train_loader.dataset)\n",
    "val_loader_pairs = get_loader_pairs(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_y = torch.tensor(np.array(train_y))\n",
    "val_torch_y = torch.tensor(np.array(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, global_sort_pool\n",
    "from torch_geometric.nn.aggr import SortAggregation\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Sigmoid\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index.to(torch.int64)\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        return x\n",
    "    \n",
    "class FullyConnectedLayer(torch.nn.Module):\n",
    "    def __init__(self, inner_dim, outer_dim):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.affine = Linear(inner_dim, outer_dim)\n",
    "        self.batch_norm = BatchNorm1d(outer_dim)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, out):\n",
    "        x = self.affine(out)\n",
    "        print(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class SiameseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SiameseGNN, self).__init__()\n",
    "        self.gnn = GNN(num_features)\n",
    "        self.sort_aggr = SortAggregation(k=50)\n",
    "        self.fc1 = FullyConnectedLayer(9950, 64)\n",
    "        self.fc2 = FullyConnectedLayer(64, 1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        out1 = self.gnn(data1)\n",
    "        out2 = self.gnn(data2)\n",
    "        out = torch.cdist(out1, out2, p=2) #Euclidean Distance\n",
    "        out = self.sort_aggr(out, data1.batch) #Sort-K Pooling Layer\n",
    "        out = out.view(out.size(0), -1)  # Flatten the pooled output\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = global_add_pool(out)\n",
    "\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1336, -1.6735,  0.7404,  1.5737, -0.1376,  0.9168,  0.1814,  1.0988,\n",
      "          0.7630, -0.2472, -0.5239,  0.5975, -3.2576, -0.1216,  1.7809, -0.4495,\n",
      "          0.6621, -0.2564,  2.1918, -1.1255, -0.1888,  2.2564, -0.7542,  0.3808,\n",
      "         -0.6299,  1.7575,  1.6498, -0.5258,  1.5399, -0.3623,  0.6560,  0.6126,\n",
      "          0.7954,  0.5136, -3.1180, -0.3158,  0.6231, -1.4749, -0.2006, -0.4121,\n",
      "          0.5074,  1.9828,  1.5758, -1.2261, -1.4405, -2.9652, -0.9406, -1.4045,\n",
      "         -0.4538, -0.7102,  1.7911, -1.7468,  0.7116, -0.0946, -0.6196,  0.5326,\n",
      "         -2.2619, -0.7477, -0.9208,  0.5321, -1.1388, -0.5202, -0.3297,  1.0752]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 64])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader)):\n\u001b[1;32m     83\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 84\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(train_loader[i][\u001b[38;5;241m0\u001b[39m], train_loader[i][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     85\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39msqueeze(), train_torch_y[i]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat))\n\u001b[1;32m     86\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[126], line 59\u001b[0m, in \u001b[0;36mSiameseGNN.forward\u001b[0;34m(self, data1, data2)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Ensure the output shape is suitable for the FullyConnectedLayer\u001b[39;00m\n\u001b[1;32m     57\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the pooled output\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(out)\n\u001b[1;32m     60\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Ensure the shape is correct before passing to global_add_pool\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[126], line 34\u001b[0m, in \u001b[0;36mFullyConnectedLayer.forward\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(x)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 64])"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.aggr import SortAggregation\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU, Sigmoid\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index.to(torch.int64)\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        return x\n",
    "    \n",
    "class FullyConnectedLayer(torch.nn.Module):\n",
    "    def __init__(self, inner_dim, outer_dim):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.affine = Linear(inner_dim, outer_dim)\n",
    "        self.batch_norm = BatchNorm1d(outer_dim)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, out):\n",
    "        x = self.affine(out)\n",
    "        if len(x.size()) == 3:  # Ensure the input is 2D\n",
    "            x = x.view(x.size(0) * x.size(1), -1)\n",
    "        print(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class SiameseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SiameseGNN, self).__init__()\n",
    "        self.gnn = GNN(num_features)\n",
    "        self.sort_aggr = SortAggregation(k=50)\n",
    "        self.fc1 = FullyConnectedLayer(9950, 64)  # Adjust based on correct input size\n",
    "        self.fc2 = FullyConnectedLayer(64, 1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        out1 = self.gnn(data1)\n",
    "        out2 = self.gnn(data2)\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        out = torch.cdist(out1, out2, p=2) # Euclidean Distance\n",
    "        \n",
    "        # Sort-K Pooling Layer\n",
    "        out = self.sort_aggr(out, data1.batch)\n",
    "        \n",
    "        # Ensure the output shape is suitable for the FullyConnectedLayer\n",
    "        out = out.view(out.size(0), -1)  # Flatten the pooled output\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        # Ensure the shape is correct before passing to global_add_pool\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = global_add_pool(out, data1.batch)\n",
    "\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "train_loader = train_loader_pairs\n",
    "val_loader = val_loader_pairs\n",
    "\n",
    "model = SiameseGNN(num_features=12)  # Adjust the number of features accordingly\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(5)):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i in range(len(train_loader)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_loader[i][0], train_loader[i][1])\n",
    "        loss = criterion(out.squeeze(), train_torch_y[i].to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()  # Add this line to update the learning rate\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(val_loader)):\n",
    "            out = model(val_loader[i][0], val_loader[i][1])\n",
    "            val_loss = criterion(out.squeeze(), val_torch_y[i].to(torch.float))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            predictions = torch.round(out.squeeze())\n",
    "            correct += (predictions == val_torch_y[i]).sum().item()\n",
    "            total += len(val_torch_y[i])\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader[0][0].batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[199, 12], edge_index=[2, 4420], edge_attr=[4420], y=[199])\n",
      "tensor([[ 1.0423,  1.1136, -0.1196, -0.0634,  0.7821,  2.1531,  1.9750,  0.7092,\n",
      "         -3.4137,  0.3919, -1.0351,  0.7561, -0.9224,  0.0053,  0.0116,  2.2491,\n",
      "          1.3743, -1.7310,  0.3565,  1.1817, -1.2984,  0.8230,  1.2342, -2.7720,\n",
      "          0.0072,  1.2638,  0.5268,  1.4335,  1.6768,  2.1548,  2.0185,  1.0940,\n",
      "          2.7574,  0.0113,  0.1723, -0.0854,  1.2454, -1.4721,  1.2925,  0.2090,\n",
      "          0.4599, -0.9316,  0.3393,  0.6429,  0.1661, -0.0868,  1.2037, -2.8661,\n",
      "         -1.0396,  0.0494,  0.7910, -1.6080,  1.3443,  0.3441, -2.6085, -0.1864,\n",
      "         -0.9635, -0.5269, -0.6951,  2.5409,  0.0445, -0.0707, -2.2662,  0.2011]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 64])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_loader[i][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m out \u001b[38;5;241m=\u001b[39m model(train_loader[i][\u001b[38;5;241m0\u001b[39m], train_loader[i][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39msqueeze(), train_torch_y[i]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat))\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[98], line 53\u001b[0m, in \u001b[0;36mSiameseGNN.forward\u001b[0;34m(self, data1, data2)\u001b[0m\n\u001b[1;32m     51\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_aggr(out, data1\u001b[38;5;241m.\u001b[39mbatch) \u001b[38;5;66;03m#Sort-K Pooling Layer\u001b[39;00m\n\u001b[1;32m     52\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the pooled output\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(out)\n\u001b[1;32m     54\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n\u001b[1;32m     55\u001b[0m out \u001b[38;5;241m=\u001b[39m global_add_pool(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[98], line 35\u001b[0m, in \u001b[0;36mFullyConnectedLayer.forward\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maffine(out)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m---> 35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(x)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 64])"
     ]
    }
   ],
   "source": [
    "# DataLoader that loads pairs of graphs\n",
    "train_loader = train_loader_pairs\n",
    "val_loader = val_loader_pairs\n",
    "\n",
    "model = SiameseGNN(num_features=train_loader[0][0].num_node_features)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(5)):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i in range(len(train_loader)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        print(train_loader[i][0])\n",
    "        out = model(train_loader[i][0], train_loader[i][1])\n",
    "        loss = criterion(out.squeeze(), train_torch_y[i].to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()  # Add this line to update the learning rate\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(val_loader)):\n",
    "            out = model(val_loader[i][0], val_loader[i][1])\n",
    "            val_loss = criterion(out.squeeze(), val_torch_y[i].to(torch.float))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            predictions = torch.round(out.squeeze())\n",
    "            correct += (predictions == val_torch_y[i]).sum().item()\n",
    "            total += 1\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sGNN with Feature Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateFeatures:\n",
    "    \"\"\"\n",
    "    We define a class which builds the feature dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"../data/\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_econ_features(self, filter_gdp = True):\n",
    "        \n",
    "        #DATA IMPORT\n",
    "        #import dictionary with all features from WB\n",
    "        with open(self.data_dir + 'all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "            \n",
    "        self.feature_list = list(features_dict.keys())[1:]\n",
    "        #import list of all features we want to select for\n",
    "\n",
    "        #look up each of the features -- add country feature in that year \n",
    "        i = 0\n",
    "        for feature in self.feature_list:\n",
    "            #find dataframe corresponding to specific feature name\n",
    "            df = features_dict[feature]\n",
    "            \n",
    "            if (i == 0):\n",
    "                self.features = df[[\"economy\", \"YR\" + str(self.year)]]\n",
    "            else: \n",
    "                self.features = pd.merge(self.features, \n",
    "                                            df[[\"economy\", \"YR\" + str(self.year)]],\n",
    "                                            on = \"economy\", how = \"outer\")\n",
    "            self.features.rename(columns = {\"YR\" + str(self.year): feature}, inplace = True)\n",
    "            i = i+1\n",
    "        \n",
    "        #prepare GDP feature\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        cols = list(self.gdp_growth.columns.copy())\n",
    "        cols.remove(\"economy\")\n",
    "        self.gdp_growth[\"country_sd\"] = self.gdp_growth[cols].std(axis=1)\n",
    "        #select potential variables \n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year)]\n",
    "        #we eliminate countries that are too volatile in growth -- probably an indicator that growth estimates are inaccurate\n",
    "        self.gdp_growth = self.gdp_growth[[\"economy\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\"]].dropna()\n",
    "        \n",
    "        #combine GDP and other features\n",
    "        self.features = pd.merge(self.gdp_growth, self.features,\n",
    "                                   on = \"economy\", how = \"left\")\n",
    "        #we only keep countries where we observe GDP growth -- otherwise nothing to predict\n",
    "        #we keep countries where other features may be missing -- and fill NAs with 0 \n",
    "        self.features.rename(columns = {\"economy\": \"country_code\"}, inplace = True)\n",
    "        \n",
    "    def prepare_network_features(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(self.data_dir + \"ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        #trade_data = trade_data[trade_data[\"product_category\"] == \"1\"]\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, on = \"importer_code\")\n",
    "        imports_table_grouped[\"import_fraction\"] = imports_table_grouped[\"import_value\"]\\\n",
    "                        /imports_table_grouped[\"import_total\"]*100\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        \n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "        \n",
    "        self.G = G\n",
    "        self.centrality_overall= nx.eigenvector_centrality(G, max_iter= 10000) \n",
    "        self.centrality_overall = pd.DataFrame(list(map(list, self.centrality_overall.items())), \n",
    "                                               columns = [\"country_code\", \"centrality_overall\"])\n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", [\"import_fraction\"])\n",
    "        weighted_centrality = nx.eigenvector_centrality(G, weight = \"import_fraction\", max_iter= 10000) \n",
    "        weighted_centrality  = pd.DataFrame(list(map(list, weighted_centrality.items())), \n",
    "                                               columns = [\"country_code\", \"weighted_centrality\"])\n",
    "        self.centrality_overall = pd.merge(self.centrality_overall, weighted_centrality, on = \"country_code\")\n",
    "        \n",
    "                               \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY IN PRODUCT CATEGORIES\n",
    "\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = imports_table.groupby([\"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = products_grouped.rename(columns = {\"import_value\": \"import_product_total\"})\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"importer_code\", \"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(products_grouped, self.export_types, on = \"product_category\")\n",
    "        \n",
    "        self.export_types[\"product_export_fraction\"] = self.export_types[\"import_value\"]\\\n",
    "                                                    /self.export_types[\"import_product_total\"]*100\n",
    "        \n",
    "        list_products = list(set(self.export_types[\"product_category\"]))\n",
    "        \n",
    "        i = 0 \n",
    "        for product in list_products:\n",
    "            \n",
    "            temp = self.export_types[self.export_types[\"product_category\"] == product].copy()\n",
    "            \n",
    "            G_w=nx.from_pandas_edgelist(temp, \n",
    "                \"exporter_code\", \"importer_code\", [\"product_export_fraction\"], create_using = nx.DiGraph())\n",
    "            centrality_product_w = nx.eigenvector_centrality(G_w, weight = \"product_export_fraction\", \n",
    "                                                           max_iter= 10000)\n",
    "\n",
    "            G=nx.from_pandas_edgelist(temp,\"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "            centrality_product = nx.eigenvector_centrality(G,max_iter= 10000)\n",
    "\n",
    "            if(i == 0):\n",
    "                self.centrality_product = pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product])\n",
    "                \n",
    "\n",
    "            else: \n",
    "                self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "                \n",
    "            self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product_w.items())), \n",
    "                                               columns = [\"country_code\", \"prod_w_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "            \n",
    "            i = i+1         \n",
    "    \n",
    "    def combine_normalize_features(self):\n",
    "        \n",
    "        self.combined_features = pd.merge(self.features, self.centrality_overall,on = \"country_code\")\n",
    "        self.combined_features = pd.merge(self.combined_features, self.centrality_product,on = \"country_code\")\n",
    "        #step eliminates NA and nodes that are not in graph, since they will have NA for graph features\n",
    "        self.combined_features = self.combined_features.drop(columns = [\"index\"])\n",
    "        #filter both trade data and features data to same subset of countries\n",
    "        self.combined_features = self.combined_features[\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.importer_code)|\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.exporter_code)]\n",
    "        self.trade_data = self.trade_data[\\\n",
    "                          self.trade_data.importer_code.isin(self.combined_features.country_code)&\\\n",
    "                          self.trade_data.exporter_code.isin(self.combined_features.country_code)]\n",
    "        \n",
    "        features_to_norm = list(self.combined_features.columns.copy())\n",
    "        non_norm = [\"country_code\", \"node_numbers\"]\n",
    "        cols_insufficient_data = list(self.combined_features.loc[:, self.combined_features.nunique() < 2].columns.copy())\n",
    "        non_norm.extend(cols_insufficient_data)\n",
    " \n",
    "        features_to_norm = [x for x in features_to_norm if x not in non_norm]\n",
    "        scaler = StandardScaler()\n",
    "        #we preserve NAs in the scaling\n",
    "        self.combined_features[features_to_norm] = scaler.fit_transform(self.combined_features[features_to_norm])\n",
    "        self.combined_features.fillna(0, inplace = True) #we fill NA after scaling \n",
    "        #check that feature has at least 20% coverage in a given year -- otherwise set to NA\n",
    "        for feature in self.feature_list:\n",
    "            coverage = len(self.combined_features[self.combined_features[feature] != 0])/len(self.combined_features)\n",
    "            if(coverage <= 0.20): self.combined_features[feature] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeNetwork:\n",
    "    \"\"\"\n",
    "    We define a class which computes the MST trade network for a given year \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"data\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_features(self, filter_gdp = True):\n",
    "        \n",
    "        ###IMPORT GDP###\n",
    "        #prepare GDP as a set of features \n",
    "        with open('data/all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "\n",
    "        self.gdp = features_dict['NY.GDP.MKTP.CD']\n",
    "        scaler = StandardScaler()\n",
    "        self.gdp[[\"prev_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year-1)]]))\n",
    "        self.gdp[[\"current_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year)]]))\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp[\"country_code\"] = self.gdp[\"economy\"]\n",
    "        self.gdp = self.gdp[[\"country_code\", \"prev_gdp\", \"current_gdp\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP GROWTH###\n",
    "        #prepare GDP growth\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth['YR'+str(self.year)] \n",
    "        self.gdp_growth[\"future_gdp_growth\"] = self.gdp_growth['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_growth[\"country_code\"] = self.gdp_growth[\"economy\"]\n",
    "        self.gdp_growth = self.gdp_growth[[\"country_code\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\", \"future_gdp_growth\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA###\n",
    "        self.gdp_per_capita = features_dict['NY.GDP.PCAP.CD']\n",
    "        self.gdp_per_capita[\"prev_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita[\"current_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year)]\n",
    "        self.gdp_per_capita[\"future_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita[\"country_code\"] = self.gdp_per_capita[\"economy\"]\n",
    "        self.gdp_per_capita = self.gdp_per_capita[[\"country_code\", \"prev_gdp_per_cap\",\n",
    "                                \"current_gdp_per_cap\", \"future_gdp_per_cap\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA GROWTH###\n",
    "        self.gdp_per_capita_growth = features_dict['NY.GDP.PCAP.KD.ZG']\n",
    "        self.gdp_per_capita_growth[\"prev_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita_growth[\"current_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year)]\n",
    "        self.gdp_per_capita_growth[\"future_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year+1)]\n",
    "        \n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita_growth[\"country_code\"] = self.gdp_per_capita_growth[\"economy\"]\n",
    "        self.gdp_per_capita_growth = self.gdp_per_capita_growth[[\"country_code\", \"prev_gdp_per_cap_growth\",\n",
    "                                \"current_gdp_per_cap_growth\", \"future_gdp_per_cap_growth\"]].dropna()\n",
    "        \n",
    "        ###MERGE ALL DATA FEATURES###\n",
    "        self.features = pd.merge(self.gdp_growth, self.gdp, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita_growth, on = \"country_code\").dropna()\n",
    "\n",
    "    def prepare_network(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(\"data/ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"/country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        #trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        #trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(self.export_types, exporter_total, on = \"exporter_code\")\n",
    "        #multiply by 100 to allow weights to scale better in GNN\n",
    "        self.export_types[\"category_fraction\"] = self.export_types.import_value/self.export_types.export_total*10\n",
    "        ss = StandardScaler()\n",
    "        columns = list(set(self.export_types[\"product_category\"]))\n",
    "        self.export_types = self.export_types[[\"exporter_code\", \"product_category\", \"category_fraction\"]]\\\n",
    "        .pivot(index = [\"exporter_code\"], columns = [\"product_category\"], values = \"category_fraction\")\\\n",
    "        .reset_index().fillna(0)\n",
    "        #rename columns\n",
    "        rename_columns = []\n",
    "        for col in self.export_types.columns:\n",
    "            if(col == \"exporter_code\"):\n",
    "                rename_columns.append(col)\n",
    "            else:\n",
    "                rename_columns.append(\"resource_\" + col)\n",
    "        self.export_types.columns = rename_columns\n",
    "        self.export_types = self.export_types.rename(columns = {\"exporter_code\": \"country_code\"})\n",
    "        self.features = pd.merge(self.features, self.export_types, \n",
    "                                on = \"country_code\", how = \"left\")\n",
    "        \n",
    "        #look at fraction of goods traded with each counterparty\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, exporter_total, how = \"left\")\n",
    "        imports_table_grouped[\"export_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"export_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"export_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"export_percent\"]]))\n",
    "        imports_table_grouped[\"export_percent_feature\"] = imports_table_grouped[\"export_percent_feature\"] + abs(min(imports_table_grouped[\"export_percent_feature\"]))\n",
    "        \n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, how = \"left\")\n",
    "        imports_table_grouped[\"import_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"import_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"import_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"import_percent\"]]))\n",
    "        imports_table_grouped[\"import_percent_feature\"] = imports_table_grouped[\"import_percent_feature\"] + abs(min(imports_table_grouped[\"import_percent_feature\"]))\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "\n",
    "    def graph_create(self, exporter = True,\n",
    "            node_features = ['prev_gdp_growth', 'current_gdp_growth','prev_gdp','current_gdp'],\n",
    "            node_labels = 'future_gdp_growth'):\n",
    "        \n",
    "        if(exporter):\n",
    "            center_node = \"exporter_code\"\n",
    "            neighbors = \"importer_code\"\n",
    "            edge_features = 'export_percent'\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        # list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "        #                 list(self.trade_data [\"exporter_code\"])))\n",
    "\n",
    "        list_active_countries = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']\n",
    "\n",
    "        # Create a new DataFrame with the list of country codes\n",
    "        df_new = pd.DataFrame(list_active_countries, columns=['country_code'])\n",
    "        self.features = pd.merge(df_new, self.features, on='country_code', how='left')\n",
    "        self.features = self.features.fillna(0)\n",
    "\n",
    "        # self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        # self.features.fillna(0, inplace = True)\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "\n",
    "        #create lookup dictionary making node number / node features combatible with ordering of nodes\n",
    "        #in our edge table\n",
    "\n",
    "        self.node_lookup1 = self.features.set_index('node_numbers').to_dict()['country_code']\n",
    "        self.node_lookup2 = self.features.set_index('country_code').to_dict()['node_numbers']\n",
    "        \n",
    "        #get individual country's features\n",
    "        self.regression_table = pd.merge(self.features, self.trade_data,\n",
    "                        left_on = \"country_code\",\n",
    "                        right_on = center_node, how = 'right')\n",
    "        #get features for trade partners\n",
    "        self.regression_table = pd.merge(self.features, self.regression_table,\n",
    "                                        left_on = \"country_code\",\n",
    "                                        right_on = neighbors, how = \"right\",\n",
    "                                        suffixes = (\"_neighbors\", \"\"))\n",
    "        \n",
    "        self.trade_data = self.trade_data[self.trade_data[neighbors].isin(self.node_lookup2)]\n",
    "        self.trade_data = self.trade_data[self.trade_data[center_node].isin(self.node_lookup2)]\n",
    "\n",
    "        self.regression_table[\"source\"] = self.trade_data[neighbors].apply(lambda x: self.node_lookup2[x])\n",
    "        self.regression_table[\"target\"] = self.trade_data[center_node].apply(lambda x: self.node_lookup2[x])    \n",
    "\n",
    "        self.regression_table = self.regression_table.dropna()\n",
    "        #filter only to relevant columns\n",
    "        self.relevant_columns = [\"source\", \"target\"]\n",
    "        self.relevant_columns.extend(node_features)\n",
    "        self.relevant_columns.append(node_labels)\n",
    "        self.graph_table = self.regression_table[self.relevant_columns]\n",
    "        \n",
    "        if(self.graph_table.isnull().values.any()): print(\"edges contain null / inf values\")\n",
    "\n",
    "        self.node_attributes = torch.tensor(np.array(self.features[node_features]))\\\n",
    "        .to(torch.float)\n",
    "        self.source_nodes = list(self.graph_table[\"source\"])\n",
    "        self.target_nodes = list(self.graph_table[\"target\"])\n",
    "\n",
    "        self.edge_attributes = list(self.trade_data[edge_features])\n",
    "        \n",
    "        self.pyg_graph = data.Data(x = self.node_attributes,\n",
    "                                   edge_index = torch.tensor([self.source_nodes, self.target_nodes]),\n",
    "                                   edge_attr = torch.tensor(self.edge_attributes).to(torch.float),\n",
    "                                   y = torch.tensor(list(self.features[node_labels])).to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_dicts/filtered_features_dict.pkl\", \"rb\") as f:\n",
    "    feat_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = range(1962, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pygcn/train_graphs.pickle\", \"rb\") as f:\n",
    "    train_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"rb\") as f:  \n",
    "    val_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"rb\") as f:         \n",
    "    test_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test_graphs, batch_size=32)\n",
    "train_loader = DataLoader(train_graphs, batch_size=32)\n",
    "val_loader = DataLoader(val_graphs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(years, graphs):\n",
    "\n",
    "    zeros = torch.zeros(59)\n",
    "\n",
    "    for i in range(len(years)):\n",
    "        new_x = torch.empty(0, 59)\n",
    "        year = years[i]\n",
    "\n",
    "        feat_dict_year = feat_dict[year].combined_features\n",
    "\n",
    "        for j, country in enumerate(all_nodes):\n",
    "            if j == 0:\n",
    "                new_x = torch.stack([zeros])\n",
    "\n",
    "            elif country in feat_dict_year[\"country_code\"].values:\n",
    "                tensor_before = graphs[i].x[j]\n",
    "                country_row = feat_dict_year[feat_dict_year[\"country_code\"] == country]\n",
    "                country_row = country_row.drop(columns = [\"prev_gdp_growth\", \"country_code\", \"current_gdp_growth\"])\n",
    "                row_values = country_row.values.tolist()\n",
    "                row_tensor = torch.tensor(row_values)[0]\n",
    "                combined_values = torch.cat((tensor_before, row_tensor))\n",
    "\n",
    "                new_x = torch.cat((new_x, combined_values.unsqueeze(0)), dim=0)\n",
    "\n",
    "            else:\n",
    "                new_x = torch.cat((new_x, zeros.unsqueeze(0)), dim=0)\n",
    "\n",
    "        graphs[i].x = new_x\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_years' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_graphs \u001b[38;5;241m=\u001b[39m add_features(train_years, train_graphs)\n\u001b[1;32m      2\u001b[0m val_graphs \u001b[38;5;241m=\u001b[39m add_features(val_years, val_graphs)\n\u001b[1;32m      3\u001b[0m test_graphs \u001b[38;5;241m=\u001b[39m add_features(test_years, test_graphs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_years' is not defined"
     ]
    }
   ],
   "source": [
    "train_graphs = add_features(train_years, train_graphs)\n",
    "val_graphs = add_features(val_years, val_graphs)\n",
    "test_graphs = add_features(test_years, test_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1983, 1982, 2008, 2002, 2016, 1967, 1962, 1989, 2012, 1963, 1993, 1986, 1996,1978]\n",
    "\n",
    "def get_year_pairs(year_range):\n",
    "    return [(year1, year2) for year1 in year_range for year2 in year_range if year2 >= year1]\n",
    "\n",
    "def get_loader_pairs(dataset):\n",
    "    return [(dataset[i], dataset[j]) for i in range(len(dataset)) for j in range(len(dataset)) if j >= i]\n",
    "\n",
    "train_pairs = get_year_pairs(train_years)\n",
    "val_pairs = get_year_pairs(val_years)\n",
    "\n",
    "train_y = check_crisis_years(train_pairs, crisis_years)\n",
    "val_y = check_crisis_years(val_pairs, crisis_years)\n",
    "\n",
    "train_loader_pairs = get_loader_pairs(train_loader.dataset)\n",
    "val_loader_pairs = get_loader_pairs(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_y = torch.tensor(np.array(train_y))\n",
    "val_torch_y = torch.tensor(np.array(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:13<01:57, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.4229666137895664, Validation Loss: 0.4994966710607211, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:25<01:42, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.41427947403628285, Validation Loss: 0.5009946116443836, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:39<01:33, 13.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.4123781653074156, Validation Loss: 0.5026700783408049, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:34<02:57, 29.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.4100686021462208, Validation Loss: 0.5035527772975691, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:29<05:02, 60.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.4088507873921835, Validation Loss: 0.5042239278554916, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:43<02:58, 44.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.41012620629758395, Validation Loss: 0.5044262244394331, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:55<01:42, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.40759952013232126, Validation Loss: 0.5053463794968345, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:07<00:54, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.4073875575068117, Validation Loss: 0.5056198169336175, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [04:19<00:22, 22.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.4068174197393305, Validation Loss: 0.5069572094715002, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [04:31<00:00, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.4066063172676984, Validation Loss: 0.508396189095396, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, global_sort_pool\n",
    "from torch_geometric.nn.aggr import SortAggregation\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Sigmoid\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index.to(torch.int64)\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        return x\n",
    "\n",
    "class SiameseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SiameseGNN, self).__init__()\n",
    "        self.gnn = GNN(num_features)\n",
    "        self.sort_aggr = SortAggregation(k=50)\n",
    "        self.fc1 = Linear(9950, 128)  # Adjust input size according to pooling output\n",
    "        self.norm1 = LayerNorm(128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Linear(128, 64)\n",
    "        self.norm2 = LayerNorm(64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc3 = Linear(64, 1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        out1 = self.gnn(data1)\n",
    "        out2 = self.gnn(data2)\n",
    "        out = torch.cdist(out1, out2, p=2)\n",
    "        out = self.sort_aggr(out, data1.batch)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the pooled output\n",
    "        out = self.fc1(out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "train_loader = train_loader_pairs\n",
    "val_loader = val_loader_pairs\n",
    "\n",
    "model = SiameseGNN(num_features=train_loader[0][0].num_node_features)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(10)):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i in range(len(train_loader)):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_loader[i][0], train_loader[i][1])\n",
    "        loss = criterion(out.squeeze(), train_torch_y[i].to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()  # Add this line to update the learning rate\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(val_loader)):\n",
    "            out = model(val_loader[i][0], val_loader[i][1])\n",
    "            val_loss = criterion(out.squeeze(), val_torch_y[i].to(torch.float))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            predictions = torch.round(out.squeeze())\n",
    "            correct += (predictions == val_torch_y[i]).sum().item()\n",
    "            total += 1\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Feature Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateFeatures:\n",
    "    \"\"\"\n",
    "    We define a class which builds the feature dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"../data/\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_econ_features(self, filter_gdp = True):\n",
    "        \n",
    "        #DATA IMPORT\n",
    "        #import dictionary with all features from WB\n",
    "        with open(self.data_dir + 'all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "            \n",
    "        self.feature_list = list(features_dict.keys())[1:]\n",
    "        #import list of all features we want to select for\n",
    "\n",
    "        #look up each of the features -- add country feature in that year \n",
    "        i = 0\n",
    "        for feature in self.feature_list:\n",
    "            #find dataframe corresponding to specific feature name\n",
    "            df = features_dict[feature]\n",
    "            \n",
    "            if (i == 0):\n",
    "                self.features = df[[\"economy\", \"YR\" + str(self.year)]]\n",
    "            else: \n",
    "                self.features = pd.merge(self.features, \n",
    "                                            df[[\"economy\", \"YR\" + str(self.year)]],\n",
    "                                            on = \"economy\", how = \"outer\")\n",
    "            self.features.rename(columns = {\"YR\" + str(self.year): feature}, inplace = True)\n",
    "            i = i+1\n",
    "        \n",
    "        #prepare GDP feature\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        cols = list(self.gdp_growth.columns.copy())\n",
    "        cols.remove(\"economy\")\n",
    "        self.gdp_growth[\"country_sd\"] = self.gdp_growth[cols].std(axis=1)\n",
    "        #select potential variables \n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year)]\n",
    "        #we eliminate countries that are too volatile in growth -- probably an indicator that growth estimates are inaccurate\n",
    "        self.gdp_growth = self.gdp_growth[[\"economy\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\"]].dropna()\n",
    "        \n",
    "        #combine GDP and other features\n",
    "        self.features = pd.merge(self.gdp_growth, self.features,\n",
    "                                   on = \"economy\", how = \"left\")\n",
    "        #we only keep countries where we observe GDP growth -- otherwise nothing to predict\n",
    "        #we keep countries where other features may be missing -- and fill NAs with 0 \n",
    "        self.features.rename(columns = {\"economy\": \"country_code\"}, inplace = True)\n",
    "        \n",
    "    def prepare_network_features(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(self.data_dir + \"ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        #trade_data = trade_data[trade_data[\"product_category\"] == \"1\"]\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, on = \"importer_code\")\n",
    "        imports_table_grouped[\"import_fraction\"] = imports_table_grouped[\"import_value\"]\\\n",
    "                        /imports_table_grouped[\"import_total\"]*100\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        \n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "        \n",
    "        self.G = G\n",
    "        self.centrality_overall= nx.eigenvector_centrality(G, max_iter= 10000) \n",
    "        self.centrality_overall = pd.DataFrame(list(map(list, self.centrality_overall.items())), \n",
    "                                               columns = [\"country_code\", \"centrality_overall\"])\n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", [\"import_fraction\"])\n",
    "        weighted_centrality = nx.eigenvector_centrality(G, weight = \"import_fraction\", max_iter= 10000) \n",
    "        weighted_centrality  = pd.DataFrame(list(map(list, weighted_centrality.items())), \n",
    "                                               columns = [\"country_code\", \"weighted_centrality\"])\n",
    "        self.centrality_overall = pd.merge(self.centrality_overall, weighted_centrality, on = \"country_code\")\n",
    "        \n",
    "                               \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY IN PRODUCT CATEGORIES\n",
    "\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = imports_table.groupby([\"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = products_grouped.rename(columns = {\"import_value\": \"import_product_total\"})\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"importer_code\", \"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(products_grouped, self.export_types, on = \"product_category\")\n",
    "        \n",
    "        self.export_types[\"product_export_fraction\"] = self.export_types[\"import_value\"]\\\n",
    "                                                    /self.export_types[\"import_product_total\"]*100\n",
    "        \n",
    "        list_products = list(set(self.export_types[\"product_category\"]))\n",
    "        \n",
    "        i = 0 \n",
    "        for product in list_products:\n",
    "            \n",
    "            temp = self.export_types[self.export_types[\"product_category\"] == product].copy()\n",
    "            \n",
    "            G_w=nx.from_pandas_edgelist(temp, \n",
    "                \"exporter_code\", \"importer_code\", [\"product_export_fraction\"], create_using = nx.DiGraph())\n",
    "            centrality_product_w = nx.eigenvector_centrality(G_w, weight = \"product_export_fraction\", \n",
    "                                                           max_iter= 10000)\n",
    "\n",
    "            G=nx.from_pandas_edgelist(temp,\"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "            centrality_product = nx.eigenvector_centrality(G,max_iter= 10000)\n",
    "\n",
    "            if(i == 0):\n",
    "                self.centrality_product = pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product])\n",
    "                \n",
    "\n",
    "            else: \n",
    "                self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "                \n",
    "            self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product_w.items())), \n",
    "                                               columns = [\"country_code\", \"prod_w_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "            \n",
    "            i = i+1         \n",
    "    \n",
    "    def combine_normalize_features(self):\n",
    "        \n",
    "        self.combined_features = pd.merge(self.features, self.centrality_overall,on = \"country_code\")\n",
    "        self.combined_features = pd.merge(self.combined_features, self.centrality_product,on = \"country_code\")\n",
    "        #step eliminates NA and nodes that are not in graph, since they will have NA for graph features\n",
    "        self.combined_features = self.combined_features.drop(columns = [\"index\"])\n",
    "        #filter both trade data and features data to same subset of countries\n",
    "        self.combined_features = self.combined_features[\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.importer_code)|\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.exporter_code)]\n",
    "        self.trade_data = self.trade_data[\\\n",
    "                          self.trade_data.importer_code.isin(self.combined_features.country_code)&\\\n",
    "                          self.trade_data.exporter_code.isin(self.combined_features.country_code)]\n",
    "        \n",
    "        features_to_norm = list(self.combined_features.columns.copy())\n",
    "        non_norm = [\"country_code\", \"node_numbers\"]\n",
    "        cols_insufficient_data = list(self.combined_features.loc[:, self.combined_features.nunique() < 2].columns.copy())\n",
    "        non_norm.extend(cols_insufficient_data)\n",
    " \n",
    "        features_to_norm = [x for x in features_to_norm if x not in non_norm]\n",
    "        scaler = StandardScaler()\n",
    "        #we preserve NAs in the scaling\n",
    "        self.combined_features[features_to_norm] = scaler.fit_transform(self.combined_features[features_to_norm])\n",
    "        self.combined_features.fillna(0, inplace = True) #we fill NA after scaling \n",
    "        #check that feature has at least 20% coverage in a given year -- otherwise set to NA\n",
    "        for feature in self.feature_list:\n",
    "            coverage = len(self.combined_features[self.combined_features[feature] != 0])/len(self.combined_features)\n",
    "            if(coverage <= 0.20): self.combined_features[feature] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeNetwork:\n",
    "    \"\"\"\n",
    "    We define a class which computes the MST trade network for a given year \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"data\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_features(self, filter_gdp = True):\n",
    "        \n",
    "        ###IMPORT GDP###\n",
    "        #prepare GDP as a set of features \n",
    "        with open('data/all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "\n",
    "        self.gdp = features_dict['NY.GDP.MKTP.CD']\n",
    "        scaler = StandardScaler()\n",
    "        self.gdp[[\"prev_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year-1)]]))\n",
    "        self.gdp[[\"current_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year)]]))\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp[\"country_code\"] = self.gdp[\"economy\"]\n",
    "        self.gdp = self.gdp[[\"country_code\", \"prev_gdp\", \"current_gdp\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP GROWTH###\n",
    "        #prepare GDP growth\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth['YR'+str(self.year)] \n",
    "        self.gdp_growth[\"future_gdp_growth\"] = self.gdp_growth['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_growth[\"country_code\"] = self.gdp_growth[\"economy\"]\n",
    "        self.gdp_growth = self.gdp_growth[[\"country_code\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\", \"future_gdp_growth\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA###\n",
    "        self.gdp_per_capita = features_dict['NY.GDP.PCAP.CD']\n",
    "        self.gdp_per_capita[\"prev_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita[\"current_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year)]\n",
    "        self.gdp_per_capita[\"future_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita[\"country_code\"] = self.gdp_per_capita[\"economy\"]\n",
    "        self.gdp_per_capita = self.gdp_per_capita[[\"country_code\", \"prev_gdp_per_cap\",\n",
    "                                \"current_gdp_per_cap\", \"future_gdp_per_cap\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA GROWTH###\n",
    "        self.gdp_per_capita_growth = features_dict['NY.GDP.PCAP.KD.ZG']\n",
    "        self.gdp_per_capita_growth[\"prev_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita_growth[\"current_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year)]\n",
    "        self.gdp_per_capita_growth[\"future_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year+1)]\n",
    "        \n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita_growth[\"country_code\"] = self.gdp_per_capita_growth[\"economy\"]\n",
    "        self.gdp_per_capita_growth = self.gdp_per_capita_growth[[\"country_code\", \"prev_gdp_per_cap_growth\",\n",
    "                                \"current_gdp_per_cap_growth\", \"future_gdp_per_cap_growth\"]].dropna()\n",
    "        \n",
    "        ###MERGE ALL DATA FEATURES###\n",
    "        self.features = pd.merge(self.gdp_growth, self.gdp, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita_growth, on = \"country_code\").dropna()\n",
    "\n",
    "    def prepare_network(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(\"data/ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"/country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        #trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        #trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(self.export_types, exporter_total, on = \"exporter_code\")\n",
    "        #multiply by 100 to allow weights to scale better in GNN\n",
    "        self.export_types[\"category_fraction\"] = self.export_types.import_value/self.export_types.export_total*10\n",
    "        ss = StandardScaler()\n",
    "        columns = list(set(self.export_types[\"product_category\"]))\n",
    "        self.export_types = self.export_types[[\"exporter_code\", \"product_category\", \"category_fraction\"]]\\\n",
    "        .pivot(index = [\"exporter_code\"], columns = [\"product_category\"], values = \"category_fraction\")\\\n",
    "        .reset_index().fillna(0)\n",
    "        #rename columns\n",
    "        rename_columns = []\n",
    "        for col in self.export_types.columns:\n",
    "            if(col == \"exporter_code\"):\n",
    "                rename_columns.append(col)\n",
    "            else:\n",
    "                rename_columns.append(\"resource_\" + col)\n",
    "        self.export_types.columns = rename_columns\n",
    "        self.export_types = self.export_types.rename(columns = {\"exporter_code\": \"country_code\"})\n",
    "        self.features = pd.merge(self.features, self.export_types, \n",
    "                                on = \"country_code\", how = \"left\")\n",
    "        \n",
    "        #look at fraction of goods traded with each counterparty\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, exporter_total, how = \"left\")\n",
    "        imports_table_grouped[\"export_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"export_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"export_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"export_percent\"]]))\n",
    "        imports_table_grouped[\"export_percent_feature\"] = imports_table_grouped[\"export_percent_feature\"] + abs(min(imports_table_grouped[\"export_percent_feature\"]))\n",
    "        \n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, how = \"left\")\n",
    "        imports_table_grouped[\"import_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"import_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"import_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"import_percent\"]]))\n",
    "        imports_table_grouped[\"import_percent_feature\"] = imports_table_grouped[\"import_percent_feature\"] + abs(min(imports_table_grouped[\"import_percent_feature\"]))\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "\n",
    "    def graph_create(self, exporter = True,\n",
    "            node_features = ['prev_gdp_growth', 'current_gdp_growth','prev_gdp','current_gdp'],\n",
    "            node_labels = 'future_gdp_growth'):\n",
    "        \n",
    "        if(exporter):\n",
    "            center_node = \"exporter_code\"\n",
    "            neighbors = \"importer_code\"\n",
    "            edge_features = 'export_percent'\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        # list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "        #                 list(self.trade_data [\"exporter_code\"])))\n",
    "\n",
    "        list_active_countries = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']\n",
    "\n",
    "        # Create a new DataFrame with the list of country codes\n",
    "        df_new = pd.DataFrame(list_active_countries, columns=['country_code'])\n",
    "        self.features = pd.merge(df_new, self.features, on='country_code', how='left')\n",
    "        self.features = self.features.fillna(0)\n",
    "\n",
    "        # self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        # self.features.fillna(0, inplace = True)\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "\n",
    "        #create lookup dictionary making node number / node features combatible with ordering of nodes\n",
    "        #in our edge table\n",
    "\n",
    "        self.node_lookup1 = self.features.set_index('node_numbers').to_dict()['country_code']\n",
    "        self.node_lookup2 = self.features.set_index('country_code').to_dict()['node_numbers']\n",
    "        \n",
    "        #get individual country's features\n",
    "        self.regression_table = pd.merge(self.features, self.trade_data,\n",
    "                        left_on = \"country_code\",\n",
    "                        right_on = center_node, how = 'right')\n",
    "        #get features for trade partners\n",
    "        self.regression_table = pd.merge(self.features, self.regression_table,\n",
    "                                        left_on = \"country_code\",\n",
    "                                        right_on = neighbors, how = \"right\",\n",
    "                                        suffixes = (\"_neighbors\", \"\"))\n",
    "        \n",
    "        self.trade_data = self.trade_data[self.trade_data[neighbors].isin(self.node_lookup2)]\n",
    "        self.trade_data = self.trade_data[self.trade_data[center_node].isin(self.node_lookup2)]\n",
    "\n",
    "        self.regression_table[\"source\"] = self.trade_data[neighbors].apply(lambda x: self.node_lookup2[x])\n",
    "        self.regression_table[\"target\"] = self.trade_data[center_node].apply(lambda x: self.node_lookup2[x])    \n",
    "\n",
    "        self.regression_table = self.regression_table.dropna()\n",
    "        #filter only to relevant columns\n",
    "        self.relevant_columns = [\"source\", \"target\"]\n",
    "        self.relevant_columns.extend(node_features)\n",
    "        self.relevant_columns.append(node_labels)\n",
    "        self.graph_table = self.regression_table[self.relevant_columns]\n",
    "        \n",
    "        if(self.graph_table.isnull().values.any()): print(\"edges contain null / inf values\")\n",
    "\n",
    "        self.node_attributes = torch.tensor(np.array(self.features[node_features]))\\\n",
    "        .to(torch.float)\n",
    "        self.source_nodes = list(self.graph_table[\"source\"])\n",
    "        self.target_nodes = list(self.graph_table[\"target\"])\n",
    "\n",
    "        self.edge_attributes = list(self.trade_data[edge_features])\n",
    "        \n",
    "        self.pyg_graph = data.Data(x = self.node_attributes,\n",
    "                                   edge_index = torch.tensor([self.source_nodes, self.target_nodes]),\n",
    "                                   edge_attr = torch.tensor(self.edge_attributes).to(torch.float),\n",
    "                                   y = torch.tensor(list(self.features[node_labels])).to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/57 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/57 [00:05<04:47,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1963\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2/57 [00:10<04:47,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1964\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/57 [00:15<04:50,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1965\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/57 [00:21<04:52,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1966\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/57 [00:27<05:00,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1967\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6/57 [00:34<05:01,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1968\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/57 [00:40<05:03,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1969\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8/57 [00:47<05:09,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1970\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 9/57 [00:54<05:14,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1971\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 10/57 [01:01<05:19,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1972\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 11/57 [01:09<05:32,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1973\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 12/57 [01:18<05:42,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1974\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 13/57 [01:27<05:50,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1975\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 14/57 [01:36<05:56,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 15/57 [01:40<04:59,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1977\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 16/57 [01:45<04:23,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1978\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 17/57 [01:51<04:16,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1979\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 18/57 [01:58<04:09,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1980\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 19/57 [02:04<04:06,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1981\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 20/57 [02:11<04:04,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1982\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 21/57 [02:18<04:03,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1983\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 22/57 [02:25<03:59,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1984\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 23/57 [02:33<03:58,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1985\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 24/57 [02:41<04:00,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1986\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 25/57 [02:48<03:51,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1987\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 26/57 [02:56<03:52,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1988\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 27/57 [03:05<03:57,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1989\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 28/57 [03:14<03:57,  8.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1990\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 29/57 [03:23<03:57,  8.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1991\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 30/57 [03:32<03:53,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1992\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 31/57 [03:43<04:06,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1993\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 32/57 [03:54<04:06,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1994\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 33/57 [04:06<04:10, 10.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1995\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 34/57 [04:17<04:07, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1996\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 35/57 [04:30<04:06, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1997\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 36/57 [04:42<04:02, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1998\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 37/57 [04:55<04:00, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "1999\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 38/57 [05:09<03:59, 12.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 39/57 [05:24<03:57, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2001\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 40/57 [05:39<03:55, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2002\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 41/57 [05:53<03:44, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2003\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 42/57 [06:09<03:37, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2004\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 43/57 [06:26<03:31, 15.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2005\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 44/57 [06:46<03:35, 16.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2006\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 45/57 [07:04<03:24, 17.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2007\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 46/57 [07:22<03:13, 17.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2008\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 47/57 [07:42<03:00, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2009\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 48/57 [08:00<02:43, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2010\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 49/57 [08:18<02:25, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2011\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 50/57 [08:36<02:06, 18.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2012\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 51/57 [08:55<01:49, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2013\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 52/57 [09:14<01:32, 18.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2014\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 53/57 [09:33<01:14, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2015\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 54/57 [09:52<00:56, 18.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2016\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 55/57 [10:12<00:38, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2017\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 56/57 [10:31<00:19, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2018\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [10:51<00:00, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "years = range(1962,2019)\n",
    "\n",
    "train_years = [2005, 1969, 2002, 1997, 1993, 1982, 2001, 2000, 1962, 1985, 1978, 2016, 1986, 1987, 1989, 1971, 2013, 1996, 1995, 1967, 2017, 1974, 1990, 1977, 1980, 2014, 1965, 1984, 2006, 1973, 1968, 1981, 1970, 1991]\n",
    "val_years = [1975, 1983, 2009, 1966, 1999, 1988, 2007, 1979, 1972, 2015, 2003]\n",
    "test_years = [1963, 1964, 1976, 1992, 1994, 1998, 2004, 2008, 2010, 2011, 2012, 2018]\n",
    "\n",
    "train_graphs = []\n",
    "val_graphs = []\n",
    "test_graphs = []\n",
    "i = 0\n",
    "\n",
    "for year in tqdm(years):\n",
    "    print(str(year), end='\\r')\n",
    "    \n",
    "    trade = TradeNetwork(year = year)\n",
    "    trade.prepare_features()\n",
    "    trade.prepare_network()\n",
    "    trade.graph_create(node_features = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth',\n",
    "    'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "       'resource_8', 'resource_9'],\n",
    "        node_labels = 'future_gdp_per_cap_growth')\n",
    "    \n",
    "    if(year in val_years):\n",
    "        val_graphs.append(trade.pyg_graph)\n",
    "    elif(year in test_years):\n",
    "        test_graphs.append(trade.pyg_graph)\n",
    "    else: \n",
    "        train_graphs.append(trade.pyg_graph)\n",
    "        \n",
    "    trade.features[\"year\"] = year\n",
    "    \n",
    "    if(i == 0):\n",
    "        trade_df = trade.features\n",
    "    else: \n",
    "        trade_df = pd.concat([trade_df, trade.features])\n",
    "        \n",
    "    i = i+1\n",
    "    print(trade.node_attributes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pygcn/train_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(train_graphs, f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(val_graphs, f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(test_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_dicts/random_features_dict.pkl\", \"rb\") as f:\n",
    "    feat_dict_random = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(years, graphs, feat_dict):\n",
    "\n",
    "    zeros = torch.zeros(434)\n",
    "\n",
    "    for i in range(len(years)):\n",
    "        new_x = torch.empty(0, 434)\n",
    "        year = years[i]\n",
    "\n",
    "        feat_dict_year = feat_dict[year].combined_features\n",
    "\n",
    "        for j, country in enumerate(all_nodes):\n",
    "            if j == 0:\n",
    "                new_x = torch.stack([zeros])\n",
    "\n",
    "            elif country in feat_dict_year[\"country_code\"].values:\n",
    "                tensor_before = graphs[i].x[j]\n",
    "                country_row = feat_dict_year[feat_dict_year[\"country_code\"] == country]\n",
    "                country_row = country_row.drop(columns = [\"prev_gdp_growth\", \"country_code\", \"current_gdp_growth\"])\n",
    "                row_values = country_row.values.tolist()\n",
    "                row_tensor = torch.tensor(row_values)[0]\n",
    "                combined_values = torch.cat((tensor_before, row_tensor))\n",
    "\n",
    "                new_x = torch.cat((new_x, combined_values.unsqueeze(0)), dim=0)\n",
    "\n",
    "            else:\n",
    "                new_x = torch.cat((new_x, zeros.unsqueeze(0)), dim=0)\n",
    "\n",
    "        graphs[i].x = new_x\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = add_features(train_years, train_graphs, feat_dict_random)\n",
    "val_graphs = add_features(val_years, val_graphs, feat_dict_random)\n",
    "test_graphs = add_features(test_years, test_graphs, feat_dict_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test_graphs, batch_size=32)\n",
    "train_loader = DataLoader(train_graphs, batch_size=32)\n",
    "val_loader = DataLoader(val_graphs, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crisis_years(year_pairs, crisis_years):\n",
    "    result = []\n",
    "    for pair in year_pairs:\n",
    "        start, end = pair\n",
    "        # Check if any crisis year is between the pair or equals the later year\n",
    "        if any(start < year <= end for year in crisis_years):\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1983, 1982, 2008, 2002, 2016, 1967, 1962, 1989, 2012, 1963, 1993, 1986, 1996,1978]\n",
    "\n",
    "def get_year_pairs(year_range):\n",
    "    return [(year1, year2) for year1 in year_range for year2 in year_range if year2 >= year1]\n",
    "\n",
    "def get_loader_pairs(dataset):\n",
    "    return [(dataset[i], dataset[j]) for i in range(len(dataset)) for j in range(len(dataset)) if j >= i]\n",
    "\n",
    "train_pairs = get_year_pairs(train_years)\n",
    "val_pairs = get_year_pairs(val_years)\n",
    "\n",
    "train_y = check_crisis_years(train_pairs, crisis_years)\n",
    "val_y = check_crisis_years(val_pairs, crisis_years)\n",
    "\n",
    "train_loader_pairs = get_loader_pairs(train_loader.dataset)\n",
    "val_loader_pairs = get_loader_pairs(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_y = torch.tensor(np.array(train_y))\n",
    "val_torch_y = torch.tensor(np.array(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_neg = torch.eq(train_torch_y, 0).sum().item()\n",
    "N_pos = torch.eq(train_torch_y, 1).sum().item()\n",
    "\n",
    "# Calculate the weight for the positive class\n",
    "weight_for_positive_class = N_neg / N_pos\n",
    "pos_weight = torch.tensor([weight_for_positive_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
