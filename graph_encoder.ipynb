{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import datetime as datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as sm\n",
    "import dgl.function as fn\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "#imports for graph creation\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#imports for graph learning\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.data as data\n",
    "import torch_geometric.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_sitc_codes():\n",
    "    # URL of the JSON file\n",
    "    url = 'https://comtradeapi.un.org/files/v1/app/reference/S4.json'\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL and fetch the data\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check that the request was successful\n",
    "        \n",
    "        # Load the JSON data\n",
    "        data = response.json()\n",
    "\n",
    "        # Since the JSON data might be nested, use json_normalize with appropriate arguments\n",
    "        if isinstance(data, list):\n",
    "            # If the top level is a list\n",
    "            df = pd.json_normalize(data)\n",
    "        else:\n",
    "            # If the top level is a dictionary\n",
    "            # Identify the key that holds the main data (adjust the path as necessary)\n",
    "            main_data_key = 'results'  # Adjust this based on the actual structure\n",
    "            df = pd.json_normalize(data[main_data_key])\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing JSON structure: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeNetwork:\n",
    "    \"\"\"\n",
    "    We define a class which computes the MST trade network for a given year \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"data\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_features(self, filter_gdp = True):\n",
    "        \n",
    "        ###IMPORT GDP###\n",
    "        #prepare GDP as a set of features \n",
    "        with open('data/all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "\n",
    "        self.gdp = features_dict['NY.GDP.MKTP.CD']\n",
    "        scaler = StandardScaler()\n",
    "        self.gdp[[\"prev_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year-1)]]))\n",
    "        self.gdp[[\"current_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year)]]))\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp[\"country_code\"] = self.gdp[\"economy\"]\n",
    "        self.gdp = self.gdp[[\"country_code\", \"prev_gdp\", \"current_gdp\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP GROWTH###\n",
    "        #prepare GDP growth\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth['YR'+str(self.year)] \n",
    "        self.gdp_growth[\"future_gdp_growth\"] = self.gdp_growth['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_growth[\"country_code\"] = self.gdp_growth[\"economy\"]\n",
    "        self.gdp_growth = self.gdp_growth[[\"country_code\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\", \"future_gdp_growth\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA###\n",
    "        self.gdp_per_capita = features_dict['NY.GDP.PCAP.CD']\n",
    "        self.gdp_per_capita[\"prev_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita[\"current_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year)]\n",
    "        self.gdp_per_capita[\"future_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita[\"country_code\"] = self.gdp_per_capita[\"economy\"]\n",
    "        self.gdp_per_capita = self.gdp_per_capita[[\"country_code\", \"prev_gdp_per_cap\",\n",
    "                                \"current_gdp_per_cap\", \"future_gdp_per_cap\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA GROWTH###\n",
    "        self.gdp_per_capita_growth = features_dict['NY.GDP.PCAP.KD.ZG']\n",
    "        self.gdp_per_capita_growth[\"prev_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita_growth[\"current_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year)]\n",
    "        self.gdp_per_capita_growth[\"future_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year+1)]\n",
    "        \n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita_growth[\"country_code\"] = self.gdp_per_capita_growth[\"economy\"]\n",
    "        self.gdp_per_capita_growth = self.gdp_per_capita_growth[[\"country_code\", \"prev_gdp_per_cap_growth\",\n",
    "                                \"current_gdp_per_cap_growth\", \"future_gdp_per_cap_growth\"]].dropna()\n",
    "        \n",
    "        ###MERGE ALL DATA FEATURES###\n",
    "        self.features = pd.merge(self.gdp_growth, self.gdp, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita_growth, on = \"country_code\").dropna()\n",
    "\n",
    "    def prepare_network(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(\"data/ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"/country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        #trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        #trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(self.export_types, exporter_total, on = \"exporter_code\")\n",
    "        #multiply by 100 to allow weights to scale better in GNN\n",
    "        self.export_types[\"category_fraction\"] = self.export_types.import_value/self.export_types.export_total*10\n",
    "        ss = StandardScaler()\n",
    "        columns = list(set(self.export_types[\"product_category\"]))\n",
    "        self.export_types = self.export_types[[\"exporter_code\", \"product_category\", \"category_fraction\"]]\\\n",
    "        .pivot(index = [\"exporter_code\"], columns = [\"product_category\"], values = \"category_fraction\")\\\n",
    "        .reset_index().fillna(0)\n",
    "        #rename columns\n",
    "        rename_columns = []\n",
    "        for col in self.export_types.columns:\n",
    "            if(col == \"exporter_code\"):\n",
    "                rename_columns.append(col)\n",
    "            else:\n",
    "                rename_columns.append(\"resource_\" + col)\n",
    "        self.export_types.columns = rename_columns\n",
    "        self.export_types = self.export_types.rename(columns = {\"exporter_code\": \"country_code\"})\n",
    "        self.features = pd.merge(self.features, self.export_types, \n",
    "                                on = \"country_code\", how = \"left\")\n",
    "        \n",
    "        #look at fraction of goods traded with each counterparty\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, exporter_total, how = \"left\")\n",
    "        imports_table_grouped[\"export_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"export_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"export_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"export_percent\"]]))\n",
    "        imports_table_grouped[\"export_percent_feature\"] = imports_table_grouped[\"export_percent_feature\"] + abs(min(imports_table_grouped[\"export_percent_feature\"]))\n",
    "        \n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, how = \"left\")\n",
    "        imports_table_grouped[\"import_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"import_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"import_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"import_percent\"]]))\n",
    "        imports_table_grouped[\"import_percent_feature\"] = imports_table_grouped[\"import_percent_feature\"] + abs(min(imports_table_grouped[\"import_percent_feature\"]))\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "\n",
    "    def graph_create(self, exporter = True,\n",
    "            node_features = ['prev_gdp_growth', 'current_gdp_growth','prev_gdp','current_gdp'],\n",
    "            node_labels = 'future_gdp_growth'):\n",
    "        \n",
    "        if(exporter):\n",
    "            center_node = \"exporter_code\"\n",
    "            neighbors = \"importer_code\"\n",
    "            edge_features = 'export_percent'\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        \n",
    "        \n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features.fillna(0, inplace = True)\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        #create lookup dictionary making node number / node features combatible with ordering of nodes\n",
    "        #in our edge table\n",
    "\n",
    "        self.node_lookup1 = self.features.set_index('node_numbers').to_dict()['country_code']\n",
    "        self.node_lookup2 = self.features.set_index('country_code').to_dict()['node_numbers']\n",
    "        \n",
    "        #get individual country's features\n",
    "        self.regression_table = pd.merge(self.features, self.trade_data,\n",
    "                        left_on = \"country_code\",\n",
    "                        right_on = center_node, how = 'right')\n",
    "        #get features for trade partners\n",
    "        self.regression_table = pd.merge(self.features, self.regression_table,\n",
    "                                        left_on = \"country_code\",\n",
    "                                        right_on = neighbors, how = \"right\",\n",
    "                                        suffixes = (\"_neighbors\", \"\"))\n",
    "        \n",
    "        self.trade_data = self.trade_data[self.trade_data[neighbors].isin(self.node_lookup2)]\n",
    "        self.trade_data = self.trade_data[self.trade_data[center_node].isin(self.node_lookup2)]\n",
    "\n",
    "        self.regression_table[\"source\"] = self.trade_data[neighbors].apply(lambda x: self.node_lookup2[x])\n",
    "        self.regression_table[\"target\"] = self.trade_data[center_node].apply(lambda x: self.node_lookup2[x])    \n",
    "\n",
    "        self.regression_table = self.regression_table.dropna()\n",
    "        #filter only to relevant columns\n",
    "        self.relevant_columns = [\"source\", \"target\"]\n",
    "        self.relevant_columns.extend(node_features)\n",
    "        self.relevant_columns.append(node_labels)\n",
    "        self.graph_table = self.regression_table[self.relevant_columns]\n",
    "        \n",
    "        if(self.graph_table.isnull().values.any()): print(\"edges contain null / inf values\")\n",
    "\n",
    "        self.node_attributes = torch.tensor(np.array(self.features[node_features]))\\\n",
    "        .to(torch.float)\n",
    "        self.source_nodes = list(self.graph_table[\"source\"])\n",
    "        self.target_nodes = list(self.graph_table[\"target\"])\n",
    "        self.edge_attributes = list(self.trade_data[edge_features])\n",
    "\n",
    "        self.pyg_graph = data.Data(x = self.node_attributes,\n",
    "                                   edge_index = torch.tensor([self.source_nodes, self.target_nodes]),\n",
    "                                   edge_attr = torch.tensor(self.edge_attributes).to(torch.float),\n",
    "                                   y = torch.tensor(list(self.features[node_labels])).to(torch.float))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train years: [2005, 1969, 2002, 1997, 1993, 1982, 2001, 2000, 1962, 1985, 1978, 2016, 1986, 1987, 1989, 1971, 2013, 1996, 1995, 1967, 2017, 1974, 1990, 1977, 1980, 2014, 1965, 1984, 2006, 1973, 1968, 1981, 1970, 1991, 2019]\n",
      "Validation years: [1975, 1983, 2009, 1966, 1999, 1988, 2007, 1979, 1972, 2015, 2003]\n",
      "Test years: [1963, 1964, 1976, 1992, 1994, 1998, 2004, 2008, 2010, 2011, 2012, 2018, 2020]\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "# years = list(range(1962,2021))\n",
    "\n",
    "# # Determine the sizes of the train, validation, and test sets\n",
    "# train_size = int(len(years) * 0.6)\n",
    "# val_size = int(len(years) * 0.20)\n",
    "\n",
    "# # Get a random subset for the train set\n",
    "# train_years = random.sample(years, train_size)\n",
    "\n",
    "# # Remove the years in the train set from the list of years\n",
    "# years = [year for year in years if year not in train_years]\n",
    "\n",
    "# # Get a random subset for the validation set\n",
    "# val_years = random.sample(years, val_size)\n",
    "\n",
    "# # Remove the years in the validation set from the list of years\n",
    "# test_years = [year for year in years if year not in val_years]\n",
    "\n",
    "# print(\"Train years:\", train_years)\n",
    "# print(\"Validation years:\", val_years)\n",
    "# print(\"Test years:\", test_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = [2005, 1969, 2002, 1997, 1993, 1982, 2001, 2000, 1962, 1985, 1978, 2016, 1986, 1987, 1989, 1971, 2013, 1996, 1995, 1967, 2017, 1974, 1990, 1977, 1980, 2014, 1965, 1984, 2006, 1973, 1968, 1981, 1970, 1991, 2019]\n",
    "val_years = [1975, 1983, 2009, 1966, 1999, 1988, 2007, 1979, 1972, 2015, 2003]\n",
    "test_years = [1963, 1964, 1976, 1992, 1994, 1998, 2004, 2008, 2010, 2011, 2012, 2018, 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/59 [00:07<06:51,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1963\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/59 [00:13<06:24,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1964\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/59 [00:20<06:22,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1965\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/59 [00:29<06:52,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1966\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/59 [00:35<06:17,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1967\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6/59 [00:41<05:55,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1968\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/59 [00:47<05:40,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1969\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 8/59 [00:54<05:40,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1970\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/59 [01:02<05:57,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1971\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10/59 [01:10<05:55,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1972\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 11/59 [01:18<06:00,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1973\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12/59 [01:26<06:09,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1974\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13/59 [01:35<06:16,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1975\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 14/59 [01:45<06:23,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15/59 [01:49<05:18,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1977\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 16/59 [01:54<04:41,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1978\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 17/59 [02:00<04:30,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1979\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 18/59 [02:07<04:25,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1980\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 19/59 [02:13<04:22,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1981\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 20/59 [02:21<04:23,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1982\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 21/59 [02:28<04:20,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1983\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22/59 [02:35<04:20,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1984\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 23/59 [02:43<04:18,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1985\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 24/59 [02:50<04:16,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1986\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 25/59 [02:58<04:12,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1987\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 26/59 [03:07<04:21,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1988\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 27/59 [03:16<04:19,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1989\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 28/59 [03:25<04:20,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1990\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 29/59 [03:35<04:27,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1991\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 30/59 [03:45<04:26,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1992\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 31/59 [03:56<04:32,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1993\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 32/59 [04:07<04:36, 10.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1994\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 33/59 [04:19<04:37, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1995\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 34/59 [04:32<04:44, 11.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1996\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 35/59 [04:47<04:58, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1997\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 36/59 [05:02<05:03, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1998\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 37/59 [05:17<05:02, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "1999\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 38/59 [05:33<05:04, 14.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 39/59 [05:49<04:57, 14.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2001\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 40/59 [06:05<04:50, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2002\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 41/59 [06:20<04:34, 15.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2003\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 42/59 [06:35<04:19, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2004\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 43/59 [06:51<04:08, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2005\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 44/59 [07:10<04:04, 16.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2006\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 45/59 [07:28<03:56, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2007\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 46/59 [07:46<03:45, 17.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2008\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 47/59 [08:06<03:35, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2009\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 48/59 [08:24<03:19, 18.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2010\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 49/59 [08:42<03:01, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2011\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 50/59 [09:01<02:44, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2012\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 51/59 [09:20<02:29, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2013\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 52/59 [09:40<02:12, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2014\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 53/59 [09:59<01:54, 19.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2015\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 54/59 [10:19<01:35, 19.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2016\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 55/59 [10:38<01:16, 19.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2017\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 56/59 [10:57<00:57, 19.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2018\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 57/59 [11:18<00:39, 19.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2019\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 58/59 [11:39<00:19, 19.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [11:58<00:00, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_graphs = []\n",
    "val_graphs = []\n",
    "test_graphs = []\n",
    "i = 0\n",
    "\n",
    "all_nodes = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']\n",
    "\n",
    "years = list(range(1962,2021))\n",
    "for year in tqdm(years):\n",
    "    print(str(year), end='\\r')\n",
    "    \n",
    "    trade = TradeNetwork(year = year)\n",
    "    trade.prepare_features()\n",
    "    trade.prepare_network()\n",
    "    trade.graph_create(node_features = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth',\n",
    "    'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "       'resource_8', 'resource_9'],\n",
    "        node_labels = 'future_gdp_per_cap_growth')\n",
    "    \n",
    "    current_nodes = list(trade.node_lookup2.keys())\n",
    "    num_features = trade.pyg_graph.x.size(1)\n",
    "    zeros = torch.zeros(1, num_features)\n",
    "\n",
    "    for i in range(len(all_nodes)):\n",
    "        if i >= len(current_nodes):\n",
    "            current_nodes.insert(i, all_nodes[i])\n",
    "            tensor_before = trade.pyg_graph.x\n",
    "            new_tensor = torch.cat([tensor_before, zeros], dim=0)\n",
    "            trade.pyg_graph.x = new_tensor\n",
    "\n",
    "            node_attr_before = trade.node_attributes\n",
    "            trade.node_attributes = torch.cat([node_attr_before, zeros], dim=0)\n",
    "\n",
    "        elif(all_nodes[i]!=current_nodes[i]):\n",
    "            current_nodes.insert(i, all_nodes[i])\n",
    "            if i==0:\n",
    "                tensor_after = trade.pyg_graph.x[i:]\n",
    "                new_tensor = torch.cat([zeros, tensor_after], dim=0)\n",
    "                trade.pyg_graph.x = new_tensor\n",
    "\n",
    "                node_attr_after = trade.node_attributes[i:]\n",
    "                trade.node_attributes = torch.cat([zeros, node_attr_after], dim=0)\n",
    "            else:\n",
    "                tensor_before = trade.pyg_graph.x[:i]\n",
    "                tensor_after = trade.pyg_graph.x[i:]\n",
    "                new_tensor = torch.cat([tensor_before, zeros, tensor_after], dim=0)\n",
    "                trade.pyg_graph.x = new_tensor\n",
    "\n",
    "                node_attr_before = trade.node_attributes[:i]\n",
    "                node_attr_after = trade.node_attributes[i:]\n",
    "                trade.node_attributes = torch.cat([node_attr_before, zeros, node_attr_after], dim=0)\n",
    "\n",
    "    for i in range(len(current_nodes) - 1, -1, -1):\n",
    "        if current_nodes[i] not in all_nodes:\n",
    "            node_to_remove = current_nodes.pop(i)\n",
    "\n",
    "            tensor_before = trade.pyg_graph.x[:i]\n",
    "            tensor_after = trade.pyg_graph.x[i+1:]\n",
    "            new_tensor = torch.cat([tensor_before, tensor_after], dim=0)\n",
    "            trade.pyg_graph.x = new_tensor\n",
    "\n",
    "            node_attr_before = trade.node_attributes[:i]\n",
    "            node_attr_after = trade.node_attributes[i+1:]\n",
    "            trade.node_attributes = torch.cat([node_attr_before, node_attr_after], dim=0)\n",
    "\n",
    "    if len(current_nodes) > len(all_nodes):\n",
    "        num_diff = len(current_nodes) - len(all_nodes)\n",
    "        trade.pyg_graph.x = trade.pyg_graph.x[:-num_diff]\n",
    "        trade.node_attributes = trade.node_attributes[:-num_diff]\n",
    "        current_nodes = current_nodes[:-num_diff]\n",
    "    \n",
    "    if current_nodes != all_nodes:\n",
    "        print(\"Nodes not equal\")\n",
    "        break\n",
    "\n",
    "    if(year in val_years):\n",
    "        val_graphs.append(trade.pyg_graph)\n",
    "    elif(year in test_years):\n",
    "        test_graphs.append(trade.pyg_graph)\n",
    "    else: \n",
    "        train_graphs.append(trade.pyg_graph)\n",
    "        \n",
    "    trade.features[\"year\"] = year\n",
    "    \n",
    "    if(i == 0):\n",
    "        trade_df = trade.features\n",
    "    else: \n",
    "        trade_df = pd.concat([trade_df, trade.features])\n",
    "        \n",
    "    i = i+1\n",
    "    print(trade.node_attributes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pygcn/train_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(train_graphs, f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(val_graphs, f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"wb\") as f:\n",
    "    pkl.dump(test_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pygcn/train_graphs.pickle\", \"rb\") as f:\n",
    "    train_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"rb\") as f:  \n",
    "    val_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"rb\") as f:         \n",
    "    test_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test_graphs, batch_size=32)\n",
    "train_loader = DataLoader(train_graphs, batch_size=32)\n",
    "val_loader = DataLoader(val_graphs, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sGNN with GCN Encoder and 3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crisis_years(year_pairs, crisis_years):\n",
    "    result = []\n",
    "    for pair in year_pairs:\n",
    "        start, end = pair\n",
    "        # Check if any crisis year is between the pair or equals the later year\n",
    "        if any(start < year <= end for year in crisis_years):\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1983, 1982, 2008, 2002, 2016, 1967, 1962, 1989, 2012, 1963, 1993, 1986, 1996,1978]\n",
    "\n",
    "def get_year_pairs(year_range):\n",
    "    return [(year1, year2) for year1 in year_range for year2 in year_range if year2 >= year1]\n",
    "\n",
    "def get_loader_pairs(dataset):\n",
    "    return [(dataset[i], dataset[j]) for i in range(len(dataset)) for j in range(len(dataset)) if j >= i]\n",
    "\n",
    "train_pairs = get_year_pairs(train_years)\n",
    "val_pairs = get_year_pairs(val_years)\n",
    "\n",
    "train_y = check_crisis_years(train_pairs, crisis_years)\n",
    "val_y = check_crisis_years(val_pairs, crisis_years)\n",
    "\n",
    "train_loader_pairs = get_loader_pairs(train_loader.dataset)\n",
    "val_loader_pairs = get_loader_pairs(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_y = torch.tensor(np.array(train_y))\n",
    "val_torch_y = torch.tensor(np.array(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:12<03:54, 12.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.405819375576481, Validation Loss: 0.5019316388802095, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:26<03:56, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.39730588025043884, Validation Loss: 0.5016955918434894, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:37<03:31, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.3966499574364178, Validation Loss: 0.5059120600873773, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:49<03:14, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.39599919425589697, Validation Loss: 0.5040413572481184, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:01<03:01, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.39489123653324826, Validation Loss: 0.505929638038982, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:14<02:55, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.3936496283090304, Validation Loss: 0.5073866661299359, Validation Accuracy: 0.803030303030303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:28<02:46, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.3931080689269399, Validation Loss: 0.5143237536152204, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:40<02:31, 12.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.39242646646878077, Validation Loss: 0.518370282695149, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:53<02:18, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.39262530285687675, Validation Loss: 0.5221203491091728, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:04<02:03, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.39264378237818914, Validation Loss: 0.5244533105781584, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [02:16<01:49, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training Loss: 0.39059378575711023, Validation Loss: 0.5258567872824091, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [02:28<01:37, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training Loss: 0.39012577284186606, Validation Loss: 0.5266470721725262, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [02:40<01:24, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training Loss: 0.3897258121342886, Validation Loss: 0.5270262195756941, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [02:52<01:11, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Training Loss: 0.3893688496734415, Validation Loss: 0.5271756935751799, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [03:05<01:01, 12.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Training Loss: 0.3891188347741725, Validation Loss: 0.5271348598780055, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [03:16<00:47, 11.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Training Loss: 0.388952060162075, Validation Loss: 0.5271157173044754, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [03:28<00:36, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Training Loss: 0.3887347985236418, Validation Loss: 0.5272215995373148, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [03:41<00:24, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Training Loss: 0.38852022199167147, Validation Loss: 0.5273394104883526, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [03:53<00:12, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training Loss: 0.38830634000755493, Validation Loss: 0.5275801982391964, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:04<00:00, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Training Loss: 0.3881064667706452, Validation Loss: 0.5276957211846655, Validation Accuracy: 0.7878787878787878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, global_sort_pool\n",
    "from torch_geometric.nn.aggr import SortAggregation\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Sigmoid\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index.to(torch.int64)\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        return x\n",
    "\n",
    "class SiameseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SiameseGNN, self).__init__()\n",
    "        self.gnn = GNN(num_features)\n",
    "        self.sort_aggr = SortAggregation(k=50)\n",
    "        self.fc1 = Linear(10050, 128)  # Adjust input size according to pooling output\n",
    "        self.norm1 = LayerNorm(128)\n",
    "        self.relu1 = ReLU()\n",
    "        self.fc2 = Linear(128, 64)\n",
    "        self.norm2 = LayerNorm(64)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc3 = Linear(64, 1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        out1 = self.gnn(data1)\n",
    "        out2 = self.gnn(data2)\n",
    "        out = torch.cdist(out1, out2, p=2)\n",
    "        out = self.sort_aggr(out, data1.batch)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the pooled output\n",
    "        out = self.fc1(out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# DataLoader that loads pairs of graphs\n",
    "train_loader = train_loader_pairs\n",
    "val_loader = val_loader_pairs\n",
    "\n",
    "model = SiameseGNN(num_features=train_loader[0][0].num_node_features)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Adjust step_size and gamma as needed\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for i in range(len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_loader[i][0], train_loader[i][1])\n",
    "        loss = criterion(out.squeeze(), train_torch_y[i].to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    scheduler.step()  # Add this line to update the learning rate\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(val_loader)):\n",
    "            out = model(val_loader[i][0], val_loader[i][1])\n",
    "            val_loss = criterion(out.squeeze(), val_torch_y[i].to(torch.float))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            predictions = torch.round(out.squeeze())\n",
    "            correct += (predictions == val_torch_y[i]).sum().item()\n",
    "            total += 1\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {sum(train_losses)/len(train_losses)}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sGNN with Feature Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateFeatures:\n",
    "    \"\"\"\n",
    "    We define a class which builds the feature dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"../data/\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_econ_features(self, filter_gdp = True):\n",
    "        \n",
    "        #DATA IMPORT\n",
    "        #import dictionary with all features from WB\n",
    "        with open(self.data_dir + 'all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "            \n",
    "        self.feature_list = list(features_dict.keys())[1:]\n",
    "        #import list of all features we want to select for\n",
    "\n",
    "        #look up each of the features -- add country feature in that year \n",
    "        i = 0\n",
    "        for feature in self.feature_list:\n",
    "            #find dataframe corresponding to specific feature name\n",
    "            df = features_dict[feature]\n",
    "            \n",
    "            if (i == 0):\n",
    "                self.features = df[[\"economy\", \"YR\" + str(self.year)]]\n",
    "            else: \n",
    "                self.features = pd.merge(self.features, \n",
    "                                            df[[\"economy\", \"YR\" + str(self.year)]],\n",
    "                                            on = \"economy\", how = \"outer\")\n",
    "            self.features.rename(columns = {\"YR\" + str(self.year): feature}, inplace = True)\n",
    "            i = i+1\n",
    "        \n",
    "        #prepare GDP feature\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        cols = list(self.gdp_growth.columns.copy())\n",
    "        cols.remove(\"economy\")\n",
    "        self.gdp_growth[\"country_sd\"] = self.gdp_growth[cols].std(axis=1)\n",
    "        #select potential variables \n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year)]\n",
    "        #we eliminate countries that are too volatile in growth -- probably an indicator that growth estimates are inaccurate\n",
    "        self.gdp_growth = self.gdp_growth[[\"economy\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\"]].dropna()\n",
    "        \n",
    "        #combine GDP and other features\n",
    "        self.features = pd.merge(self.gdp_growth, self.features,\n",
    "                                   on = \"economy\", how = \"left\")\n",
    "        #we only keep countries where we observe GDP growth -- otherwise nothing to predict\n",
    "        #we keep countries where other features may be missing -- and fill NAs with 0 \n",
    "        self.features.rename(columns = {\"economy\": \"country_code\"}, inplace = True)\n",
    "        \n",
    "    def prepare_network_features(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(self.data_dir + \"ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        #trade_data = trade_data[trade_data[\"product_category\"] == \"1\"]\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, on = \"importer_code\")\n",
    "        imports_table_grouped[\"import_fraction\"] = imports_table_grouped[\"import_value\"]\\\n",
    "                        /imports_table_grouped[\"import_total\"]*100\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        \n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "        \n",
    "        self.G = G\n",
    "        self.centrality_overall= nx.eigenvector_centrality(G, max_iter= 10000) \n",
    "        self.centrality_overall = pd.DataFrame(list(map(list, self.centrality_overall.items())), \n",
    "                                               columns = [\"country_code\", \"centrality_overall\"])\n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", [\"import_fraction\"])\n",
    "        weighted_centrality = nx.eigenvector_centrality(G, weight = \"import_fraction\", max_iter= 10000) \n",
    "        weighted_centrality  = pd.DataFrame(list(map(list, weighted_centrality.items())), \n",
    "                                               columns = [\"country_code\", \"weighted_centrality\"])\n",
    "        self.centrality_overall = pd.merge(self.centrality_overall, weighted_centrality, on = \"country_code\")\n",
    "        \n",
    "                               \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY IN PRODUCT CATEGORIES\n",
    "\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = imports_table.groupby([\"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = products_grouped.rename(columns = {\"import_value\": \"import_product_total\"})\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"importer_code\", \"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(products_grouped, self.export_types, on = \"product_category\")\n",
    "        \n",
    "        self.export_types[\"product_export_fraction\"] = self.export_types[\"import_value\"]\\\n",
    "                                                    /self.export_types[\"import_product_total\"]*100\n",
    "        \n",
    "        list_products = list(set(self.export_types[\"product_category\"]))\n",
    "        \n",
    "        i = 0 \n",
    "        for product in list_products:\n",
    "            \n",
    "            temp = self.export_types[self.export_types[\"product_category\"] == product].copy()\n",
    "            \n",
    "            G_w=nx.from_pandas_edgelist(temp, \n",
    "                \"exporter_code\", \"importer_code\", [\"product_export_fraction\"], create_using = nx.DiGraph())\n",
    "            centrality_product_w = nx.eigenvector_centrality(G_w, weight = \"product_export_fraction\", \n",
    "                                                           max_iter= 10000)\n",
    "\n",
    "            G=nx.from_pandas_edgelist(temp,\"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "            centrality_product = nx.eigenvector_centrality(G,max_iter= 10000)\n",
    "\n",
    "            if(i == 0):\n",
    "                self.centrality_product = pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product])\n",
    "                \n",
    "\n",
    "            else: \n",
    "                self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "                \n",
    "            self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product_w.items())), \n",
    "                                               columns = [\"country_code\", \"prod_w_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "            \n",
    "            i = i+1         \n",
    "    \n",
    "    def combine_normalize_features(self):\n",
    "        \n",
    "        self.combined_features = pd.merge(self.features, self.centrality_overall,on = \"country_code\")\n",
    "        self.combined_features = pd.merge(self.combined_features, self.centrality_product,on = \"country_code\")\n",
    "        #step eliminates NA and nodes that are not in graph, since they will have NA for graph features\n",
    "        self.combined_features = self.combined_features.drop(columns = [\"index\"])\n",
    "        #filter both trade data and features data to same subset of countries\n",
    "        self.combined_features = self.combined_features[\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.importer_code)|\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.exporter_code)]\n",
    "        self.trade_data = self.trade_data[\\\n",
    "                          self.trade_data.importer_code.isin(self.combined_features.country_code)&\\\n",
    "                          self.trade_data.exporter_code.isin(self.combined_features.country_code)]\n",
    "        \n",
    "        features_to_norm = list(self.combined_features.columns.copy())\n",
    "        non_norm = [\"country_code\", \"node_numbers\"]\n",
    "        cols_insufficient_data = list(self.combined_features.loc[:, self.combined_features.nunique() < 2].columns.copy())\n",
    "        non_norm.extend(cols_insufficient_data)\n",
    " \n",
    "        features_to_norm = [x for x in features_to_norm if x not in non_norm]\n",
    "        scaler = StandardScaler()\n",
    "        #we preserve NAs in the scaling\n",
    "        self.combined_features[features_to_norm] = scaler.fit_transform(self.combined_features[features_to_norm])\n",
    "        self.combined_features.fillna(0, inplace = True) #we fill NA after scaling \n",
    "        #check that feature has at least 20% coverage in a given year -- otherwise set to NA\n",
    "        for feature in self.feature_list:\n",
    "            coverage = len(self.combined_features[self.combined_features[feature] != 0])/len(self.combined_features)\n",
    "            if(coverage <= 0.20): self.combined_features[feature] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeNetwork:\n",
    "    \"\"\"\n",
    "    We define a class which computes the MST trade network for a given year \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"data\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_features(self, filter_gdp = True):\n",
    "        \n",
    "        ###IMPORT GDP###\n",
    "        #prepare GDP as a set of features \n",
    "        with open('data/all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "\n",
    "        self.gdp = features_dict['NY.GDP.MKTP.CD']\n",
    "        scaler = StandardScaler()\n",
    "        self.gdp[[\"prev_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year-1)]]))\n",
    "        self.gdp[[\"current_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year)]]))\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp[\"country_code\"] = self.gdp[\"economy\"]\n",
    "        self.gdp = self.gdp[[\"country_code\", \"prev_gdp\", \"current_gdp\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP GROWTH###\n",
    "        #prepare GDP growth\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth['YR'+str(self.year)] \n",
    "        self.gdp_growth[\"future_gdp_growth\"] = self.gdp_growth['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_growth[\"country_code\"] = self.gdp_growth[\"economy\"]\n",
    "        self.gdp_growth = self.gdp_growth[[\"country_code\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\", \"future_gdp_growth\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA###\n",
    "        self.gdp_per_capita = features_dict['NY.GDP.PCAP.CD']\n",
    "        self.gdp_per_capita[\"prev_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita[\"current_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year)]\n",
    "        self.gdp_per_capita[\"future_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita[\"country_code\"] = self.gdp_per_capita[\"economy\"]\n",
    "        self.gdp_per_capita = self.gdp_per_capita[[\"country_code\", \"prev_gdp_per_cap\",\n",
    "                                \"current_gdp_per_cap\", \"future_gdp_per_cap\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA GROWTH###\n",
    "        self.gdp_per_capita_growth = features_dict['NY.GDP.PCAP.KD.ZG']\n",
    "        self.gdp_per_capita_growth[\"prev_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita_growth[\"current_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year)]\n",
    "        self.gdp_per_capita_growth[\"future_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year+1)]\n",
    "        \n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita_growth[\"country_code\"] = self.gdp_per_capita_growth[\"economy\"]\n",
    "        self.gdp_per_capita_growth = self.gdp_per_capita_growth[[\"country_code\", \"prev_gdp_per_cap_growth\",\n",
    "                                \"current_gdp_per_cap_growth\", \"future_gdp_per_cap_growth\"]].dropna()\n",
    "        \n",
    "        ###MERGE ALL DATA FEATURES###\n",
    "        self.features = pd.merge(self.gdp_growth, self.gdp, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita_growth, on = \"country_code\").dropna()\n",
    "\n",
    "    def prepare_network(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(\"data/ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"/country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        #trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        #trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(self.export_types, exporter_total, on = \"exporter_code\")\n",
    "        #multiply by 100 to allow weights to scale better in GNN\n",
    "        self.export_types[\"category_fraction\"] = self.export_types.import_value/self.export_types.export_total*10\n",
    "        ss = StandardScaler()\n",
    "        columns = list(set(self.export_types[\"product_category\"]))\n",
    "        self.export_types = self.export_types[[\"exporter_code\", \"product_category\", \"category_fraction\"]]\\\n",
    "        .pivot(index = [\"exporter_code\"], columns = [\"product_category\"], values = \"category_fraction\")\\\n",
    "        .reset_index().fillna(0)\n",
    "        #rename columns\n",
    "        rename_columns = []\n",
    "        for col in self.export_types.columns:\n",
    "            if(col == \"exporter_code\"):\n",
    "                rename_columns.append(col)\n",
    "            else:\n",
    "                rename_columns.append(\"resource_\" + col)\n",
    "        self.export_types.columns = rename_columns\n",
    "        self.export_types = self.export_types.rename(columns = {\"exporter_code\": \"country_code\"})\n",
    "        self.features = pd.merge(self.features, self.export_types, \n",
    "                                on = \"country_code\", how = \"left\")\n",
    "        \n",
    "        #look at fraction of goods traded with each counterparty\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, exporter_total, how = \"left\")\n",
    "        imports_table_grouped[\"export_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"export_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"export_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"export_percent\"]]))\n",
    "        imports_table_grouped[\"export_percent_feature\"] = imports_table_grouped[\"export_percent_feature\"] + abs(min(imports_table_grouped[\"export_percent_feature\"]))\n",
    "        \n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, how = \"left\")\n",
    "        imports_table_grouped[\"import_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"import_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"import_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"import_percent\"]]))\n",
    "        imports_table_grouped[\"import_percent_feature\"] = imports_table_grouped[\"import_percent_feature\"] + abs(min(imports_table_grouped[\"import_percent_feature\"]))\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "\n",
    "    def graph_create(self, exporter = True,\n",
    "            node_features = ['prev_gdp_growth', 'current_gdp_growth','prev_gdp','current_gdp'],\n",
    "            node_labels = 'future_gdp_growth'):\n",
    "        \n",
    "        if(exporter):\n",
    "            center_node = \"exporter_code\"\n",
    "            neighbors = \"importer_code\"\n",
    "            edge_features = 'export_percent'\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        \n",
    "        \n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features.fillna(0, inplace = True)\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        #create lookup dictionary making node number / node features combatible with ordering of nodes\n",
    "        #in our edge table\n",
    "\n",
    "        self.node_lookup1 = self.features.set_index('node_numbers').to_dict()['country_code']\n",
    "        self.node_lookup2 = self.features.set_index('country_code').to_dict()['node_numbers']\n",
    "        \n",
    "        #get individual country's features\n",
    "        self.regression_table = pd.merge(self.features, self.trade_data,\n",
    "                        left_on = \"country_code\",\n",
    "                        right_on = center_node, how = 'right')\n",
    "        #get features for trade partners\n",
    "        self.regression_table = pd.merge(self.features, self.regression_table,\n",
    "                                        left_on = \"country_code\",\n",
    "                                        right_on = neighbors, how = \"right\",\n",
    "                                        suffixes = (\"_neighbors\", \"\"))\n",
    "        \n",
    "        self.trade_data = self.trade_data[self.trade_data[neighbors].isin(self.node_lookup2)]\n",
    "        self.trade_data = self.trade_data[self.trade_data[center_node].isin(self.node_lookup2)]\n",
    "\n",
    "        self.regression_table[\"source\"] = self.trade_data[neighbors].apply(lambda x: self.node_lookup2[x])\n",
    "        self.regression_table[\"target\"] = self.trade_data[center_node].apply(lambda x: self.node_lookup2[x])    \n",
    "\n",
    "        self.regression_table = self.regression_table.dropna()\n",
    "        #filter only to relevant columns\n",
    "        self.relevant_columns = [\"source\", \"target\"]\n",
    "        self.relevant_columns.extend(node_features)\n",
    "        self.relevant_columns.append(node_labels)\n",
    "        self.graph_table = self.regression_table[self.relevant_columns]\n",
    "        \n",
    "        if(self.graph_table.isnull().values.any()): print(\"edges contain null / inf values\")\n",
    "\n",
    "        self.node_attributes = torch.tensor(np.array(self.features[node_features]))\\\n",
    "        .to(torch.float)\n",
    "        self.source_nodes = list(self.graph_table[\"source\"])\n",
    "        self.target_nodes = list(self.graph_table[\"target\"])\n",
    "        self.edge_attributes = list(self.trade_data[edge_features])\n",
    "\n",
    "        self.pyg_graph = data.Data(x = self.node_attributes,\n",
    "                                   edge_index = torch.tensor([self.source_nodes, self.target_nodes]),\n",
    "                                   edge_attr = torch.tensor(self.edge_attributes).to(torch.float),\n",
    "                                   y = torch.tensor(list(self.features[node_labels])).to(torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/filtered_features_dict.pkl\", \"rb\") as f:\n",
    "    feat_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_gdp_growth</th>\n",
       "      <th>FM.AST.PRVT.ZG.M3</th>\n",
       "      <th>FM.LBL.BMNY.ZG</th>\n",
       "      <th>IT.MLT.MAIN.P2</th>\n",
       "      <th>NE.CON.PRVT.KD.ZG</th>\n",
       "      <th>NE.CON.PRVT.PC.KD.ZG</th>\n",
       "      <th>NE.CON.TOTL.KD.ZG</th>\n",
       "      <th>NE.GDI.FTOT.KD.ZG</th>\n",
       "      <th>NE.GDI.TOTL.KD.ZG</th>\n",
       "      <th>NE.IMP.GNFS.KD.ZG</th>\n",
       "      <th>...</th>\n",
       "      <th>SP.POP.7074.FE.5Y</th>\n",
       "      <th>SP.POP.7074.MA.5Y</th>\n",
       "      <th>SP.POP.7579.MA.5Y</th>\n",
       "      <th>SP.POP.80UP.MA.5Y</th>\n",
       "      <th>SP.POP.DPND.OL</th>\n",
       "      <th>SP.POP.DPND.YG</th>\n",
       "      <th>SP.POP.GROW</th>\n",
       "      <th>SP.URB.GROW</th>\n",
       "      <th>current_gdp_growth</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190007</td>\n",
       "      <td>0.145456</td>\n",
       "      <td>-0.309841</td>\n",
       "      <td>-0.111774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.024184</td>\n",
       "      <td>-1.066195</td>\n",
       "      <td>-0.806555</td>\n",
       "      <td>-0.258654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188540</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>-0.093827</td>\n",
       "      <td>-0.580121</td>\n",
       "      <td>-0.024536</td>\n",
       "      <td>-1.192493</td>\n",
       "      <td>-0.627666</td>\n",
       "      <td>-0.906953</td>\n",
       "      <td>-1.112815</td>\n",
       "      <td>ARG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.292556</td>\n",
       "      <td>0.145012</td>\n",
       "      <td>-0.424987</td>\n",
       "      <td>1.301831</td>\n",
       "      <td>-0.340188</td>\n",
       "      <td>-0.282538</td>\n",
       "      <td>-0.246697</td>\n",
       "      <td>-0.540698</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.949600</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366470</td>\n",
       "      <td>1.289176</td>\n",
       "      <td>1.081964</td>\n",
       "      <td>0.970590</td>\n",
       "      <td>1.229090</td>\n",
       "      <td>-1.161966</td>\n",
       "      <td>0.184470</td>\n",
       "      <td>-0.696583</td>\n",
       "      <td>-0.714648</td>\n",
       "      <td>AUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.208053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.652608</td>\n",
       "      <td>2.290681</td>\n",
       "      <td>2.279018</td>\n",
       "      <td>1.948592</td>\n",
       "      <td>2.477550</td>\n",
       "      <td>-1.855186</td>\n",
       "      <td>-1.645028</td>\n",
       "      <td>-1.742712</td>\n",
       "      <td>-0.463491</td>\n",
       "      <td>AUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.116371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.355234</td>\n",
       "      <td>2.329008</td>\n",
       "      <td>2.448650</td>\n",
       "      <td>2.382717</td>\n",
       "      <td>2.419029</td>\n",
       "      <td>-1.744078</td>\n",
       "      <td>-1.860075</td>\n",
       "      <td>-1.820086</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>BEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.184642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.685382</td>\n",
       "      <td>-0.913001</td>\n",
       "      <td>-0.753715</td>\n",
       "      <td>-0.700270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.447176</td>\n",
       "      <td>-0.038399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216401</td>\n",
       "      <td>0.317638</td>\n",
       "      <td>0.277749</td>\n",
       "      <td>0.234998</td>\n",
       "      <td>0.401940</td>\n",
       "      <td>-0.064347</td>\n",
       "      <td>-0.620756</td>\n",
       "      <td>1.736960</td>\n",
       "      <td>-1.590323</td>\n",
       "      <td>BEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.509915</td>\n",
       "      <td>0.146601</td>\n",
       "      <td>-0.352908</td>\n",
       "      <td>-0.611217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.185401</td>\n",
       "      <td>-0.427716</td>\n",
       "      <td>-0.470985</td>\n",
       "      <td>-0.241155</td>\n",
       "      <td>-0.142476</td>\n",
       "      <td>0.354390</td>\n",
       "      <td>0.462587</td>\n",
       "      <td>0.089780</td>\n",
       "      <td>0.078634</td>\n",
       "      <td>TUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.290855</td>\n",
       "      <td>0.146321</td>\n",
       "      <td>-0.526697</td>\n",
       "      <td>-0.129166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102288</td>\n",
       "      <td>-0.490386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947874</td>\n",
       "      <td>1.081684</td>\n",
       "      <td>1.100397</td>\n",
       "      <td>1.256356</td>\n",
       "      <td>0.978531</td>\n",
       "      <td>-1.451146</td>\n",
       "      <td>-1.043111</td>\n",
       "      <td>-1.345748</td>\n",
       "      <td>-1.246832</td>\n",
       "      <td>URY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-0.322484</td>\n",
       "      <td>0.145460</td>\n",
       "      <td>-0.371624</td>\n",
       "      <td>2.761470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.428942</td>\n",
       "      <td>1.687666</td>\n",
       "      <td>1.564861</td>\n",
       "      <td>1.494738</td>\n",
       "      <td>1.606441</td>\n",
       "      <td>-1.045552</td>\n",
       "      <td>-0.719038</td>\n",
       "      <td>-1.051855</td>\n",
       "      <td>0.176675</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.176246</td>\n",
       "      <td>-6.855651</td>\n",
       "      <td>-0.759620</td>\n",
       "      <td>-0.495877</td>\n",
       "      <td>-0.245292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.328785</td>\n",
       "      <td>-0.399401</td>\n",
       "      <td>0.120419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.922368</td>\n",
       "      <td>-0.900646</td>\n",
       "      <td>-0.821015</td>\n",
       "      <td>-0.755621</td>\n",
       "      <td>-0.929226</td>\n",
       "      <td>1.013644</td>\n",
       "      <td>1.243462</td>\n",
       "      <td>0.486535</td>\n",
       "      <td>0.627946</td>\n",
       "      <td>VEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.069382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.198863</td>\n",
       "      <td>-0.245512</td>\n",
       "      <td>-0.273655</td>\n",
       "      <td>0.173022</td>\n",
       "      <td>-0.760312</td>\n",
       "      <td>-0.546148</td>\n",
       "      <td>-0.197392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541077</td>\n",
       "      <td>-0.682702</td>\n",
       "      <td>-0.542326</td>\n",
       "      <td>-0.261034</td>\n",
       "      <td>-0.573256</td>\n",
       "      <td>0.254674</td>\n",
       "      <td>0.722998</td>\n",
       "      <td>-0.482774</td>\n",
       "      <td>0.191130</td>\n",
       "      <td>ZAF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    prev_gdp_growth  FM.AST.PRVT.ZG.M3  FM.LBL.BMNY.ZG  IT.MLT.MAIN.P2  \\\n",
       "0          0.190007           0.145456       -0.309841       -0.111774   \n",
       "1         -0.292556           0.145012       -0.424987        1.301831   \n",
       "2          0.208053           0.000000        0.000000        0.097787   \n",
       "3          0.116371           0.000000        0.000000        0.417067   \n",
       "4         -0.184642           0.000000        0.000000       -0.685382   \n",
       "..              ...                ...             ...             ...   \n",
       "71        -0.509915           0.146601       -0.352908       -0.611217   \n",
       "72        -0.290855           0.146321       -0.526697       -0.129166   \n",
       "73        -0.322484           0.145460       -0.371624        2.761470   \n",
       "74        -0.176246          -6.855651       -0.759620       -0.495877   \n",
       "75        -0.069382           0.000000        0.000000       -0.198863   \n",
       "\n",
       "    NE.CON.PRVT.KD.ZG  NE.CON.PRVT.PC.KD.ZG  NE.CON.TOTL.KD.ZG  \\\n",
       "0            0.000000              0.000000          -1.024184   \n",
       "1           -0.340188             -0.282538          -0.246697   \n",
       "2            0.000000              0.000000           0.000000   \n",
       "3            0.000000              0.000000           0.000000   \n",
       "4           -0.913001             -0.753715          -0.700270   \n",
       "..                ...                   ...                ...   \n",
       "71           0.000000              0.000000           0.000000   \n",
       "72           0.000000              0.000000           0.102288   \n",
       "73           0.000000              0.000000           0.000000   \n",
       "74          -0.245292              0.000000          -0.328785   \n",
       "75          -0.245512             -0.273655           0.173022   \n",
       "\n",
       "    NE.GDI.FTOT.KD.ZG  NE.GDI.TOTL.KD.ZG  NE.IMP.GNFS.KD.ZG  ...  \\\n",
       "0           -1.066195          -0.806555          -0.258654  ...   \n",
       "1           -0.540698           0.000000          -0.949600  ...   \n",
       "2            0.000000           0.000000           0.000000  ...   \n",
       "3            0.000000           0.000000           0.000000  ...   \n",
       "4            0.000000          -0.447176          -0.038399  ...   \n",
       "..                ...                ...                ...  ...   \n",
       "71           0.000000           0.000000           0.000000  ...   \n",
       "72          -0.490386           0.000000           0.391464  ...   \n",
       "73           0.000000           0.000000           0.000000  ...   \n",
       "74          -0.399401           0.120419           0.000000  ...   \n",
       "75          -0.760312          -0.546148          -0.197392  ...   \n",
       "\n",
       "    SP.POP.7074.FE.5Y  SP.POP.7074.MA.5Y  SP.POP.7579.MA.5Y  \\\n",
       "0            0.188540           0.346821          -0.093827   \n",
       "1            1.366470           1.289176           1.081964   \n",
       "2            2.652608           2.290681           2.279018   \n",
       "3            2.355234           2.329008           2.448650   \n",
       "4            0.216401           0.317638           0.277749   \n",
       "..                ...                ...                ...   \n",
       "71          -0.185401          -0.427716          -0.470985   \n",
       "72           0.947874           1.081684           1.100397   \n",
       "73           1.428942           1.687666           1.564861   \n",
       "74          -0.922368          -0.900646          -0.821015   \n",
       "75          -0.541077          -0.682702          -0.542326   \n",
       "\n",
       "    SP.POP.80UP.MA.5Y  SP.POP.DPND.OL  SP.POP.DPND.YG  SP.POP.GROW  \\\n",
       "0           -0.580121       -0.024536       -1.192493    -0.627666   \n",
       "1            0.970590        1.229090       -1.161966     0.184470   \n",
       "2            1.948592        2.477550       -1.855186    -1.645028   \n",
       "3            2.382717        2.419029       -1.744078    -1.860075   \n",
       "4            0.234998        0.401940       -0.064347    -0.620756   \n",
       "..                ...             ...             ...          ...   \n",
       "71          -0.241155       -0.142476        0.354390     0.462587   \n",
       "72           1.256356        0.978531       -1.451146    -1.043111   \n",
       "73           1.494738        1.606441       -1.045552    -0.719038   \n",
       "74          -0.755621       -0.929226        1.013644     1.243462   \n",
       "75          -0.261034       -0.573256        0.254674     0.722998   \n",
       "\n",
       "    SP.URB.GROW  current_gdp_growth  country_code  \n",
       "0     -0.906953           -1.112815           ARG  \n",
       "1     -0.696583           -0.714648           AUS  \n",
       "2     -1.742712           -0.463491           AUT  \n",
       "3     -1.820086            0.011966           BEL  \n",
       "4      1.736960           -1.590323           BEN  \n",
       "..          ...                 ...           ...  \n",
       "71     0.089780            0.078634           TUR  \n",
       "72    -1.345748           -1.246832           URY  \n",
       "73    -1.051855            0.176675           USA  \n",
       "74     0.486535            0.627946           VEN  \n",
       "75    -0.482774            0.191130           ZAF  \n",
       "\n",
       "[76 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dict[1962].combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = ['ABW', 'AFG', 'AGO', 'ALB', 'AND', 'ARE', 'ARG', 'ARM', 'ASM',\n",
    "       'ATG', 'AUS', 'AUT', 'AZE', 'BDI', 'BEL', 'BEN', 'BFA', 'BGD',\n",
    "       'BGR', 'BHR', 'BHS', 'BIH', 'BLR', 'BLZ', 'BMU', 'BOL', 'BRA',\n",
    "       'BRB', 'BRN', 'BTN', 'BWA', 'CAF', 'CAN', 'CHE', 'CHL', 'CHN',\n",
    "       'CIV', 'CMR', 'COD', 'COG', 'COL', 'COM', 'CPV', 'CRI', 'CUB',\n",
    "       'CUW', 'CYM', 'CYP', 'CZE', 'DEU', 'DMA', 'DNK', 'DOM', 'DZA',\n",
    "       'ECU', 'EGY', 'ESP', 'EST', 'ETH', 'FIN', 'FJI', 'FRA', 'FSM',\n",
    "       'GAB', 'GBR', 'GEO', 'GHA', 'GIN', 'GMB', 'GNB', 'GNQ', 'GRC',\n",
    "       'GRD', 'GRL', 'GTM', 'GUM', 'GUY', 'HKG', 'HND', 'HRV', 'HTI',\n",
    "       'HUN', 'IDN', 'IND', 'IRL', 'IRN', 'IRQ', 'ISL', 'ISR', 'ITA',\n",
    "       'JAM', 'JOR', 'JPN', 'KAZ', 'KEN', 'KGZ', 'KHM', 'KNA', 'KOR',\n",
    "       'KWT', 'LAO', 'LBN', 'LBR', 'LBY', 'LCA', 'LKA', 'LSO', 'LTU',\n",
    "       'LUX', 'LVA', 'MAC', 'MAR', 'MDA', 'MDG', 'MDV', 'MEX', 'MHL',\n",
    "       'MKD', 'MLI', 'MLT', 'MMR', 'MNE', 'MNG', 'MNP', 'MOZ', 'MRT',\n",
    "       'MUS', 'MWI', 'MYS', 'NAM', 'NER', 'NGA', 'NIC', 'NLD', 'NOR',\n",
    "       'NPL', 'NRU', 'NZL', 'OMN', 'PAK', 'PAN', 'PER', 'PHL', 'PLW',\n",
    "       'PNG', 'POL', 'PRT', 'PRY', 'PSE', 'PYF', 'QAT', 'ROU', 'RUS',\n",
    "       'RWA', 'SAU', 'SDN', 'SEN', 'SGP', 'SLB', 'SLE', 'SLV', 'SMR',\n",
    "       'SRB', 'SSD', 'STP', 'SUR', 'SVK', 'SVN', 'SWE', 'SWZ', 'SXM',\n",
    "       'SYC', 'SYR', 'TCD', 'TGO', 'THA', 'TJK', 'TKM', 'TLS', 'TON',\n",
    "       'TTO', 'TUN', 'TUR', 'TUV', 'TZA', 'UGA', 'UKR', 'URY', 'USA',\n",
    "       'UZB', 'VCT', 'VEN', 'VNM', 'VUT', 'WSM', 'YEM', 'ZAF', 'ZMB',\n",
    "       'ZWE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = []\n",
    "val_graphs = []\n",
    "test_graphs = []\n",
    "i = 0\n",
    "\n",
    "years = list(range(1962,2021))\n",
    "for year in tqdm(years):\n",
    "    \n",
    "    print(str(year), end='\\r')\n",
    "    \n",
    "    trade = TradeNetwork(year = year)\n",
    "    trade.prepare_features()\n",
    "    trade.prepare_network()\n",
    "    trade.graph_create(node_features = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth',\n",
    "    'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "       'resource_8', 'resource_9'],\n",
    "        node_labels = 'future_gdp_per_cap_growth')\n",
    "    \n",
    "    current_nodes = list(trade.node_lookup2.keys())\n",
    "    num_features = trade.pyg_graph.x.size(1)\n",
    "    zeros = torch.zeros(1, num_features)\n",
    "\n",
    "    for i in range(len(all_nodes)):\n",
    "        if i >= len(current_nodes):\n",
    "            current_nodes.insert(i, all_nodes[i])\n",
    "            tensor_before = trade.pyg_graph.x\n",
    "            new_tensor = torch.cat([tensor_before, zeros], dim=0)\n",
    "            trade.pyg_graph.x = new_tensor\n",
    "\n",
    "            node_attr_before = trade.node_attributes\n",
    "            trade.node_attributes = torch.cat([node_attr_before, zeros], dim=0)\n",
    "\n",
    "        elif(all_nodes[i]!=current_nodes[i]):\n",
    "            current_nodes.insert(i, all_nodes[i])\n",
    "            if i==0:\n",
    "                tensor_after = trade.pyg_graph.x[i:]\n",
    "                new_tensor = torch.cat([zeros, tensor_after], dim=0)\n",
    "                trade.pyg_graph.x = new_tensor\n",
    "\n",
    "                node_attr_after = trade.node_attributes[i:]\n",
    "                trade.node_attributes = torch.cat([zeros, node_attr_after], dim=0)\n",
    "            else:\n",
    "                tensor_before = trade.pyg_graph.x[:i]\n",
    "                tensor_after = trade.pyg_graph.x[i:]\n",
    "                new_tensor = torch.cat([tensor_before, zeros, tensor_after], dim=0)\n",
    "                trade.pyg_graph.x = new_tensor\n",
    "\n",
    "                node_attr_before = trade.node_attributes[:i]\n",
    "                node_attr_after = trade.node_attributes[i:]\n",
    "                trade.node_attributes = torch.cat([node_attr_before, zeros, node_attr_after], dim=0)\n",
    "\n",
    "    for i in range(len(current_nodes) - 1, -1, -1):\n",
    "        if current_nodes[i] not in all_nodes:\n",
    "            node_to_remove = current_nodes.pop(i)\n",
    "\n",
    "            tensor_before = trade.pyg_graph.x[:i]\n",
    "            tensor_after = trade.pyg_graph.x[i+1:]\n",
    "            new_tensor = torch.cat([tensor_before, tensor_after], dim=0)\n",
    "            trade.pyg_graph.x = new_tensor\n",
    "\n",
    "            node_attr_before = trade.node_attributes[:i]\n",
    "            node_attr_after = trade.node_attributes[i+1:]\n",
    "            trade.node_attributes = torch.cat([node_attr_before, node_attr_after], dim=0)\n",
    "\n",
    "    if len(current_nodes) > len(all_nodes):\n",
    "        num_diff = len(current_nodes) - len(all_nodes)\n",
    "        trade.pyg_graph.x = trade.pyg_graph.x[:-num_diff]\n",
    "        trade.node_attributes = trade.node_attributes[:-num_diff]\n",
    "        current_nodes = current_nodes[:-num_diff]\n",
    "    \n",
    "    if current_nodes != all_nodes:\n",
    "        print(\"Nodes not equal\")\n",
    "        break\n",
    "\n",
    "    if(year in val_years):\n",
    "        val_graphs.append(trade.pyg_graph)\n",
    "    elif(year in test_years):\n",
    "        test_graphs.append(trade.pyg_graph)\n",
    "    else: \n",
    "        train_graphs.append(trade.pyg_graph)\n",
    "        \n",
    "    trade.features[\"year\"] = year\n",
    "    \n",
    "    if(i == 0):\n",
    "        trade_df = trade.features\n",
    "    else: \n",
    "        trade_df = pd.concat([trade_df, trade.features])\n",
    "        \n",
    "    i = i+1\n",
    "    print(trade.node_attributes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pygcn/train_graphs.pickle\", \"rb\") as f:\n",
    "    train_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"rb\") as f:  \n",
    "    val_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"rb\") as f:         \n",
    "    test_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros(59)\n",
    "\n",
    "for i in range(len(val_years)):\n",
    "    new_x = torch.empty(0, 59)\n",
    "    year = val_years[i]\n",
    "    data = val_graphs[i]\n",
    "\n",
    "    feat_dict_year = feat_dict[year].combined_features\n",
    "\n",
    "    for j, country in enumerate(all_nodes):\n",
    "        if j == 0:\n",
    "            new_x = torch.stack([zeros])\n",
    "\n",
    "        elif country in feat_dict_year[\"country_code\"].values:\n",
    "            tensor_before = val_graphs[i].x[j]\n",
    "            country_row = feat_dict_year[feat_dict_year[\"country_code\"] == country]\n",
    "            country_row = country_row.drop(columns = [\"prev_gdp_growth\", \"country_code\", \"current_gdp_growth\"])\n",
    "            row_values = country_row.values.tolist()\n",
    "            row_tensor = torch.tensor(row_values)[0]\n",
    "            combined_values = torch.cat((tensor_before, row_tensor))\n",
    "\n",
    "            new_x = torch.cat((new_x, combined_values.unsqueeze(0)), dim=0)\n",
    "\n",
    "        else:\n",
    "            new_x = torch.cat((new_x, zeros.unsqueeze(0)), dim=0)\n",
    "\n",
    "    val_graphs[i].x = new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
