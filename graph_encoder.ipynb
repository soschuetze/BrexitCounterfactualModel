{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle as pkl\n",
    "import datetime as datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.formula.api as sm\n",
    "import dgl.function as fn\n",
    "from tqdm import tqdm\n",
    "\n",
    "#imports for graph creation\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#imports for graph learning\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from tqdm import trange\n",
    "import torch\n",
    "import torch_geometric.datasets as datasets\n",
    "import torch_geometric.data as data\n",
    "import torch_geometric.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_sitc_codes():\n",
    "    # URL of the JSON file\n",
    "    url = 'https://comtradeapi.un.org/files/v1/app/reference/S4.json'\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL and fetch the data\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check that the request was successful\n",
    "        \n",
    "        # Load the JSON data\n",
    "        data = response.json()\n",
    "\n",
    "        # Since the JSON data might be nested, use json_normalize with appropriate arguments\n",
    "        if isinstance(data, list):\n",
    "            # If the top level is a list\n",
    "            df = pd.json_normalize(data)\n",
    "        else:\n",
    "            # If the top level is a dictionary\n",
    "            # Identify the key that holds the main data (adjust the path as necessary)\n",
    "            main_data_key = 'results'  # Adjust this based on the actual structure\n",
    "            df = pd.json_normalize(data[main_data_key])\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing JSON structure: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeNetwork:\n",
    "    \"\"\"\n",
    "    We define a class which computes the MST trade network for a given year \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"data\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_features(self, filter_gdp = True):\n",
    "        \n",
    "        ###IMPORT GDP###\n",
    "        #prepare GDP as a set of features \n",
    "        with open('data/all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "\n",
    "        self.gdp = features_dict['NY.GDP.MKTP.CD']\n",
    "        scaler = StandardScaler()\n",
    "        self.gdp[[\"prev_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year-1)]]))\n",
    "        self.gdp[[\"current_gdp\"]] = scaler.fit_transform(np.log(self.gdp[['YR'+str(self.year)]]))\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp[\"country_code\"] = self.gdp[\"economy\"]\n",
    "        self.gdp = self.gdp[[\"country_code\", \"prev_gdp\", \"current_gdp\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP GROWTH###\n",
    "        #prepare GDP growth\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth['YR'+str(self.year)] \n",
    "        self.gdp_growth[\"future_gdp_growth\"] = self.gdp_growth['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_growth[\"country_code\"] = self.gdp_growth[\"economy\"]\n",
    "        self.gdp_growth = self.gdp_growth[[\"country_code\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\", \"future_gdp_growth\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA###\n",
    "        self.gdp_per_capita = features_dict['NY.GDP.PCAP.CD']\n",
    "        self.gdp_per_capita[\"prev_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita[\"current_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year)]\n",
    "        self.gdp_per_capita[\"future_gdp_per_cap\"] = self.gdp_per_capita['YR'+str(self.year+1)]\n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita[\"country_code\"] = self.gdp_per_capita[\"economy\"]\n",
    "        self.gdp_per_capita = self.gdp_per_capita[[\"country_code\", \"prev_gdp_per_cap\",\n",
    "                                \"current_gdp_per_cap\", \"future_gdp_per_cap\"]].dropna()\n",
    "        \n",
    "        ###IMPORT GDP PER CAPITA GROWTH###\n",
    "        self.gdp_per_capita_growth = features_dict['NY.GDP.PCAP.KD.ZG']\n",
    "        self.gdp_per_capita_growth[\"prev_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year-1)]\n",
    "        self.gdp_per_capita_growth[\"current_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year)]\n",
    "        self.gdp_per_capita_growth[\"future_gdp_per_cap_growth\"] = self.gdp_per_capita_growth['YR'+str(self.year+1)]\n",
    "        \n",
    "        #rename and keep relevant columns\n",
    "        self.gdp_per_capita_growth[\"country_code\"] = self.gdp_per_capita_growth[\"economy\"]\n",
    "        self.gdp_per_capita_growth = self.gdp_per_capita_growth[[\"country_code\", \"prev_gdp_per_cap_growth\",\n",
    "                                \"current_gdp_per_cap_growth\", \"future_gdp_per_cap_growth\"]].dropna()\n",
    "        \n",
    "        ###MERGE ALL DATA FEATURES###\n",
    "        self.features = pd.merge(self.gdp_growth, self.gdp, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita, on = \"country_code\").dropna()\n",
    "        self.features = pd.merge(self.features, self.gdp_per_capita_growth, on = \"country_code\").dropna()\n",
    "\n",
    "    def prepare_network(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(\"data/ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"/country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        #trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        #trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(self.export_types, exporter_total, on = \"exporter_code\")\n",
    "        #multiply by 100 to allow weights to scale better in GNN\n",
    "        self.export_types[\"category_fraction\"] = self.export_types.import_value/self.export_types.export_total*10\n",
    "        ss = StandardScaler()\n",
    "        columns = list(set(self.export_types[\"product_category\"]))\n",
    "        self.export_types = self.export_types[[\"exporter_code\", \"product_category\", \"category_fraction\"]]\\\n",
    "        .pivot(index = [\"exporter_code\"], columns = [\"product_category\"], values = \"category_fraction\")\\\n",
    "        .reset_index().fillna(0)\n",
    "        #rename columns\n",
    "        rename_columns = []\n",
    "        for col in self.export_types.columns:\n",
    "            if(col == \"exporter_code\"):\n",
    "                rename_columns.append(col)\n",
    "            else:\n",
    "                rename_columns.append(\"resource_\" + col)\n",
    "        self.export_types.columns = rename_columns\n",
    "        self.export_types = self.export_types.rename(columns = {\"exporter_code\": \"country_code\"})\n",
    "        self.features = pd.merge(self.features, self.export_types, \n",
    "                                on = \"country_code\", how = \"left\")\n",
    "        \n",
    "        #look at fraction of goods traded with each counterparty\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, exporter_total, how = \"left\")\n",
    "        imports_table_grouped[\"export_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"export_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"export_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"export_percent\"]]))\n",
    "        imports_table_grouped[\"export_percent_feature\"] = imports_table_grouped[\"export_percent_feature\"] + abs(min(imports_table_grouped[\"export_percent_feature\"]))\n",
    "        \n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, how = \"left\")\n",
    "        imports_table_grouped[\"import_percent\"] = imports_table_grouped[\"import_value\"]/imports_table_grouped[\"import_total\"]\n",
    "        scaler = StandardScaler()\n",
    "        imports_table_grouped[[\"import_percent_feature\"]] = scaler.fit_transform(np.log(imports_table_grouped[[\"import_percent\"]]))\n",
    "        imports_table_grouped[\"import_percent_feature\"] = imports_table_grouped[\"import_percent_feature\"] + abs(min(imports_table_grouped[\"import_percent_feature\"]))\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "\n",
    "    def graph_create(self, exporter = True,\n",
    "            node_features = ['prev_gdp_growth', 'current_gdp_growth','prev_gdp','current_gdp'],\n",
    "            node_labels = 'future_gdp_growth'):\n",
    "        \n",
    "        if(exporter):\n",
    "            center_node = \"exporter_code\"\n",
    "            neighbors = \"importer_code\"\n",
    "            edge_features = 'export_percent'\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        \n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features.fillna(0, inplace = True)\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        #create lookup dictionary making node number / node features combatible with ordering of nodes\n",
    "        #in our edge table\n",
    "\n",
    "        self.node_lookup1 = self.features.set_index('node_numbers').to_dict()['country_code']\n",
    "        self.node_lookup2 = self.features.set_index('country_code').to_dict()['node_numbers']\n",
    "        \n",
    "        #get individual country's features\n",
    "        self.regression_table = pd.merge(self.features, self.trade_data,\n",
    "                        left_on = \"country_code\",\n",
    "                        right_on = center_node, how = 'right')\n",
    "        #get features for trade partners\n",
    "        self.regression_table = pd.merge(self.features, self.regression_table,\n",
    "                                        left_on = \"country_code\",\n",
    "                                        right_on = neighbors, how = \"right\",\n",
    "                                        suffixes = (\"_neighbors\", \"\"))\n",
    "        \n",
    "        self.trade_data = self.trade_data[self.trade_data[neighbors].isin(self.node_lookup2)]\n",
    "        self.trade_data = self.trade_data[self.trade_data[center_node].isin(self.node_lookup2)]\n",
    "\n",
    "        self.regression_table[\"source\"] = self.trade_data[neighbors].apply(lambda x: self.node_lookup2[x])\n",
    "        self.regression_table[\"target\"] = self.trade_data[center_node].apply(lambda x: self.node_lookup2[x])    \n",
    "\n",
    "        self.regression_table = self.regression_table.dropna()\n",
    "        #filter only to relevant columns\n",
    "        self.relevant_columns = [\"source\", \"target\"]\n",
    "        self.relevant_columns.extend(node_features)\n",
    "        self.relevant_columns.append(node_labels)\n",
    "        self.graph_table = self.regression_table[self.relevant_columns]\n",
    "        \n",
    "        if(self.graph_table.isnull().values.any()): print(\"edges contain null / inf values\")\n",
    "\n",
    "        self.node_attributes = torch.tensor(np.array(self.features[node_features]))\\\n",
    "        .to(torch.float)\n",
    "        self.source_nodes = list(self.graph_table[\"source\"])\n",
    "        self.target_nodes = list(self.graph_table[\"target\"])\n",
    "        self.edge_attributes = list(self.trade_data[edge_features])\n",
    "\n",
    "        self.pyg_graph = data.Data(x = self.node_attributes,\n",
    "                                   edge_index = torch.tensor([self.source_nodes, self.target_nodes]),\n",
    "                                   edge_attr = torch.tensor(self.edge_attributes).to(torch.float),\n",
    "                                   y = torch.tensor(list(self.features[node_labels])).to(torch.float))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1962\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/59 [00:05<05:27,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 12])\n",
      "1963\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/59 [00:10<05:06,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([82, 12])\n",
      "1964\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/59 [00:16<05:11,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([82, 12])\n",
      "1965\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/59 [00:23<05:44,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85, 12])\n",
      "1966\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 5/59 [00:31<06:08,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87, 12])\n",
      "1967\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 6/59 [00:38<05:51,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([92, 12])\n",
      "1968\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 7/59 [00:44<05:49,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 12])\n",
      "1969\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 8/59 [00:51<05:41,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([94, 12])\n",
      "1970\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/59 [00:59<05:48,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 12])\n",
      "1971\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 10/59 [01:06<05:49,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([99, 12])\n",
      "1972\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 11/59 [01:14<05:49,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([108, 12])\n",
      "1973\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 12/59 [01:22<05:58,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([108, 12])\n",
      "1974\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13/59 [01:31<06:03,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([108, 12])\n",
      "1975\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 14/59 [01:40<06:10,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([108, 12])\n",
      "1976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15/59 [01:44<05:06,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([110, 12])\n",
      "1977\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 16/59 [01:48<04:28,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112, 12])\n",
      "1978\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 17/59 [01:54<04:19,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([114, 12])\n",
      "1979\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 18/59 [02:01<04:13,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([119, 12])\n",
      "1980\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 19/59 [02:07<04:15,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([119, 12])\n",
      "1981\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 20/59 [02:14<04:15,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 12])\n",
      "1982\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 21/59 [02:21<04:11,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([130, 12])\n",
      "1983\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22/59 [02:28<04:05,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([133, 12])\n",
      "1984\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 23/59 [02:35<04:09,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([136, 12])\n",
      "1985\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 24/59 [02:42<04:01,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([137, 12])\n",
      "1986\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 25/59 [02:50<03:59,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([139, 12])\n",
      "1987\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 26/59 [02:58<04:03,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([139, 12])\n",
      "1988\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 27/59 [03:06<04:00,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([141, 12])\n",
      "1989\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 28/59 [03:14<04:03,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([141, 12])\n",
      "1990\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 29/59 [03:23<04:04,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([141, 12])\n",
      "1991\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 30/59 [03:33<04:07,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([142, 12])\n",
      "1992\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 31/59 [03:43<04:13,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([159, 12])\n",
      "1993\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 32/59 [03:53<04:16,  9.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([162, 12])\n",
      "1994\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 33/59 [04:05<04:22, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([166, 12])\n",
      "1995\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 34/59 [04:16<04:20, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([167, 12])\n",
      "1996\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 35/59 [04:28<04:24, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([168, 12])\n",
      "1997\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 36/59 [04:41<04:21, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 12])\n",
      "1998\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 37/59 [04:53<04:17, 11.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([177, 12])\n",
      "1999\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 38/59 [05:06<04:14, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([179, 12])\n",
      "2000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 39/59 [05:19<04:09, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([184, 12])\n",
      "2001\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 40/59 [05:33<04:03, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([185, 12])\n",
      "2002\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 41/59 [05:47<03:55, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189, 12])\n",
      "2003\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 42/59 [06:02<03:51, 13.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([190, 12])\n",
      "2004\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 43/59 [06:17<03:46, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([194, 12])\n",
      "2005\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 44/59 [06:34<03:45, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 12])\n",
      "2006\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 45/59 [06:54<03:49, 16.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 12])\n",
      "2007\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 46/59 [07:11<03:37, 16.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([196, 12])\n",
      "2008\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 47/59 [07:30<03:28, 17.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 12])\n",
      "2009\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 48/59 [07:49<03:16, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 12])\n",
      "2010\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 49/59 [08:07<02:59, 17.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 12])\n",
      "2011\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 50/59 [08:25<02:41, 17.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 12])\n",
      "2012\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 51/59 [08:44<02:25, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 12])\n",
      "2013\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 52/59 [09:03<02:08, 18.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2014\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 53/59 [09:22<01:52, 18.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 12])\n",
      "2015\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 54/59 [09:42<01:34, 18.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2016\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 55/59 [10:01<01:16, 19.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2017\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 56/59 [10:20<00:56, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([201, 12])\n",
      "2018\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 57/59 [10:40<00:38, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199, 12])\n",
      "2019\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 58/59 [10:59<00:19, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([197, 12])\n",
      "2020\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [11:18<00:00, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "years = range(1962,2021)\n",
    "\n",
    "train_years = years[:30]\n",
    "val_years = years[30:42]\n",
    "test_years = years[42:]\n",
    "\n",
    "train_graphs = []\n",
    "val_graphs = []\n",
    "test_graphs = []\n",
    "i = 0\n",
    "\n",
    "for year in tqdm(years):\n",
    "    print(str(year), end='\\r')\n",
    "    \n",
    "    trade = TradeNetwork(year = year)\n",
    "    trade.prepare_features()\n",
    "    trade.prepare_network()\n",
    "    trade.graph_create(node_features = ['prev_gdp_per_cap_growth', 'current_gdp_per_cap_growth',\n",
    "    'resource_0', 'resource_1', 'resource_2', 'resource_3', 'resource_4', 'resource_5', 'resource_6', 'resource_7',\n",
    "       'resource_8', 'resource_9'],\n",
    "        node_labels = 'future_gdp_per_cap_growth')\n",
    "    \n",
    "    if(year in val_years):\n",
    "        val_graphs.append(trade.pyg_graph)\n",
    "    elif(year in test_years):\n",
    "        test_graphs.append(trade.pyg_graph)\n",
    "    else: \n",
    "        train_graphs.append(trade.pyg_graph)\n",
    "        \n",
    "    trade.features[\"year\"] = year\n",
    "    \n",
    "    if(i == 0):\n",
    "        trade_df = trade.features\n",
    "    else: \n",
    "        trade_df = pd.concat([trade_df, trade.features])\n",
    "        \n",
    "    i = i+1\n",
    "    print(trade.node_attributes.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle as pkl\n",
    "# with open(\"pygcn/train_graphs.pickle\", \"wb\") as f:\n",
    "#     pkl.dump(train_graphs, f)\n",
    "\n",
    "# with open(\"pygcn/val_graphs.pickle\", \"wb\") as f:\n",
    "#     pkl.dump(val_graphs, f)\n",
    "\n",
    "# with open(\"pygcn/test_graphs.pickle\", \"wb\") as f:\n",
    "#     pkl.dump(test_graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"pygcn/train_graphs.pickle\", \"rb\") as f:\n",
    "    train_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/val_graphs.pickle\", \"rb\") as f:  \n",
    "    val_graphs = pkl.load(f)\n",
    "\n",
    "with open(\"pygcn/test_graphs.pickle\", \"rb\") as f:         \n",
    "    test_graphs = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "test_loader = DataLoader(test_graphs, batch_size=32)\n",
    "train_loader = DataLoader(train_graphs, batch_size=32)\n",
    "val_loader = DataLoader(val_graphs, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crisis_years(year_pairs, crisis_years):\n",
    "    result = []\n",
    "    for pair in year_pairs:\n",
    "        start, end = pair\n",
    "        # Check if any crisis year is between the pair or equals the later year\n",
    "        if any(start < year <= end for year in crisis_years):\n",
    "            result.append(0)\n",
    "        else:\n",
    "            result.append(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis_years = [1963, 1967, 1989, 2001, 1986, 1993, 1996, 1983, 1978, 1973]\n",
    "\n",
    "def get_year_pairs(year_range):\n",
    "    return [(year1, year2) for year1 in year_range for year2 in year_range if year2 >= year1]\n",
    "\n",
    "def get_loader_pairs(dataset):\n",
    "    return [(dataset[i], dataset[j]) for i in range(len(dataset)) for j in range(len(dataset)) if j >= i]\n",
    "\n",
    "train_pairs = get_year_pairs(train_years)\n",
    "val_pairs = get_year_pairs(val_years)\n",
    "\n",
    "train_y = check_crisis_years(train_pairs, crisis_years)\n",
    "val_y = check_crisis_years(val_pairs, crisis_years)\n",
    "\n",
    "train_loader_pairs = get_loader_pairs(train_loader.dataset)\n",
    "val_loader_pairs = get_loader_pairs(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch_y = torch.tensor(np.array(train_y))\n",
    "val_torch_y = torch.tensor(np.array(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:05<01:43,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.629701554775238, Validation Loss: 0.567571588815787, Validation Accuracy: 0.7564102564102564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:10<01:33,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Training Loss: 0.4698948264122009, Validation Loss: 0.57479627430439, Validation Accuracy: 0.7307692307692307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:15<01:28,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Training Loss: 0.3630760908126831, Validation Loss: 0.5268542104615614, Validation Accuracy: 0.7307692307692307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:21<01:26,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Training Loss: 0.5112922787666321, Validation Loss: 0.6014474702951236, Validation Accuracy: 0.6538461538461539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:25<01:16,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Training Loss: 0.6125710010528564, Validation Loss: 0.6402017921209335, Validation Accuracy: 0.6794871794871795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:31<01:11,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Training Loss: 0.4627165198326111, Validation Loss: 0.7000292711533033, Validation Accuracy: 0.47435897435897434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:36<01:08,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Training Loss: 0.48933175206184387, Validation Loss: 0.5120823982720956, Validation Accuracy: 0.7435897435897436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:41<01:01,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Training Loss: 0.3403298556804657, Validation Loss: 0.6655868670592705, Validation Accuracy: 0.6282051282051282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:46<00:55,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Training Loss: 0.26297247409820557, Validation Loss: 0.6708501799939535, Validation Accuracy: 0.6282051282051282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:51<00:49,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Training Loss: 0.17774733901023865, Validation Loss: 0.6869776827307084, Validation Accuracy: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:56<00:44,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Training Loss: 0.1545894891023636, Validation Loss: 0.6727524689976095, Validation Accuracy: 0.7692307692307693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:00<00:39,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Training Loss: 0.14467701315879822, Validation Loss: 0.6328304145628443, Validation Accuracy: 0.7051282051282052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [01:05<00:33,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Training Loss: 0.13659539818763733, Validation Loss: 0.6478452763038103, Validation Accuracy: 0.7307692307692307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [01:10<00:28,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Training Loss: 0.11671500653028488, Validation Loss: 0.7346716875370662, Validation Accuracy: 0.717948717948718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [01:14<00:23,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Training Loss: 0.10145282745361328, Validation Loss: 0.7010075153472523, Validation Accuracy: 0.6794871794871795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [01:19<00:19,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Training Loss: 0.10997656732797623, Validation Loss: 0.7853454008064802, Validation Accuracy: 0.6153846153846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [01:24<00:14,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Training Loss: 0.08316101133823395, Validation Loss: 0.7110449823993781, Validation Accuracy: 0.6923076923076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [01:29<00:09,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Training Loss: 0.05988450348377228, Validation Loss: 0.9449647622588763, Validation Accuracy: 0.6794871794871795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [01:33<00:04,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Training Loss: 0.04096135124564171, Validation Loss: 0.8534668202893301, Validation Accuracy: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:38<00:00,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Training Loss: 0.02946605533361435, Validation Loss: 0.9629213948388667, Validation Accuracy: 0.7051282051282052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Sigmoid\n",
    "from torch_geometric.nn import GCNConv, global_sort_pool\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)\n",
    "        self.conv2 = GCNConv(128, 64)\n",
    "        self.fc = Linear(64*50, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index.to(torch.int64)).relu()\n",
    "        x = self.conv2(x, edge_index.to(torch.int64)).relu()\n",
    "        x = global_sort_pool(x, batch, k=50)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SiameseGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(SiameseGNN, self).__init__()\n",
    "        self.gnn = GNN(num_features)\n",
    "        self.fc = Linear(1,1)\n",
    "        self.sigmoid = Sigmoid()\n",
    "\n",
    "    def forward(self, data1, data2):\n",
    "        out1 = self.gnn(data1)\n",
    "        out2 = self.gnn(data2)\n",
    "        out = self.fc(torch.abs(out1 - out2))\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# DataLoader that loads pairs of graphs\n",
    "train_loader = train_loader_pairs\n",
    "val_loader = val_loader_pairs\n",
    "\n",
    "model = SiameseGNN(num_features=train_loader_pairs[0][0].num_node_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for i in range(len(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_loader[i][0], train_loader[i][1])\n",
    "        loss = criterion(out.squeeze(1).to(torch.float), train_torch_y[i].unsqueeze(-1).to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(len(val_loader)):\n",
    "            out = model(val_loader[i][0], val_loader[i][1])\n",
    "            val_loss = criterion(out.squeeze(1).to(torch.float), val_torch_y[i].unsqueeze(-1).to(torch.float))\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "            predictions = torch.round(out).squeeze(1)  # Round to 0 or 1\n",
    "            correct += (predictions == val_torch_y[i]).sum().item()\n",
    "            total += 1\n",
    "\n",
    "        val_loss = sum(val_losses) / len(val_losses)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {loss.item()}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
