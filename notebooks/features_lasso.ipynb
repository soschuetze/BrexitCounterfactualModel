{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econometric Feature Selection Using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.formula.api as sm\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange, tqdm\n",
    "import statsmodels.api as sm\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateFeatures:\n",
    "    \"\"\"\n",
    "    We define a class which builds the feature dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"data/\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_econ_features(self, filter_gdp = True):\n",
    "        \n",
    "        #DATA IMPORT\n",
    "        #import dictionary with all features from WB\n",
    "        with open(self.data_dir + 'all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "            \n",
    "        self.feature_list = list(features_dict.keys())[1:]\n",
    "        #import list of all features we want to select for\n",
    "\n",
    "        #look up each of the features -- add country feature in that year \n",
    "        i = 0\n",
    "        for feature in self.feature_list:\n",
    "            #find dataframe corresponding to specific feature name\n",
    "            df = features_dict[feature]\n",
    "            \n",
    "            if (i == 0):\n",
    "                self.features = df[[\"economy\", \"YR\" + str(self.year)]]\n",
    "            else: \n",
    "                self.features = pd.merge(self.features, \n",
    "                                            df[[\"economy\", \"YR\" + str(self.year)]],\n",
    "                                            on = \"economy\", how = \"outer\")\n",
    "            self.features.rename(columns = {\"YR\" + str(self.year): feature}, inplace = True)\n",
    "            i = i+1\n",
    "        \n",
    "        #prepare GDP feature\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        cols = list(self.gdp_growth.columns.copy())\n",
    "        cols.remove(\"economy\")\n",
    "        self.gdp_growth[\"country_sd\"] = self.gdp_growth[cols].std(axis=1)\n",
    "        #select potential variables \n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year)] \n",
    "        self.gdp_growth[\"future_3y_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year+1)]\\\n",
    "        +self.gdp_growth[\"YR\" + str(self.year+2)]+self.gdp_growth[\"YR\" + str(self.year+3)] \n",
    "        #we eliminate countries that are too volatile in growth -- probably an indicator that growth estimates are inaccurate\n",
    "        self.high_vol_countries = list(set(self.gdp_growth[self.gdp_growth[\"country_sd\"] > 10][\"economy\"]))\n",
    "        self.gdp_growth = self.gdp_growth[[\"economy\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\", \"future_3y_gdp_growth\"]].dropna()\n",
    "        \n",
    "        #combine GDP and other features\n",
    "        self.features = pd.merge(self.gdp_growth, self.features,\n",
    "                                   on = \"economy\", how = \"left\")\n",
    "        #we only keep countries where we observe GDP growth -- otherwise nothing to predict\n",
    "        #we keep countries where other features may be missing -- and fill NAs with 0 \n",
    "        self.features.rename(columns = {\"economy\": \"country_code\"}, inplace = True)\n",
    "        \n",
    "    def prepare_network_features(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = pd.read_json('https://comtrade.un.org/data/cache/classificationS2.json')\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"results\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def[\"text\"].split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(self.data_dir + \"ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        #trade_data = trade_data[trade_data[\"product_category\"] == \"1\"]\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, on = \"importer_code\")\n",
    "        imports_table_grouped[\"import_fraction\"] = imports_table_grouped[\"import_value\"]\\\n",
    "                        /imports_table_grouped[\"import_total\"]*100\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        \n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "        \n",
    "        self.G = G\n",
    "        self.centrality_overall= nx.eigenvector_centrality(G, max_iter= 10000) \n",
    "        self.centrality_overall = pd.DataFrame(list(map(list, self.centrality_overall.items())), \n",
    "                                               columns = [\"country_code\", \"centrality_overall\"])\n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", [\"import_fraction\"])\n",
    "        weighted_centrality = nx.eigenvector_centrality(G, weight = \"import_fraction\", max_iter= 10000) \n",
    "        weighted_centrality  = pd.DataFrame(list(map(list, weighted_centrality.items())), \n",
    "                                               columns = [\"country_code\", \"weighted_centrality\"])\n",
    "        self.centrality_overall = pd.merge(self.centrality_overall, weighted_centrality, on = \"country_code\")\n",
    "        \n",
    "                               \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY IN PRODUCT CATEGORIES\n",
    "\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = imports_table.groupby([\"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = products_grouped.rename(columns = {\"import_value\": \"import_product_total\"})\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"importer_code\", \"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(products_grouped, self.export_types, on = \"product_category\")\n",
    "        \n",
    "        self.export_types[\"product_export_fraction\"] = self.export_types[\"import_value\"]\\\n",
    "                                                    /self.export_types[\"import_product_total\"]*100\n",
    "        \n",
    "        list_products = list(set(self.export_types[\"product_category\"]))\n",
    "        \n",
    "        i = 0 \n",
    "        for product in list_products:\n",
    "            \n",
    "            temp = self.export_types[self.export_types[\"product_category\"] == product].copy()\n",
    "            \n",
    "            G_w=nx.from_pandas_edgelist(temp, \n",
    "                \"exporter_code\", \"importer_code\", [\"product_export_fraction\"], create_using = nx.DiGraph())\n",
    "            centrality_product_w = nx.eigenvector_centrality(G_w, weight = \"product_export_fraction\", \n",
    "                                                           max_iter= 10000)\n",
    "\n",
    "            G=nx.from_pandas_edgelist(temp,\"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "            centrality_product = nx.eigenvector_centrality(G,max_iter= 10000)\n",
    "\n",
    "            if(i == 0):\n",
    "                self.centrality_product = pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product])\n",
    "                \n",
    "\n",
    "            else: \n",
    "                self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "                \n",
    "            self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product_w.items())), \n",
    "                                               columns = [\"country_code\", \"prod_w_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "            \n",
    "            i = i+1         \n",
    "    \n",
    "    def combine_normalize_features(self):\n",
    "        \n",
    "        self.combined_features = pd.merge(self.features, self.centrality_overall,on = \"country_code\")\n",
    "        self.combined_features = pd.merge(self.combined_features, self.centrality_product,on = \"country_code\")\n",
    "        #step eliminates NA and nodes that are not in graph, since they will have NA for graph features\n",
    "        self.combined_features = self.combined_features.drop(columns = [\"index\"])\n",
    "        #filter to non-volatile countries\n",
    "        self.combined_features = self.combined_features[\\\n",
    "                                ~self.combined_features.country_code.isin(self.high_vol_countries)]\n",
    "        #filter both trade data and features data to same subset of countries\n",
    "        self.combined_features = self.combined_features[\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.importer_code)|\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.exporter_code)]\n",
    "        self.trade_data = self.trade_data[\\\n",
    "                          self.trade_data.importer_code.isin(self.combined_features.country_code)&\\\n",
    "                          self.trade_data.exporter_code.isin(self.combined_features.country_code)]\n",
    "        \n",
    "        #extract categorical features and dependent variable for growth\n",
    "        self.current_growth_list = list(self.combined_features[\"current_gdp_growth\"])\n",
    "        self.future_growth_list = list(self.combined_features[\"future_3y_gdp_growth\"])\n",
    "        self.combined_features[\"current_growth_cat\"] = self.combined_features[\"current_gdp_growth\"].\\\n",
    "                                                        apply(lambda x: growth_categories(x, self.current_growth_list))\n",
    "        self.combined_features[\"future_growth_cat\"] = self.combined_features[\"future_3y_gdp_growth\"].\\\n",
    "                                                        apply(lambda x: growth_categories(x, self.future_growth_list))\n",
    "        \n",
    "        features_to_norm = list(self.combined_features.columns.copy())\n",
    "        non_norm = [\"country_code\",'current_growth_cat','future_growth_cat', \"node_numbers\"]\n",
    "        cols_insufficient_data = list(self.combined_features.loc[:, self.combined_features.nunique() < 2].columns.copy())\n",
    "        non_norm.extend(cols_insufficient_data)\n",
    " \n",
    "        features_to_norm = [x for x in features_to_norm if x not in non_norm]\n",
    "        scaler = StandardScaler()\n",
    "        #we preserve NAs in the scaling\n",
    "        self.combined_features[features_to_norm] = scaler.fit_transform(self.combined_features[features_to_norm])\n",
    "        self.combined_features.fillna(0, inplace = True) #we fill NA after scaling \n",
    "        #check that feature has at least 20% coverage in a given year -- otherwise set to NA\n",
    "        for feature in self.feature_list:\n",
    "            coverage = len(self.combined_features[self.combined_features[feature] != 0])/len(self.combined_features)\n",
    "            if(coverage <= 0.20): self.combined_features[feature] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Trade_exploration/trade_data.pickle', 'rb') as handle:\n",
    "    data_dict = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n",
      "/var/folders/qh/0wyfgfcs3399gr_m2wrrtnzc0000gn/T/ipykernel_35843/1524783330.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data_dict[year].combined_features['year'] = year\n"
     ]
    }
   ],
   "source": [
    "for year in data_dict:\n",
    "    data_dict[year].combined_features['year'] = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = data_dict[1990].combined_features\n",
    "\n",
    "for year in range(1990, 2019):\n",
    "    combined_df = pd.concat([combined_df, data_dict[year].combined_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_col = combined_df['year']\n",
    "brexit = []\n",
    "for i in year_col:\n",
    "    if i<2016:\n",
    "        brexit.append(0)\n",
    "    else:\n",
    "        brexit.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_features = ['EG.ELC.ACCS.ZS', 'EG.FEC.RNEW.ZS', 'MS.MIL.TOTL.TF.ZS',\n",
    "       'SL.UEM.TOTL.ZS', \"country_code\", \"const\", \"prev_gdp_growth\",\"current_gdp_growth\",\"future_3y_gdp_growth\", \"current_growth_cat\",\"future_growth_cat\",\"year\"]\n",
    "feat_columns = [col for col in list(combined_df.columns) if col not in non_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_df[feat_columns]\n",
    "X = sm.add_constant(X)\n",
    "Y = brexit\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.965e+01, tolerance: 2.829e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.062e+01, tolerance: 2.673e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.562e+01, tolerance: 2.767e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.840e+01, tolerance: 2.744e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.674e+01, tolerance: 2.829e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params {'alpha': 1e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.173e+02, tolerance: 3.461e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# parameters to be tested on GridSearchCV\n",
    "params = {\"alpha\":np.arange(0.00001, 10, 500)}\n",
    "\n",
    "# Number of Folds and adding the random state for replication\n",
    "kf=KFold(n_splits=5,shuffle=True, random_state=42)\n",
    "\n",
    "# Initializing the Model\n",
    "lasso = Lasso()\n",
    "\n",
    "# GridSearchCV with model, params and folds.\n",
    "lasso_cv=GridSearchCV(lasso, param_grid=params, cv=kf)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "print(\"Best Params {}\".format(lasso_cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinclaireschuetze/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.173e+02, tolerance: 3.461e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# calling the model with the best parameter\n",
    "lasso1 = Lasso(alpha=lasso_cv.best_params_['alpha'])\n",
    "lasso1.fit(X_train, y_train)\n",
    "\n",
    "# Using np.abs() to make coefficients positive.\n",
    "lasso1_coef = np.abs(lasso1.coef_)\n",
    "\n",
    "names=X_train.columns\n",
    "\n",
    "# Subsetting the features which has more than 0.001 importance.\n",
    "feature_subset=np.array(names)[lasso1_coef>0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[feature_subset]\n",
    "X_test = X_test[feature_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AG.CON.FERT.PT.ZS</th>\n",
       "      <th>AG.CON.FERT.ZS</th>\n",
       "      <th>AG.LND.AGRI.K2</th>\n",
       "      <th>AG.LND.AGRI.ZS</th>\n",
       "      <th>AG.LND.ARBL.HA</th>\n",
       "      <th>AG.LND.ARBL.HA.PC</th>\n",
       "      <th>AG.LND.ARBL.ZS</th>\n",
       "      <th>AG.LND.CREL.HA</th>\n",
       "      <th>AG.LND.CROP.ZS</th>\n",
       "      <th>AG.LND.EL5M.RU.K2</th>\n",
       "      <th>...</th>\n",
       "      <th>prod_6</th>\n",
       "      <th>prod_w_6</th>\n",
       "      <th>prod_1</th>\n",
       "      <th>prod_w_1</th>\n",
       "      <th>prod_4</th>\n",
       "      <th>prod_w_4</th>\n",
       "      <th>prod_7</th>\n",
       "      <th>prod_w_7</th>\n",
       "      <th>prod_2</th>\n",
       "      <th>prod_w_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.279209</td>\n",
       "      <td>-0.330270</td>\n",
       "      <td>-1.403446</td>\n",
       "      <td>-0.233606</td>\n",
       "      <td>0.236460</td>\n",
       "      <td>-0.602872</td>\n",
       "      <td>-0.274581</td>\n",
       "      <td>-0.583312</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.247002</td>\n",
       "      <td>0.107595</td>\n",
       "      <td>1.176298</td>\n",
       "      <td>0.381088</td>\n",
       "      <td>1.207157</td>\n",
       "      <td>0.813162</td>\n",
       "      <td>1.326765</td>\n",
       "      <td>0.025638</td>\n",
       "      <td>0.942449</td>\n",
       "      <td>-0.032664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.199725</td>\n",
       "      <td>0.165362</td>\n",
       "      <td>-0.188574</td>\n",
       "      <td>-0.177829</td>\n",
       "      <td>0.675096</td>\n",
       "      <td>-0.813427</td>\n",
       "      <td>-0.290387</td>\n",
       "      <td>-0.530957</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.380713</td>\n",
       "      <td>-0.319738</td>\n",
       "      <td>-0.406570</td>\n",
       "      <td>-0.321306</td>\n",
       "      <td>-0.488288</td>\n",
       "      <td>-0.362811</td>\n",
       "      <td>-0.408224</td>\n",
       "      <td>-0.310418</td>\n",
       "      <td>-0.498671</td>\n",
       "      <td>-0.311677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.369551</td>\n",
       "      <td>-0.024114</td>\n",
       "      <td>-1.129407</td>\n",
       "      <td>-0.040696</td>\n",
       "      <td>-0.207089</td>\n",
       "      <td>-0.786083</td>\n",
       "      <td>-0.186754</td>\n",
       "      <td>-0.544899</td>\n",
       "      <td>-0.38631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.420007</td>\n",
       "      <td>-0.339532</td>\n",
       "      <td>-0.730559</td>\n",
       "      <td>-0.336850</td>\n",
       "      <td>-0.591661</td>\n",
       "      <td>-0.402941</td>\n",
       "      <td>-0.334412</td>\n",
       "      <td>-0.355532</td>\n",
       "      <td>-0.547192</td>\n",
       "      <td>-0.325859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.384966</td>\n",
       "      <td>-1.637327</td>\n",
       "      <td>-0.337774</td>\n",
       "      <td>-0.903715</td>\n",
       "      <td>-0.993164</td>\n",
       "      <td>-0.352655</td>\n",
       "      <td>-0.436777</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.411490</td>\n",
       "      <td>-0.367302</td>\n",
       "      <td>-0.083634</td>\n",
       "      <td>-0.319838</td>\n",
       "      <td>-0.260770</td>\n",
       "      <td>-0.365332</td>\n",
       "      <td>-0.468529</td>\n",
       "      <td>-0.332921</td>\n",
       "      <td>-0.344792</td>\n",
       "      <td>-0.358135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.218880</td>\n",
       "      <td>0.210941</td>\n",
       "      <td>0.577790</td>\n",
       "      <td>-0.100131</td>\n",
       "      <td>0.128385</td>\n",
       "      <td>-0.564866</td>\n",
       "      <td>-0.146141</td>\n",
       "      <td>-0.515386</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063917</td>\n",
       "      <td>-0.380640</td>\n",
       "      <td>-0.724668</td>\n",
       "      <td>-0.334659</td>\n",
       "      <td>-0.010350</td>\n",
       "      <td>-0.315406</td>\n",
       "      <td>0.476118</td>\n",
       "      <td>-0.344302</td>\n",
       "      <td>-0.012871</td>\n",
       "      <td>-0.317351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192855</td>\n",
       "      <td>-0.376983</td>\n",
       "      <td>-0.950914</td>\n",
       "      <td>-0.347905</td>\n",
       "      <td>-0.794110</td>\n",
       "      <td>-1.003424</td>\n",
       "      <td>-0.342616</td>\n",
       "      <td>-0.528567</td>\n",
       "      <td>-0.36003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451501</td>\n",
       "      <td>-0.363062</td>\n",
       "      <td>-0.340638</td>\n",
       "      <td>-0.334186</td>\n",
       "      <td>-0.348795</td>\n",
       "      <td>-0.361676</td>\n",
       "      <td>-0.504238</td>\n",
       "      <td>-0.300399</td>\n",
       "      <td>-0.281869</td>\n",
       "      <td>-0.281664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.392109</td>\n",
       "      <td>-1.665857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.943546</td>\n",
       "      <td>-0.363232</td>\n",
       "      <td>-0.734649</td>\n",
       "      <td>-0.352862</td>\n",
       "      <td>-1.228760</td>\n",
       "      <td>-0.395932</td>\n",
       "      <td>-0.934872</td>\n",
       "      <td>-0.397078</td>\n",
       "      <td>-1.023276</td>\n",
       "      <td>-0.352036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.044789</td>\n",
       "      <td>-0.263965</td>\n",
       "      <td>-0.338334</td>\n",
       "      <td>-0.280562</td>\n",
       "      <td>-0.534896</td>\n",
       "      <td>-0.684050</td>\n",
       "      <td>-0.292175</td>\n",
       "      <td>0.061449</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159051</td>\n",
       "      <td>-0.312424</td>\n",
       "      <td>-0.216908</td>\n",
       "      <td>-0.319075</td>\n",
       "      <td>-0.280049</td>\n",
       "      <td>-0.307520</td>\n",
       "      <td>-0.056781</td>\n",
       "      <td>-0.282004</td>\n",
       "      <td>-0.123900</td>\n",
       "      <td>-0.284097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.217937</td>\n",
       "      <td>1.419020</td>\n",
       "      <td>-0.369742</td>\n",
       "      <td>-1.502996</td>\n",
       "      <td>-0.348598</td>\n",
       "      <td>-0.851303</td>\n",
       "      <td>-1.027185</td>\n",
       "      <td>-0.354519</td>\n",
       "      <td>-0.500453</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.572465</td>\n",
       "      <td>0.650149</td>\n",
       "      <td>1.867790</td>\n",
       "      <td>0.384771</td>\n",
       "      <td>1.585214</td>\n",
       "      <td>-0.148643</td>\n",
       "      <td>1.365299</td>\n",
       "      <td>0.506813</td>\n",
       "      <td>1.615818</td>\n",
       "      <td>-0.074545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.340081</td>\n",
       "      <td>1.215777</td>\n",
       "      <td>-0.382474</td>\n",
       "      <td>-1.485441</td>\n",
       "      <td>-0.340076</td>\n",
       "      <td>-0.845977</td>\n",
       "      <td>-0.995178</td>\n",
       "      <td>-0.355378</td>\n",
       "      <td>-0.502779</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>-0.079951</td>\n",
       "      <td>0.619512</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>0.566633</td>\n",
       "      <td>-0.225567</td>\n",
       "      <td>-0.054550</td>\n",
       "      <td>-0.076193</td>\n",
       "      <td>0.484091</td>\n",
       "      <td>-0.319016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3526 rows × 1256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AG.CON.FERT.PT.ZS  AG.CON.FERT.ZS  AG.LND.AGRI.K2  AG.LND.AGRI.ZS  \\\n",
       "169           0.000000       -0.279209       -0.330270       -1.403446   \n",
       "25            0.000000       -0.199725        0.165362       -0.188574   \n",
       "30            0.000000       -0.369551       -0.024114       -1.129407   \n",
       "24            0.000000        0.000000       -0.384966       -1.637327   \n",
       "123           0.000000       -0.218880        0.210941        0.577790   \n",
       "..                 ...             ...             ...             ...   \n",
       "123           0.000000        0.192855       -0.376983       -0.950914   \n",
       "63            0.000000        0.000000       -0.392109       -1.665857   \n",
       "53            0.000000       -0.044789       -0.263965       -0.338334   \n",
       "5            -0.217937        1.419020       -0.369742       -1.502996   \n",
       "4            -0.340081        1.215777       -0.382474       -1.485441   \n",
       "\n",
       "     AG.LND.ARBL.HA  AG.LND.ARBL.HA.PC  AG.LND.ARBL.ZS  AG.LND.CREL.HA  \\\n",
       "169       -0.233606           0.236460       -0.602872       -0.274581   \n",
       "25        -0.177829           0.675096       -0.813427       -0.290387   \n",
       "30        -0.040696          -0.207089       -0.786083       -0.186754   \n",
       "24        -0.337774          -0.903715       -0.993164       -0.352655   \n",
       "123       -0.100131           0.128385       -0.564866       -0.146141   \n",
       "..              ...                ...             ...             ...   \n",
       "123       -0.347905          -0.794110       -1.003424       -0.342616   \n",
       "63         0.000000           0.000000        0.000000        0.000000   \n",
       "53        -0.280562          -0.534896       -0.684050       -0.292175   \n",
       "5         -0.348598          -0.851303       -1.027185       -0.354519   \n",
       "4         -0.340076          -0.845977       -0.995178       -0.355378   \n",
       "\n",
       "     AG.LND.CROP.ZS  AG.LND.EL5M.RU.K2  ...    prod_6  prod_w_6    prod_1  \\\n",
       "169       -0.583312            0.00000  ...  1.247002  0.107595  1.176298   \n",
       "25        -0.530957            0.00000  ... -0.380713 -0.319738 -0.406570   \n",
       "30        -0.544899           -0.38631  ... -0.420007 -0.339532 -0.730559   \n",
       "24        -0.436777            0.00000  ... -0.411490 -0.367302 -0.083634   \n",
       "123       -0.515386            0.00000  ...  0.063917 -0.380640 -0.724668   \n",
       "..              ...                ...  ...       ...       ...       ...   \n",
       "123       -0.528567           -0.36003  ... -0.451501 -0.363062 -0.340638   \n",
       "63         0.000000            0.00000  ... -0.943546 -0.363232 -0.734649   \n",
       "53         0.061449            0.00000  ... -0.159051 -0.312424 -0.216908   \n",
       "5         -0.500453            0.00000  ...  1.572465  0.650149  1.867790   \n",
       "4         -0.502779            0.00000  ...  0.015038 -0.079951  0.619512   \n",
       "\n",
       "     prod_w_1    prod_4  prod_w_4    prod_7  prod_w_7    prod_2  prod_w_2  \n",
       "169  0.381088  1.207157  0.813162  1.326765  0.025638  0.942449 -0.032664  \n",
       "25  -0.321306 -0.488288 -0.362811 -0.408224 -0.310418 -0.498671 -0.311677  \n",
       "30  -0.336850 -0.591661 -0.402941 -0.334412 -0.355532 -0.547192 -0.325859  \n",
       "24  -0.319838 -0.260770 -0.365332 -0.468529 -0.332921 -0.344792 -0.358135  \n",
       "123 -0.334659 -0.010350 -0.315406  0.476118 -0.344302 -0.012871 -0.317351  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "123 -0.334186 -0.348795 -0.361676 -0.504238 -0.300399 -0.281869 -0.281664  \n",
       "63  -0.352862 -1.228760 -0.395932 -0.934872 -0.397078 -1.023276 -0.352036  \n",
       "53  -0.319075 -0.280049 -0.307520 -0.056781 -0.282004 -0.123900 -0.284097  \n",
       "5    0.384771  1.585214 -0.148643  1.365299  0.506813  1.615818 -0.074545  \n",
       "4    0.007568  0.566633 -0.225567 -0.054550 -0.076193  0.484091 -0.319016  \n",
       "\n",
       "[3526 rows x 1256 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
