{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econometric Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import statsmodels.formula.api as sm\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange, tqdm\n",
    "import statsmodels.api as sm\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_sitc_codes():\n",
    "    # URL of the JSON file\n",
    "    url = 'https://comtradeapi.un.org/files/v1/app/reference/S4.json'\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL and fetch the data\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check that the request was successful\n",
    "        \n",
    "        # Load the JSON data\n",
    "        data = response.json()\n",
    "\n",
    "        # Since the JSON data might be nested, use json_normalize with appropriate arguments\n",
    "        if isinstance(data, list):\n",
    "            # If the top level is a list\n",
    "            df = pd.json_normalize(data)\n",
    "        else:\n",
    "            # If the top level is a dictionary\n",
    "            # Identify the key that holds the main data (adjust the path as necessary)\n",
    "            main_data_key = 'results'  # Adjust this based on the actual structure\n",
    "            df = pd.json_normalize(data[main_data_key])\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error processing JSON structure: {e}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we mimic 4-class system from World Bank -- 4 growth categoris: 0 - L, 1 - LM, 2 - UM, 3 - U\n",
    "def growth_categories(x, growth_list):\n",
    "    threshold_high = np.percentile(growth_list, 75)\n",
    "    threshold_low = np.percentile(growth_list, 25)\n",
    "    threshold_mid = np.percentile(growth_list, 50)\n",
    "    \n",
    "    if(x > threshold_high): return 3\n",
    "    elif(x < threshold_low): return 0\n",
    "    elif(x < threshold_mid): return 1\n",
    "    else: return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateFeatures:\n",
    "    \"\"\"\n",
    "    We define a class which builds the feature dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, year = 1962, data_dir = \"../data/\"):\n",
    "        self.year = year\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "    def prepare_econ_features(self, filter_gdp = True):\n",
    "        \n",
    "        #DATA IMPORT\n",
    "        #import dictionary with all features from WB\n",
    "        with open(self.data_dir + 'all_wb_indicators.pickle', 'rb') as handle:\n",
    "            features_dict = pkl.load(handle)\n",
    "            \n",
    "        self.feature_list = list(features_dict.keys())[1:]\n",
    "        #import list of all features we want to select for\n",
    "\n",
    "        #look up each of the features -- add country feature in that year \n",
    "        i = 0\n",
    "        for feature in self.feature_list:\n",
    "            #find dataframe corresponding to specific feature name\n",
    "            df = features_dict[feature]\n",
    "            \n",
    "            if (i == 0):\n",
    "                self.features = df[[\"economy\", \"YR\" + str(self.year)]]\n",
    "            else: \n",
    "                self.features = pd.merge(self.features, \n",
    "                                            df[[\"economy\", \"YR\" + str(self.year)]],\n",
    "                                            on = \"economy\", how = \"outer\")\n",
    "            self.features.rename(columns = {\"YR\" + str(self.year): feature}, inplace = True)\n",
    "            i = i+1\n",
    "        \n",
    "        #prepare GDP feature\n",
    "        self.gdp_growth = features_dict['NY.GDP.MKTP.KD.ZG']\n",
    "        cols = list(self.gdp_growth.columns.copy())\n",
    "        cols.remove(\"economy\")\n",
    "        self.gdp_growth[\"country_sd\"] = self.gdp_growth[cols].std(axis=1)\n",
    "        #select potential variables \n",
    "        self.gdp_growth[\"prev_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year-1)]\n",
    "        self.gdp_growth[\"current_gdp_growth\"] = self.gdp_growth[\"YR\" + str(self.year)]\n",
    "        #we eliminate countries that are too volatile in growth -- probably an indicator that growth estimates are inaccurate\n",
    "        self.gdp_growth = self.gdp_growth[[\"economy\", \"prev_gdp_growth\",\n",
    "                                \"current_gdp_growth\"]].dropna()\n",
    "        \n",
    "        #combine GDP and other features\n",
    "        self.features = pd.merge(self.gdp_growth, self.features,\n",
    "                                   on = \"economy\", how = \"left\")\n",
    "        #we only keep countries where we observe GDP growth -- otherwise nothing to predict\n",
    "        #we keep countries where other features may be missing -- and fill NAs with 0 \n",
    "        self.features.rename(columns = {\"economy\": \"country_code\"}, inplace = True)\n",
    "        \n",
    "    def prepare_network_features(self):\n",
    "        \"\"\"\n",
    "        We create an initial, import-centric trade link pandas dataframe for a given year\n",
    "        \"\"\"\n",
    "        #get product codes\n",
    "        data_dict = get_sitc_codes()\n",
    "        data_cross = []\n",
    "        i = 0\n",
    "        for item_def in list(data_dict[\"text\"]):\n",
    "            if(i >= 2):\n",
    "                data_cross.append(item_def.split(\" - \", 1))\n",
    "            i = i+1\n",
    "\n",
    "        self.product_codes = pd.DataFrame(data_cross, columns = ['code', 'product'])\n",
    "        self.product_codes[\"sitc_product_code\"] = self.product_codes[\"code\"]\n",
    "        \n",
    "        #get country codes\n",
    "        self.country_codes = pd.read_excel(self.data_dir + \"ISO3166.xlsx\")\n",
    "        self.country_codes[\"location_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"partner_code\"] = self.country_codes[\"Alpha-3 code\"]\n",
    "        self.country_codes[\"country_i\"] = self.country_codes[\"English short name\"]\n",
    "        self.country_codes[\"country_j\"] = self.country_codes[\"English short name\"]\n",
    "        \n",
    "        #get trade data for a given year\n",
    "        trade_data = pd.read_stata(self.data_dir + \"country_partner_sitcproduct4digit_year_\"+ str(self.year)+\".dta\") \n",
    "        #merge with product / country descriptions\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"location_code\", \"country_i\"]],on = [\"location_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.country_codes[[\"partner_code\", \"country_j\"]],on = [\"partner_code\"])\n",
    "        trade_data = pd.merge(trade_data, self.product_codes[[\"sitc_product_code\", \"product\"]], \n",
    "                              on = [\"sitc_product_code\"])\n",
    "        ###select level of product aggregation\n",
    "        trade_data[\"product_category\"] = trade_data[\"sitc_product_code\"].apply(lambda x: x[0:1])\n",
    "        #trade_data = trade_data[trade_data[\"product_category\"] == \"1\"]\n",
    "        \n",
    "        #keep only nodes that we have features for\n",
    "        trade_data = trade_data[trade_data[\"location_code\"].isin(self.features[\"country_code\"])]\n",
    "        trade_data = trade_data[trade_data[\"partner_code\"].isin(self.features[\"country_code\"])]\n",
    "        \n",
    "        if (len(trade_data.groupby([\"location_code\", \"partner_code\", \"sitc_product_code\"])[\"import_value\"].sum().reset_index()) != len(trade_data)):\n",
    "            print(\"import, export, product combination not unique!\")\n",
    "        self.trade_data1 = trade_data\n",
    "        #from import-export table, create only import table\n",
    "        #extract imports\n",
    "        imports1 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports1 = imports1[imports1[\"import_value\"] != 0]\n",
    "        #transform records of exports into imports\n",
    "        imports2 = trade_data[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'export_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        imports2[\"temp1\"] = imports2['partner_code']\n",
    "        imports2[\"temp2\"] = imports2['location_code']\n",
    "\n",
    "        imports2['location_code'] = imports2[\"temp1\"]\n",
    "        imports2['partner_code'] = imports2[\"temp2\"]\n",
    "        imports2[\"import_value\"] = imports2[\"export_value\"]\n",
    "        imports2 = imports2[imports2[\"import_value\"] != 0]\n",
    "        imports2 = imports2[['location_id', 'partner_id', 'product_id', 'year',\n",
    "               'import_value', 'sitc_eci', 'sitc_coi', 'location_code', 'partner_code',\n",
    "               'sitc_product_code', 'country_i', 'country_j', 'product', \"product_category\"]]\n",
    "        \n",
    "        imports_table = pd.concat([imports1, imports2]).drop_duplicates()\n",
    "        \n",
    "        #rename columns for better clarity\n",
    "        imports_table[\"importer_code\"] = imports_table[\"location_code\"]\n",
    "        imports_table[\"exporter_code\"] = imports_table[\"partner_code\"]\n",
    "        imports_table[\"importer_name\"] = imports_table[\"country_i\"]\n",
    "        imports_table[\"exporter_name\"] = imports_table[\"country_j\"]\n",
    "        \n",
    "        cols = [\"importer_code\", \"exporter_code\", \"importer_name\", \"exporter_name\",\n",
    "               'product_id', 'year', 'import_value', 'sitc_eci', 'sitc_coi',\n",
    "               'sitc_product_code', 'product', \"product_category\"]\n",
    "        imports_table = imports_table[cols]\n",
    "        \n",
    "        exporter_total = imports_table.groupby([\"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        exporter_total = exporter_total.rename(columns = {\"import_value\": \"export_total\"})\n",
    "        \n",
    "        importer_total = imports_table.groupby([\"importer_code\"])[\"import_value\"].sum().reset_index()\n",
    "        importer_total = importer_total.rename(columns = {\"import_value\": \"import_total\"})\n",
    "        \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        imports_table_grouped = pd.merge(imports_table_grouped, importer_total, on = \"importer_code\")\n",
    "        imports_table_grouped[\"import_fraction\"] = imports_table_grouped[\"import_value\"]\\\n",
    "                        /imports_table_grouped[\"import_total\"]*100\n",
    "        \n",
    "        self.trade_data = imports_table_grouped\n",
    "        \n",
    "        #filter features and nodes to ones that are connected to others in trade data\n",
    "        list_active_countries = list(set(list(self.trade_data [\"importer_code\"])+\\\n",
    "                        list(self.trade_data [\"exporter_code\"])))\n",
    "        self.features = self.features[self.features[\"country_code\"].isin(list_active_countries)].reset_index()\n",
    "        self.features[\"node_numbers\"] = self.features.index\n",
    "        \n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "        \n",
    "        self.G = G\n",
    "        self.centrality_overall= nx.eigenvector_centrality(G, max_iter= 10000) \n",
    "        self.centrality_overall = pd.DataFrame(list(map(list, self.centrality_overall.items())), \n",
    "                                               columns = [\"country_code\", \"centrality_overall\"])\n",
    "        G=nx.from_pandas_edgelist(self.trade_data, \n",
    "                          \"exporter_code\", \"importer_code\", [\"import_fraction\"])\n",
    "        weighted_centrality = nx.eigenvector_centrality(G, weight = \"import_fraction\", max_iter= 10000) \n",
    "        weighted_centrality  = pd.DataFrame(list(map(list, weighted_centrality.items())), \n",
    "                                               columns = [\"country_code\", \"weighted_centrality\"])\n",
    "        self.centrality_overall = pd.merge(self.centrality_overall, weighted_centrality, on = \"country_code\")\n",
    "        \n",
    "                               \n",
    "        ##### COMPUTE CENTRALITY FOR COUNTRY IN PRODUCT CATEGORIES\n",
    "\n",
    "        #sum imports across all products between countries into single value \n",
    "        imports_table_grouped = imports_table.groupby([\"importer_code\", \"exporter_code\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = imports_table.groupby([\"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        products_grouped = products_grouped.rename(columns = {\"import_value\": \"import_product_total\"})\n",
    "        \n",
    "        #sum exports in each category \n",
    "        self.export_types = imports_table.groupby([\"importer_code\", \"exporter_code\", \"product_category\"])[\"import_value\"].sum().reset_index()\n",
    "        self.export_types = pd.merge(products_grouped, self.export_types, on = \"product_category\")\n",
    "        \n",
    "        self.export_types[\"product_export_fraction\"] = self.export_types[\"import_value\"]\\\n",
    "                                                    /self.export_types[\"import_product_total\"]*100\n",
    "        \n",
    "        list_products = list(set(self.export_types[\"product_category\"]))\n",
    "        \n",
    "        i = 0 \n",
    "        for product in list_products:\n",
    "            \n",
    "            temp = self.export_types[self.export_types[\"product_category\"] == product].copy()\n",
    "            \n",
    "            G_w=nx.from_pandas_edgelist(temp, \n",
    "                \"exporter_code\", \"importer_code\", [\"product_export_fraction\"], create_using = nx.DiGraph())\n",
    "            centrality_product_w = nx.eigenvector_centrality(G_w, weight = \"product_export_fraction\", \n",
    "                                                           max_iter= 10000)\n",
    "\n",
    "            G=nx.from_pandas_edgelist(temp,\"exporter_code\", \"importer_code\", create_using = nx.DiGraph())\n",
    "            centrality_product = nx.eigenvector_centrality(G,max_iter= 10000)\n",
    "\n",
    "            if(i == 0):\n",
    "                self.centrality_product = pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product])\n",
    "                \n",
    "\n",
    "            else: \n",
    "                self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product.items())), \n",
    "                                               columns = [\"country_code\", \"prod_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "                \n",
    "            self.centrality_product = pd.merge(self.centrality_product, \n",
    "                                               pd.DataFrame(list(map(list, centrality_product_w.items())), \n",
    "                                               columns = [\"country_code\", \"prod_w_\" + product]), \n",
    "                                                  on = \"country_code\")\n",
    "            \n",
    "            i = i+1         \n",
    "    \n",
    "    def combine_normalize_features(self):\n",
    "        \n",
    "        self.combined_features = pd.merge(self.features, self.centrality_overall,on = \"country_code\")\n",
    "        self.combined_features = pd.merge(self.combined_features, self.centrality_product,on = \"country_code\")\n",
    "        #step eliminates NA and nodes that are not in graph, since they will have NA for graph features\n",
    "        self.combined_features = self.combined_features.drop(columns = [\"index\"])\n",
    "        #filter both trade data and features data to same subset of countries\n",
    "        self.combined_features = self.combined_features[\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.importer_code)|\\\n",
    "                                self.combined_features.country_code.isin(self.trade_data.exporter_code)]\n",
    "        self.trade_data = self.trade_data[\\\n",
    "                          self.trade_data.importer_code.isin(self.combined_features.country_code)&\\\n",
    "                          self.trade_data.exporter_code.isin(self.combined_features.country_code)]\n",
    "        \n",
    "        features_to_norm = list(self.combined_features.columns.copy())\n",
    "        non_norm = [\"country_code\", \"node_numbers\"]\n",
    "        cols_insufficient_data = list(self.combined_features.loc[:, self.combined_features.nunique() < 2].columns.copy())\n",
    "        non_norm.extend(cols_insufficient_data)\n",
    " \n",
    "        features_to_norm = [x for x in features_to_norm if x not in non_norm]\n",
    "        scaler = StandardScaler()\n",
    "        #we preserve NAs in the scaling\n",
    "        self.combined_features[features_to_norm] = scaler.fit_transform(self.combined_features[features_to_norm])\n",
    "        self.combined_features.fillna(0, inplace = True) #we fill NA after scaling \n",
    "        #check that feature has at least 20% coverage in a given year -- otherwise set to NA\n",
    "        for feature in self.feature_list:\n",
    "            coverage = len(self.combined_features[self.combined_features[feature] != 0])/len(self.combined_features)\n",
    "            if(coverage <= 0.20): self.combined_features[feature] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "\n",
    "#GDP data goes up to 2018 -- trange up to 2019 spans up to 2018\n",
    "for year in trange(1962, 2019):\n",
    "    trade = CreateFeatures(year = year)\n",
    "    trade.prepare_econ_features()\n",
    "    trade.prepare_network_features()\n",
    "    trade.combine_normalize_features()\n",
    "    \n",
    "    data_dict[year] = trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[2000].combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/features_dict.pkl\", \"wb\") as f:\n",
    "    pkl.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../feature_dicts/features_dict.pkl\", \"rb\") as f:\n",
    "    data_dict = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to keep track of the DataFrame with the most rows\n",
    "max_rows = 0\n",
    "df_with_max_rows = None\n",
    "\n",
    "# Iterate over the dictionary\n",
    "for key, df in data_dict.items():\n",
    "    # If this DataFrame has more rows than the current maximum, update the maximum and the DataFrame\n",
    "    if len(df.combined_features) > max_rows:\n",
    "        max_rows = len(df.combined_features)\n",
    "        df_with_max_rows = df.combined_features\n",
    "        max_year = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the variance of each column\n",
    "numeric_columns = df_with_max_rows.drop(['country_code'], axis=1)\n",
    "variances = numeric_columns.var()\n",
    "\n",
    "# Find columns with variance less than 0.1 (this is the threshold, adjust as needed)\n",
    "columns_to_drop = variances[variances < 0.1].index\n",
    "filtered_df = df_with_max_rows.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dictionary\n",
    "for key, df in data_dict.items():\n",
    "    df.combined_features = df.combined_features.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = filtered_df.drop(['country_code','current_gdp_growth'], axis = 1)\n",
    "Y_train = filtered_df['current_gdp_growth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "mutual_info = mutual_info_regression(X_train, Y_train)\n",
    "\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_train.columns\n",
    "mutual_info.sort_values(ascending=False)\n",
    "\n",
    "selected_top_columns = SelectPercentile(mutual_info_regression, percentile=2)\n",
    "selected_top_columns.fit(X_train, Y_train)\n",
    "selected_top_columns.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns[selected_top_columns.get_support()]\n",
    "X_train = X_train[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = list(columns)\n",
    "column_list.append('current_gdp_growth')\n",
    "column_list.append('country_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data_dict.items():\n",
    "    df.combined_features = df.combined_features[column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_all_zeros = []\n",
    "for key, df in data_dict.items():\n",
    "    zero_columns = [col for col in df.combined_features.columns if (df.combined_features[col] == 0).all()]\n",
    "\n",
    "    # Iterate over list1\n",
    "    for item in zero_columns:\n",
    "        # If the item is not in list2, add it\n",
    "        if item not in columns_all_zeros:\n",
    "            columns_all_zeros.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in data_dict.items():\n",
    "    df.combined_features = df.combined_features.drop(columns_all_zeros, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IT.MLT.MAIN.P2</th>\n",
       "      <th>NE.CON.PRVT.KD.ZG</th>\n",
       "      <th>NE.CON.TOTL.KD.ZG</th>\n",
       "      <th>NV.IND.TOTL.KD.ZG</th>\n",
       "      <th>NV.SRV.TOTL.KD.ZG</th>\n",
       "      <th>NY.GDP.MKTP.KD.ZG</th>\n",
       "      <th>NY.GDP.PCAP.KD.ZG</th>\n",
       "      <th>NY.GNP.MKTP.KD.ZG</th>\n",
       "      <th>NY.GNP.PCAP.KD.ZG</th>\n",
       "      <th>SP.ADO.TFRT</th>\n",
       "      <th>SP.POP.2024.FE.5Y</th>\n",
       "      <th>SP.POP.2024.MA.5Y</th>\n",
       "      <th>SP.POP.6569.FE.5Y</th>\n",
       "      <th>SP.POP.6569.MA.5Y</th>\n",
       "      <th>SP.POP.65UP.MA.ZS</th>\n",
       "      <th>current_gdp_growth</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.111774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.024184</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.112815</td>\n",
       "      <td>-1.038860</td>\n",
       "      <td>-0.703257</td>\n",
       "      <td>-0.630105</td>\n",
       "      <td>-0.892056</td>\n",
       "      <td>0.112493</td>\n",
       "      <td>-0.161362</td>\n",
       "      <td>0.282761</td>\n",
       "      <td>0.626574</td>\n",
       "      <td>0.204211</td>\n",
       "      <td>-1.112815</td>\n",
       "      <td>ARG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.301831</td>\n",
       "      <td>-0.340188</td>\n",
       "      <td>-0.246697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.714648</td>\n",
       "      <td>-0.780316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.351662</td>\n",
       "      <td>-1.104107</td>\n",
       "      <td>-1.529665</td>\n",
       "      <td>-1.375682</td>\n",
       "      <td>1.179255</td>\n",
       "      <td>1.004898</td>\n",
       "      <td>1.104184</td>\n",
       "      <td>-0.714648</td>\n",
       "      <td>AUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.097787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.463491</td>\n",
       "      <td>-0.153232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.882694</td>\n",
       "      <td>-0.717122</td>\n",
       "      <td>0.877274</td>\n",
       "      <td>2.809942</td>\n",
       "      <td>2.482537</td>\n",
       "      <td>2.321666</td>\n",
       "      <td>-0.463491</td>\n",
       "      <td>AUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.417067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>0.395375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.402846</td>\n",
       "      <td>-2.383251</td>\n",
       "      <td>-2.241161</td>\n",
       "      <td>2.496846</td>\n",
       "      <td>2.458060</td>\n",
       "      <td>2.431750</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>BEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.685382</td>\n",
       "      <td>-0.913001</td>\n",
       "      <td>-0.700270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.590323</td>\n",
       "      <td>-1.541320</td>\n",
       "      <td>-1.038805</td>\n",
       "      <td>-0.983784</td>\n",
       "      <td>0.733933</td>\n",
       "      <td>0.551797</td>\n",
       "      <td>0.097609</td>\n",
       "      <td>0.329067</td>\n",
       "      <td>0.341150</td>\n",
       "      <td>0.307165</td>\n",
       "      <td>-1.590323</td>\n",
       "      <td>BEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-0.611217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078634</td>\n",
       "      <td>-0.011029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.635345</td>\n",
       "      <td>0.119556</td>\n",
       "      <td>0.378782</td>\n",
       "      <td>-0.147779</td>\n",
       "      <td>-0.315724</td>\n",
       "      <td>-0.368509</td>\n",
       "      <td>0.078634</td>\n",
       "      <td>TUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-0.129166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102288</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.246832</td>\n",
       "      <td>-1.099825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.759923</td>\n",
       "      <td>-0.112155</td>\n",
       "      <td>-0.163143</td>\n",
       "      <td>0.871415</td>\n",
       "      <td>1.106939</td>\n",
       "      <td>1.135215</td>\n",
       "      <td>-1.246832</td>\n",
       "      <td>URY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2.761470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176675</td>\n",
       "      <td>0.333370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.553262</td>\n",
       "      <td>-1.746300</td>\n",
       "      <td>-2.167133</td>\n",
       "      <td>1.292109</td>\n",
       "      <td>1.442964</td>\n",
       "      <td>1.559219</td>\n",
       "      <td>0.176675</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-0.495877</td>\n",
       "      <td>-0.245292</td>\n",
       "      <td>-0.328785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.627946</td>\n",
       "      <td>0.396841</td>\n",
       "      <td>0.655605</td>\n",
       "      <td>0.504555</td>\n",
       "      <td>0.276462</td>\n",
       "      <td>0.416738</td>\n",
       "      <td>0.129565</td>\n",
       "      <td>-0.947292</td>\n",
       "      <td>-0.888139</td>\n",
       "      <td>-0.864482</td>\n",
       "      <td>0.627946</td>\n",
       "      <td>VEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-0.198863</td>\n",
       "      <td>-0.245512</td>\n",
       "      <td>0.173022</td>\n",
       "      <td>-0.062343</td>\n",
       "      <td>0.263255</td>\n",
       "      <td>0.191130</td>\n",
       "      <td>0.052638</td>\n",
       "      <td>0.393830</td>\n",
       "      <td>0.222922</td>\n",
       "      <td>-0.088021</td>\n",
       "      <td>0.264219</td>\n",
       "      <td>0.331525</td>\n",
       "      <td>-0.541908</td>\n",
       "      <td>-0.685512</td>\n",
       "      <td>-0.590398</td>\n",
       "      <td>0.191130</td>\n",
       "      <td>ZAF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    IT.MLT.MAIN.P2  NE.CON.PRVT.KD.ZG  NE.CON.TOTL.KD.ZG  NV.IND.TOTL.KD.ZG  \\\n",
       "0        -0.111774           0.000000          -1.024184           0.000000   \n",
       "1         1.301831          -0.340188          -0.246697           0.000000   \n",
       "2         0.097787           0.000000           0.000000           0.000000   \n",
       "3         0.417067           0.000000           0.000000           0.000000   \n",
       "4        -0.685382          -0.913001          -0.700270           0.000000   \n",
       "..             ...                ...                ...                ...   \n",
       "71       -0.611217           0.000000           0.000000           0.000000   \n",
       "72       -0.129166           0.000000           0.102288           0.000000   \n",
       "73        2.761470           0.000000           0.000000           0.000000   \n",
       "74       -0.495877          -0.245292          -0.328785           0.000000   \n",
       "75       -0.198863          -0.245512           0.173022          -0.062343   \n",
       "\n",
       "    NV.SRV.TOTL.KD.ZG  NY.GDP.MKTP.KD.ZG  NY.GDP.PCAP.KD.ZG  \\\n",
       "0            0.000000          -1.112815          -1.038860   \n",
       "1            0.000000          -0.714648          -0.780316   \n",
       "2            0.000000          -0.463491          -0.153232   \n",
       "3            0.000000           0.011966           0.395375   \n",
       "4            0.000000          -1.590323          -1.541320   \n",
       "..                ...                ...                ...   \n",
       "71           0.000000           0.078634          -0.011029   \n",
       "72           0.000000          -1.246832          -1.099825   \n",
       "73           0.000000           0.176675           0.333370   \n",
       "74           0.000000           0.627946           0.396841   \n",
       "75           0.263255           0.191130           0.052638   \n",
       "\n",
       "    NY.GNP.MKTP.KD.ZG  NY.GNP.PCAP.KD.ZG  SP.ADO.TFRT  SP.POP.2024.FE.5Y  \\\n",
       "0           -0.703257          -0.630105    -0.892056           0.112493   \n",
       "1            0.000000          -0.351662    -1.104107          -1.529665   \n",
       "2            0.000000           0.000000    -0.882694          -0.717122   \n",
       "3            0.000000           0.000000    -1.402846          -2.383251   \n",
       "4           -1.038805          -0.983784     0.733933           0.551797   \n",
       "..                ...                ...          ...                ...   \n",
       "71           0.000000           0.000000     0.635345           0.119556   \n",
       "72           0.000000           0.000000    -0.759923          -0.112155   \n",
       "73           0.000000           0.000000    -0.553262          -1.746300   \n",
       "74           0.655605           0.504555     0.276462           0.416738   \n",
       "75           0.393830           0.222922    -0.088021           0.264219   \n",
       "\n",
       "    SP.POP.2024.MA.5Y  SP.POP.6569.FE.5Y  SP.POP.6569.MA.5Y  \\\n",
       "0           -0.161362           0.282761           0.626574   \n",
       "1           -1.375682           1.179255           1.004898   \n",
       "2            0.877274           2.809942           2.482537   \n",
       "3           -2.241161           2.496846           2.458060   \n",
       "4            0.097609           0.329067           0.341150   \n",
       "..                ...                ...                ...   \n",
       "71           0.378782          -0.147779          -0.315724   \n",
       "72          -0.163143           0.871415           1.106939   \n",
       "73          -2.167133           1.292109           1.442964   \n",
       "74           0.129565          -0.947292          -0.888139   \n",
       "75           0.331525          -0.541908          -0.685512   \n",
       "\n",
       "    SP.POP.65UP.MA.ZS  current_gdp_growth country_code  \n",
       "0            0.204211           -1.112815          ARG  \n",
       "1            1.104184           -0.714648          AUS  \n",
       "2            2.321666           -0.463491          AUT  \n",
       "3            2.431750            0.011966          BEL  \n",
       "4            0.307165           -1.590323          BEN  \n",
       "..                ...                 ...          ...  \n",
       "71          -0.368509            0.078634          TUR  \n",
       "72           1.135215           -1.246832          URY  \n",
       "73           1.559219            0.176675          USA  \n",
       "74          -0.864482            0.627946          VEN  \n",
       "75          -0.590398            0.191130          ZAF  \n",
       "\n",
       "[76 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[1962].combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/filtered_features_dict.pkl\", \"wb\") as f:\n",
    "    pkl.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Subset Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = filtered_df.drop(['country_code','current_gdp_growth'], axis = 1).iloc[:,1:-23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_columns = X_train.sample(n=15, axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GC.XPN.TOTL.CN', 'DC.DAC.DNKL.CD', 'ST.INT.DPRT', 'SP.POP.6064.FE.5Y',\n",
       "       'SL.IND.EMPL.MA.ZS', 'DT.NFL.PRVT.CD', 'SE.XPD.CSEC.ZS',\n",
       "       'SP.URB.TOTL.IN.ZS', 'NY.GNP.MKTP.PP.KD', 'DC.DAC.POLL.CD',\n",
       "       'TX.VAL.MRCH.R5.ZS', 'SE.PRM.OENR.FE.ZS', 'NE.CON.GOVT.CN',\n",
       "       'SL.TLF.BASC.ZS', 'EN.ATM.PM25.MC.M3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dictionary\n",
    "for key, df in data_dict.items():\n",
    "\n",
    "    # Combine the lists\n",
    "    all_cols = ['country_code','current_gdp_growth'] + random_columns.tolist()\n",
    "\n",
    "    # Select the columns from the dataframe\n",
    "    df.combined_features = df.combined_features[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../feature_dicts/random_features_dict.pkl\", \"wb\") as f:\n",
    "    pkl.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
